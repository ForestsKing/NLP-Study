{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10RYw2RphUPP"
      },
      "source": [
        "# 导包"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSDBp0S7hSWp"
      },
      "source": [
        "import torch\r\n",
        "import numpy as np\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.utils.data as Data\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "dtype = torch.FloatTensor\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpnzkORUhbQ3"
      },
      "source": [
        "# 自定义数据和参数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvqGRtpThfUO"
      },
      "source": [
        "# 3 words sentences (=sequence_length is 3)\r\n",
        "sentences = [\"i love you\", \"he loves me\", \"she likes baseball\", \"i hate you\", \"sorry for that\", \"this is awful\"]\r\n",
        "labels = [1, 1, 1, 0, 0, 0]  # 1 is good, 0 is not good.\r\n",
        "\r\n",
        "# TextCNN Parameter\r\n",
        "embedding_size = 2\r\n",
        "sequence_length = len(sentences[0].split()) # every sentences contains sequence_length(=3) words\r\n",
        "num_classes = len(set(labels))  # num_classes=2\r\n",
        "batch_size = 3\r\n",
        "\r\n",
        "word_list = \" \".join(sentences).split()\r\n",
        "vocab = list(set(word_list))\r\n",
        "word2idx = {w: i for i, w in enumerate(vocab)}\r\n",
        "vocab_size = len(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp0Fy-Y2jzw3"
      },
      "source": [
        "# 数据预处理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTWkBZ5Aj1Av"
      },
      "source": [
        "def make_data(sentences, labels):\r\n",
        "  inputs = []\r\n",
        "  for sen in sentences: # sen是一个句子\r\n",
        "      inputs.append([word2idx[n] for n in sen.split()])\r\n",
        "\r\n",
        "  targets = []\r\n",
        "  for out in labels:\r\n",
        "      targets.append(out) # To using Torch Softmax Loss function\r\n",
        "  return inputs, targets\r\n",
        "\r\n",
        "input_batch, target_batch = make_data(sentences, labels)\r\n",
        "input_batch, target_batch = torch.LongTensor(input_batch), torch.LongTensor(target_batch)\r\n",
        "\r\n",
        "dataset = Data.TensorDataset(input_batch, target_batch)\r\n",
        "loader = Data.DataLoader(dataset, batch_size, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4_K8BODk-hm"
      },
      "source": [
        "# 构建模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pMEVcc3k-_-"
      },
      "source": [
        "class TextCNN(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(TextCNN, self).__init__()\r\n",
        "        self.W = nn.Embedding(vocab_size, embedding_size)\r\n",
        "        output_channel = 3\r\n",
        "        self.conv = nn.Sequential(\r\n",
        "            # conv : [input_channel(=1), output_channel, (filter_height, filter_width), stride=1]\r\n",
        "            nn.Conv2d(1, output_channel, (2, embedding_size)),\r\n",
        "            nn.ReLU(),\r\n",
        "            # pool : ((filter_height, filter_width))\r\n",
        "            nn.MaxPool2d((2, 1)),\r\n",
        "        )\r\n",
        "        # fc\r\n",
        "        self.fc = nn.Linear(output_channel, num_classes)\r\n",
        "\r\n",
        "    def forward(self, X):\r\n",
        "      '''\r\n",
        "      X: [batch_size, sequence_length]\r\n",
        "      '''\r\n",
        "      batch_size = X.shape[0]\r\n",
        "      embedding_X = self.W(X) # [batch_size, sequence_length, embedding_size]\r\n",
        "      embedding_X = embedding_X.unsqueeze(1) # add channel(=1) [batch, channel(=1), sequence_length, embedding_size]\r\n",
        "      conved = self.conv(embedding_X) # [batch_size, output_channel*1*1]\r\n",
        "      flatten = conved.view(batch_size, -1)\r\n",
        "      output = self.fc(flatten)\r\n",
        "      return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJmL5PYRlthn"
      },
      "source": [
        "# 训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wG30S4Cylv3v",
        "outputId": "886247af-91e6-4516-ec3d-39f62564bf07"
      },
      "source": [
        "model = TextCNN().to(device)\r\n",
        "criterion = nn.CrossEntropyLoss().to(device)\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\r\n",
        "\r\n",
        "# Training\r\n",
        "for epoch in range(5000):\r\n",
        "  for batch_x, batch_y in loader:\r\n",
        "    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\r\n",
        "    pred = model(batch_x)\r\n",
        "    loss = criterion(pred, batch_y)\r\n",
        "    if (epoch + 1) % 1000 == 0:\r\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\r\n",
        "\r\n",
        "    optimizer.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1000 loss = 0.136996\n",
            "Epoch: 1000 loss = 0.501182\n",
            "Epoch: 2000 loss = 0.636514\n",
            "Epoch: 2000 loss = 0.000245\n",
            "Epoch: 3000 loss = 0.135205\n",
            "Epoch: 3000 loss = 0.501388\n",
            "Epoch: 4000 loss = 0.501312\n",
            "Epoch: 4000 loss = 0.135233\n",
            "Epoch: 5000 loss = 0.501346\n",
            "Epoch: 5000 loss = 0.135189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH66BgBulydv"
      },
      "source": [
        "# 测试"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MBsasvHlz5u",
        "outputId": "0395d0d0-3114-4d07-97a7-3453bf43fab8"
      },
      "source": [
        "# Test\r\n",
        "test_text = 'i hate me'\r\n",
        "tests = [[word2idx[n] for n in test_text.split()]]\r\n",
        "test_batch = torch.LongTensor(tests).to(device)\r\n",
        "# Predict\r\n",
        "model = model.eval()\r\n",
        "predict = model(test_batch).data.max(1, keepdim=True)[1] # 返回最大的元素并返回索引\r\n",
        "if predict[0][0] == 0:\r\n",
        "    print(test_text,\"is Bad Mean...\")\r\n",
        "else:\r\n",
        "    print(test_text,\"is Good Mean!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i hate me is Bad Mean...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}