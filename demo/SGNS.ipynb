{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SGNS.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2pto0JhuqTD"
      },
      "source": [
        "# 导包并定义模型参数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLb4VEUeuPkz"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.utils.data as tud\r\n",
        "\r\n",
        "from collections import Counter # 计算频率\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "\r\n",
        "import scipy\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity # 计算相似度\r\n",
        "\r\n",
        "dtype = torch.FloatTensor\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "random.seed(1) # 固定种子，便于复现\r\n",
        "np.random.seed(1)\r\n",
        "torch.manual_seed(1)\r\n",
        "\r\n",
        "C = 3 # 窗口大小\r\n",
        "K = 15 # 负采样数\r\n",
        "epochs = 1 # 训练1次\r\n",
        "MAX_VOCAB_SIZE = 10000 # 取频率前9999个和一个<UNK>\r\n",
        "EMBEDDING_SIZE = 100 # 词向量维度\r\n",
        "batch_size = 32 # 每批32个"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xa6enQnwqCb"
      },
      "source": [
        "# 读取文本数据并处理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7ZfqW6lwufO",
        "outputId": "86ada209-13d6-4e7c-9e9f-6134447a7f3b"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "with open('/content/drive//MyDrive/data/text8/text8.test.txt') as f:\r\n",
        "    text = f.read() # 得到文本内容\r\n",
        "\r\n",
        "text = text.lower().split() #　分割成单词列表\r\n",
        "vocab_dict = dict(Counter(text).most_common(MAX_VOCAB_SIZE - 1)) # 得到单词字典表，key是单词，value是次数\r\n",
        "vocab_dict['<UNK>'] = len(text) - np.sum(list(vocab_dict.values())) # 把不常用的单词都编码为\"<UNK>\"\r\n",
        "word2idx = {word:i for i, word in enumerate(vocab_dict.keys())}\r\n",
        "idx2word = {i:word for i, word in enumerate(vocab_dict.keys())} # 建立索引\r\n",
        "word_counts = np.array([count for count in vocab_dict.values()], dtype=np.float32) # 转变成数组\r\n",
        "word_freqs = word_counts / np.sum(word_counts) # 计算频率\r\n",
        "word_freqs = word_freqs ** (3./4.) # 将频率变为原来的 0.75 次方"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JzC7o1u2Woj"
      },
      "source": [
        "# 实现 DataLoader\r\n",
        "- 把所有 word 编码成数字\r\n",
        "- 保存 vocabulary，单词 count、normalized word frequency\r\n",
        "- 每个 iteration sample 一个中心词\r\n",
        "- 根据当前的中心词返回 context 单词\r\n",
        "- 根据中心词 sample 一些 negative 单词\r\n",
        "- 返回 sample 出的所有数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3fc5FiI2a3R"
      },
      "source": [
        "class WordEmbeddingDataset(tud.Dataset):\r\n",
        "    def __init__(self, text, word2idx, word_freqs):\r\n",
        "        super(WordEmbeddingDataset, self).__init__() # #通过父类初始化模型，然后重写两个方法\r\n",
        "        self.text_encoded = [word2idx.get(word, word2idx['<UNK>']) for word in text] # 把单词数字化表示。如果不在词典中，也表示为unk\r\n",
        "        self.text_encoded = torch.LongTensor(self.text_encoded) # nn.Embedding需要传入LongTensor类型\r\n",
        "        self.word2idx = word2idx\r\n",
        "        self.word_freqs = torch.Tensor(word_freqs)\r\n",
        "        \r\n",
        "        \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.text_encoded) # 返回所有单词的总数，即item的总数\r\n",
        "    \r\n",
        "    def __getitem__(self, idx):\r\n",
        "        ''' 这个function返回以下数据用于训练\r\n",
        "            - 中心词\r\n",
        "            - 这个单词附近的positive word\r\n",
        "            - 随机采样的K个单词作为negative word\r\n",
        "        '''\r\n",
        "        center_words = self.text_encoded[idx] # 取得中心词\r\n",
        "        pos_indices = list(range(idx - C, idx)) + list(range(idx + 1, idx + C + 1)) # 先取得中心左右各C个词的索引\r\n",
        "        pos_indices = [i % len(self.text_encoded) for i in pos_indices] # 为了避免索引越界，所以进行取余处理\r\n",
        "        pos_words = self.text_encoded[pos_indices] # tensor(list)\r\n",
        "        \r\n",
        "        neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[0], True)\r\n",
        "        # torch.multinomial作用是对self.word_freqs做K * pos_words.shape[0]次取值，输出的是self.word_freqs对应的下标\r\n",
        "        # 取样方式采用有放回的采样，并且self.word_freqs数值越大，取样概率越大\r\n",
        "        # 每采样一个正确的单词(positive word)，就采样K个错误的单词(negative word)，pos_words.shape[0]是正确单词数量\r\n",
        "        \r\n",
        "        # while 循环是为了保证 neg_words中不能包含背景词\r\n",
        "        while len(set(pos_words.numpy().tolist()) & set(neg_words.numpy().tolist())) > 0:\r\n",
        "            neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[0], True)\r\n",
        "\r\n",
        "        return center_words, pos_words, neg_words\r\n",
        "\r\n",
        "dataset = WordEmbeddingDataset(text, word2idx, word_freqs)\r\n",
        "dataloader = tud.DataLoader(dataset, batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFmuYVad9F5M"
      },
      "source": [
        "# 定义 PyTorch 模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFBOD4aU9HA1"
      },
      "source": [
        "class EmbeddingModel(nn.Module):\r\n",
        "    def __init__(self, vocab_size, embed_size):\r\n",
        "        super(EmbeddingModel, self).__init__()\r\n",
        "        \r\n",
        "        self.vocab_size = vocab_size\r\n",
        "        self.embed_size = embed_size\r\n",
        "        \r\n",
        "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size) # 训练出来的权重就是每个词作为中心词的权重\r\n",
        "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size) # 训练出来的权重就是每个词作为背景词的权重\r\n",
        "        \r\n",
        "    def forward(self, input_labels, pos_labels, neg_labels):\r\n",
        "        ''' input_labels: center words, [batch_size]\r\n",
        "            pos_labels: positive words, [batch_size, (window_size * 2)]\r\n",
        "            neg_labels：negative words, [batch_size, (window_size * 2 * K)]\r\n",
        "            \r\n",
        "            return: loss, [batch_size]\r\n",
        "        '''\r\n",
        "        input_embedding = self.in_embed(input_labels) # [batch_size, embed_size]\r\n",
        "        pos_embedding = self.out_embed(pos_labels)# [batch_size, (window * 2), embed_size]\r\n",
        "        neg_embedding = self.out_embed(neg_labels) # [batch_size, (window * 2 * K), embed_size]\r\n",
        "        \r\n",
        "        input_embedding = input_embedding.unsqueeze(2) # [batch_size, embed_size, 1] # 增加维度\r\n",
        "\r\n",
        "        # bmm(a, b)，batch matrix multiply。函数中的两个参数 a,b \r\n",
        "        # 都是维度为 3 的 tensor，并且这两个 tensor 的第一个维度必须相同，后面两个维度必须满足矩阵乘法的要求     \r\n",
        "        pos_dot = torch.bmm(pos_embedding, input_embedding) # [batch_size, (window * 2), 1]\r\n",
        "        pos_dot = pos_dot.squeeze(2) # [batch_size, (window * 2)]\r\n",
        "        \r\n",
        "        neg_dot = torch.bmm(neg_embedding, -input_embedding) # [batch_size, (window * 2 * K), 1]\r\n",
        "        neg_dot = neg_dot.squeeze(2) # batch_size, (window * 2 * K)]\r\n",
        "        \r\n",
        "        log_pos = F.logsigmoid(pos_dot).sum(1) # .sum()结果只为一个数，.sum(1)结果是一维的张量\r\n",
        "        log_neg = F.logsigmoid(neg_dot).sum(1)\r\n",
        "        \r\n",
        "        loss = log_pos + log_neg\r\n",
        "        \r\n",
        "        return -loss\r\n",
        "    \r\n",
        "    def input_embedding(self):\r\n",
        "        return self.in_embed.weight.detach().numpy()\r\n",
        "\r\n",
        "model = EmbeddingModel(MAX_VOCAB_SIZE, EMBEDDING_SIZE).to(device)\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KroimpjN_Uk0"
      },
      "source": [
        "# 训练模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFmQ-CfZ_e_z",
        "outputId": "4f1de509-a72f-46c0-fc26-844ce33adce7"
      },
      "source": [
        "for e in range(epochs):\r\n",
        "    for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\r\n",
        "        input_labels = input_labels.long().to(device)\r\n",
        "        pos_labels = pos_labels.long().to(device)\r\n",
        "        neg_labels = neg_labels.long().to(device)\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss = model(input_labels, pos_labels, neg_labels).mean()\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        if i % 1000 == 0:\r\n",
        "            print('epoch', e, 'iteration', i, loss.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 iteration 0 397.1173095703125\n",
            "epoch 0 iteration 1000 261.72503662109375\n",
            "epoch 0 iteration 2000 228.12376403808594\n",
            "epoch 0 iteration 3000 159.30996704101562\n",
            "epoch 0 iteration 4000 131.16302490234375\n",
            "epoch 0 iteration 5000 88.49677276611328\n",
            "epoch 0 iteration 6000 128.16195678710938\n",
            "epoch 0 iteration 7000 109.7065200805664\n",
            "epoch 0 iteration 8000 69.03596496582031\n",
            "epoch 0 iteration 9000 72.1278305053711\n",
            "epoch 0 iteration 10000 62.440711975097656\n",
            "epoch 0 iteration 11000 69.9999008178711\n",
            "epoch 0 iteration 12000 39.46931838989258\n",
            "epoch 0 iteration 13000 40.16891860961914\n",
            "epoch 0 iteration 14000 46.689842224121094\n",
            "epoch 0 iteration 15000 37.453643798828125\n",
            "epoch 0 iteration 16000 32.9650764465332\n",
            "epoch 0 iteration 17000 29.186105728149414\n",
            "epoch 0 iteration 18000 45.67950439453125\n",
            "epoch 0 iteration 19000 29.1345157623291\n",
            "epoch 0 iteration 20000 28.938091278076172\n",
            "epoch 0 iteration 21000 31.526216506958008\n",
            "epoch 0 iteration 22000 29.683401107788086\n",
            "epoch 0 iteration 23000 27.85382080078125\n",
            "epoch 0 iteration 24000 23.91950225830078\n",
            "epoch 0 iteration 25000 47.711341857910156\n",
            "epoch 0 iteration 26000 28.47548484802246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s8RHoH9JRZq"
      },
      "source": [
        "embedding_weights = model.cpu().input_embedding()\r\n",
        "torch.save(model.state_dict(), \"embedding-{}.th\".format(EMBEDDING_SIZE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z1100a0AKSs"
      },
      "source": [
        "# 词向量应用"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXiRUIK9AK4t"
      },
      "source": [
        "def find_nearest(word):\r\n",
        "    index = word2idx[word]\r\n",
        "    embedding = embedding_weights[index]\r\n",
        "    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\r\n",
        "    return [idx2word[i] for i in cos_dis.argsort()[:10]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Rvd0TS0AOfr",
        "outputId": "b5436329-0df7-4362-9a0d-a9dbe67ed861"
      },
      "source": [
        "for word in [\"two\", \"america\", \"computer\"]:\r\n",
        "    print(word, find_nearest(word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "two ['two', 'three', 'five', 'four', 'zero', 'six', 'one', 'seven', 'eight', 'nine']\n",
            "america ['america', 'world', 'into', 'also', 'first', 'area', 'from', 'mutation', 'binary', 'younger']\n",
            "computer ['computer', 'has', 'often', 'windows', 'popular', 'system', 'not', 'phillips', 'sultan', 'have']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}