{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHZAvGesXSBx"
      },
      "source": [
        "# 导包"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHCqDPIRUf2y"
      },
      "source": [
        "import re\r\n",
        "import math\r\n",
        "import torch\r\n",
        "import numpy as np\r\n",
        "from random import *\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.utils.data as Data"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57LXb0lJXYII"
      },
      "source": [
        "# 准备数据集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD98ajXqXakY"
      },
      "source": [
        "text = (\r\n",
        "    'Hello, how are you? I am Romeo.\\n' # R\r\n",
        "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n' # J\r\n",
        "    'Nice meet you too. How are you today?\\n' # R\r\n",
        "    'Great. My baseball team won the competition.\\n' # J\r\n",
        "    'Oh Congratulations, Juliet\\n' # R\r\n",
        "    'Thank you Romeo\\n' # J\r\n",
        "    'Where are you going today?\\n' # R\r\n",
        "    'I am going shopping. What about you?\\n' # J\r\n",
        "    'I am going to visit my grandmother. she is not very well' # R\r\n",
        ")\r\n",
        "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n') # filter '.', ',', '?', '!'\r\n",
        "word_list = list(set(\" \".join(sentences).split())) # ['hello', 'how', 'are', 'you',...]\r\n",
        "word2idx = {'[PAD]' : 0, '[CLS]' : 1, '[SEP]' : 2, '[MASK]' : 3}\r\n",
        "for i, w in enumerate(word_list):\r\n",
        "    word2idx[w] = i + 4\r\n",
        "idx2word = {i: w for i, w in enumerate(word2idx)}\r\n",
        "vocab_size = len(word2idx)\r\n",
        "\r\n",
        "token_list = list()\r\n",
        "for sentence in sentences:\r\n",
        "    arr = [word2idx[s] for s in sentence.split()]\r\n",
        "    token_list.append(arr)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-szRaZ9IXdCg"
      },
      "source": [
        "# 模型参数\r\n",
        "- maxlen 表示同一个 batch 中的所有句子都由 30 个 token 组成，不够的补 PAD（这里我实现的方式比较粗暴，直接固定所有 batch 中的所有句子都为 30）\r\n",
        "- max_pred 表示最多需要预测多少个单词，即 BERT 中的完形填空任务\r\n",
        "- n_layers 表示 Encoder Layer 的数量\r\n",
        "- d_model 表示 Token Embeddings、Segment Embeddings、Position Embeddings 的维度\r\n",
        "- d_ff 表示 Encoder Layer 中全连接层的维度\r\n",
        "- n_segments 表示 Decoder input 由几句话组成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuSnsa75XgYI"
      },
      "source": [
        "# BERT Parameters\r\n",
        "maxlen = 30\r\n",
        "batch_size = 6\r\n",
        "max_pred = 5 # max tokens of prediction\r\n",
        "n_layers = 6\r\n",
        "n_heads = 12\r\n",
        "d_model = 768\r\n",
        "d_ff = 768*4 # 4*d_model, FeedForward dimension\r\n",
        "d_k = d_v = 64  # dimension of K(=Q), V\r\n",
        "n_segments = 2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsJTdx6RXiox"
      },
      "source": [
        "# 数据预处理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkuReyHhXxoQ"
      },
      "source": [
        "# sample IsNext and NotNext to be same in small batch size\r\n",
        "def make_data():\r\n",
        "    batch = []\r\n",
        "    positive = negative = 0\r\n",
        "    while positive != batch_size/2 or negative != batch_size/2:\r\n",
        "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences)) # sample random index in sentences\r\n",
        "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\r\n",
        "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\r\n",
        "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\r\n",
        "\r\n",
        "        # MASK LM\r\n",
        "        n_pred =  min(max_pred, max(1, int(len(input_ids) * 0.15))) # 15 % of tokens in one sentence\r\n",
        "        cand_maked_pos = [i for i, token in enumerate(input_ids)\r\n",
        "                          if token != word2idx['[CLS]'] and token != word2idx['[SEP]']] # candidate masked position\r\n",
        "        shuffle(cand_maked_pos)\r\n",
        "        masked_tokens, masked_pos = [], []\r\n",
        "        for pos in cand_maked_pos[:n_pred]:\r\n",
        "            masked_pos.append(pos)\r\n",
        "            masked_tokens.append(input_ids[pos])\r\n",
        "            if random() < 0.8:  # 80%\r\n",
        "                input_ids[pos] = word2idx['[MASK]'] # make mask\r\n",
        "            elif random() > 0.9:  # 10%\r\n",
        "                index = randint(0, vocab_size - 1) # random index in vocabulary\r\n",
        "                while index < 4: # can't involve 'CLS', 'SEP', 'PAD'\r\n",
        "                  index = randint(0, vocab_size - 1)\r\n",
        "                input_ids[pos] = index # replace\r\n",
        "\r\n",
        "        # Zero Paddings\r\n",
        "        n_pad = maxlen - len(input_ids)\r\n",
        "        input_ids.extend([0] * n_pad)\r\n",
        "        segment_ids.extend([0] * n_pad)\r\n",
        "\r\n",
        "        # Zero Padding (100% - 15%) tokens\r\n",
        "        if max_pred > n_pred:\r\n",
        "            n_pad = max_pred - n_pred\r\n",
        "            masked_tokens.extend([0] * n_pad)\r\n",
        "            masked_pos.extend([0] * n_pad)\r\n",
        "\r\n",
        "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\r\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\r\n",
        "            positive += 1\r\n",
        "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\r\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\r\n",
        "            negative += 1\r\n",
        "    return batch\r\n",
        "# Proprecessing Finished\r\n",
        "\r\n",
        "batch = make_data()\r\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = zip(*batch)\r\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = \\\r\n",
        "    torch.LongTensor(input_ids),  torch.LongTensor(segment_ids), torch.LongTensor(masked_tokens),\\\r\n",
        "    torch.LongTensor(masked_pos), torch.LongTensor(isNext)\r\n",
        "\r\n",
        "class MyDataSet(Data.Dataset):\r\n",
        "  def __init__(self, input_ids, segment_ids, masked_tokens, masked_pos, isNext):\r\n",
        "    self.input_ids = input_ids\r\n",
        "    self.segment_ids = segment_ids\r\n",
        "    self.masked_tokens = masked_tokens\r\n",
        "    self.masked_pos = masked_pos\r\n",
        "    self.isNext = isNext\r\n",
        "  \r\n",
        "  def __len__(self):\r\n",
        "    return len(self.input_ids)\r\n",
        "  \r\n",
        "  def __getitem__(self, idx):\r\n",
        "    return self.input_ids[idx], self.segment_ids[idx], self.masked_tokens[idx], self.masked_pos[idx], self.isNext[idx]\r\n",
        "\r\n",
        "loader = Data.DataLoader(MyDataSet(input_ids, segment_ids, masked_tokens, masked_pos, isNext), batch_size, True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBM70keKX0hh"
      },
      "source": [
        "# 模型构建"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNIZlRteX3ox"
      },
      "source": [
        "def get_attn_pad_mask(seq_q, seq_k):\r\n",
        "    batch_size, seq_len = seq_q.size()\r\n",
        "    # eq(zero) is PAD token\r\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # [batch_size, 1, seq_len]\r\n",
        "    return pad_attn_mask.expand(batch_size, seq_len, seq_len)  # [batch_size, seq_len, seq_len]\r\n",
        "\r\n",
        "def gelu(x):\r\n",
        "    \"\"\"\r\n",
        "      Implementation of the gelu activation function.\r\n",
        "      For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\r\n",
        "      0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\r\n",
        "      Also see https://arxiv.org/abs/1606.08415\r\n",
        "    \"\"\"\r\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\r\n",
        "\r\n",
        "class Embedding(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Embedding, self).__init__()\r\n",
        "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\r\n",
        "        self.pos_embed = nn.Embedding(maxlen, d_model)  # position embedding\r\n",
        "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\r\n",
        "        self.norm = nn.LayerNorm(d_model)\r\n",
        "\r\n",
        "    def forward(self, x, seg):\r\n",
        "        seq_len = x.size(1)\r\n",
        "        pos = torch.arange(seq_len, dtype=torch.long)\r\n",
        "        pos = pos.unsqueeze(0).expand_as(x)  # [seq_len] -> [batch_size, seq_len]\r\n",
        "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\r\n",
        "        return self.norm(embedding)\r\n",
        "\r\n",
        "class ScaledDotProductAttention(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(ScaledDotProductAttention, self).__init__()\r\n",
        "\r\n",
        "    def forward(self, Q, K, V, attn_mask):\r\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size, n_heads, seq_len, seq_len]\r\n",
        "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\r\n",
        "        attn = nn.Softmax(dim=-1)(scores)\r\n",
        "        context = torch.matmul(attn, V)\r\n",
        "        return context\r\n",
        "\r\n",
        "class MultiHeadAttention(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(MultiHeadAttention, self).__init__()\r\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\r\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)\r\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads)\r\n",
        "    def forward(self, Q, K, V, attn_mask):\r\n",
        "        # q: [batch_size, seq_len, d_model], k: [batch_size, seq_len, d_model], v: [batch_size, seq_len, d_model]\r\n",
        "        residual, batch_size = Q, Q.size(0)\r\n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\r\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size, n_heads, seq_len, d_k]\r\n",
        "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size, n_heads, seq_len, d_k]\r\n",
        "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size, n_heads, seq_len, d_v]\r\n",
        "\r\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size, n_heads, seq_len, seq_len]\r\n",
        "\r\n",
        "        # context: [batch_size, n_heads, seq_len, d_v], attn: [batch_size, n_heads, seq_len, seq_len]\r\n",
        "        context = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\r\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size, seq_len, n_heads, d_v]\r\n",
        "        output = nn.Linear(n_heads * d_v, d_model)(context)\r\n",
        "        return nn.LayerNorm(d_model)(output + residual) # output: [batch_size, seq_len, d_model]\r\n",
        "\r\n",
        "class PoswiseFeedForwardNet(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\r\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\r\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_ff) -> (batch_size, seq_len, d_model)\r\n",
        "        return self.fc2(gelu(self.fc1(x)))\r\n",
        "\r\n",
        "class EncoderLayer(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(EncoderLayer, self).__init__()\r\n",
        "        self.enc_self_attn = MultiHeadAttention()\r\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\r\n",
        "\r\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\r\n",
        "        enc_outputs = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\r\n",
        "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size, seq_len, d_model]\r\n",
        "        return enc_outputs\r\n",
        "\r\n",
        "class BERT(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(BERT, self).__init__()\r\n",
        "        self.embedding = Embedding()\r\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\r\n",
        "        self.fc = nn.Sequential(\r\n",
        "            nn.Linear(d_model, d_model),\r\n",
        "            nn.Dropout(0.5),\r\n",
        "            nn.Tanh(),\r\n",
        "        )\r\n",
        "        self.classifier = nn.Linear(d_model, 2)\r\n",
        "        self.linear = nn.Linear(d_model, d_model)\r\n",
        "        self.activ2 = gelu\r\n",
        "        # fc2 is shared with embedding layer\r\n",
        "        embed_weight = self.embedding.tok_embed.weight\r\n",
        "        self.fc2 = nn.Linear(d_model, vocab_size, bias=False)\r\n",
        "        self.fc2.weight = embed_weight\r\n",
        "\r\n",
        "    def forward(self, input_ids, segment_ids, masked_pos):\r\n",
        "        output = self.embedding(input_ids, segment_ids) # [bach_size, seq_len, d_model]\r\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids) # [batch_size, maxlen, maxlen]\r\n",
        "        for layer in self.layers:\r\n",
        "            # output: [batch_size, max_len, d_model]\r\n",
        "            output = layer(output, enc_self_attn_mask)\r\n",
        "        # it will be decided by first token(CLS)\r\n",
        "        h_pooled = self.fc(output[:, 0]) # [batch_size, d_model]\r\n",
        "        logits_clsf = self.classifier(h_pooled) # [batch_size, 2] predict isNext\r\n",
        "\r\n",
        "        masked_pos = masked_pos[:, :, None].expand(-1, -1, d_model) # [batch_size, max_pred, d_model]\r\n",
        "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\r\n",
        "        h_masked = self.activ2(self.linear(h_masked)) # [batch_size, max_pred, d_model]\r\n",
        "        logits_lm = self.fc2(h_masked) # [batch_size, max_pred, vocab_size]\r\n",
        "        return logits_lm, logits_clsf\r\n",
        "model = BERT()\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=0.001)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofnHbVSeX7Yo"
      },
      "source": [
        "# 训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcBZn7CrX_fR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3548b0bb-baf3-4479-a71f-dbc969761024"
      },
      "source": [
        "for epoch in range(180):\r\n",
        "    for input_ids, segment_ids, masked_tokens, masked_pos, isNext in loader:\r\n",
        "      logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\r\n",
        "      loss_lm = criterion(logits_lm.view(-1, vocab_size), masked_tokens.view(-1)) # for masked LM\r\n",
        "      loss_lm = (loss_lm.float()).mean()\r\n",
        "      loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\r\n",
        "      loss = loss_lm + loss_clsf\r\n",
        "      if (epoch + 1) % 10 == 0:\r\n",
        "          print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\r\n",
        "      optimizer.zero_grad()\r\n",
        "      loss.backward()\r\n",
        "      optimizer.step()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0010 loss = 1.761259\n",
            "Epoch: 0020 loss = 1.056670\n",
            "Epoch: 0030 loss = 0.831934\n",
            "Epoch: 0040 loss = 0.811621\n",
            "Epoch: 0050 loss = 0.780151\n",
            "Epoch: 0060 loss = 0.726939\n",
            "Epoch: 0070 loss = 0.721764\n",
            "Epoch: 0080 loss = 0.696536\n",
            "Epoch: 0090 loss = 0.702473\n",
            "Epoch: 0100 loss = 0.708774\n",
            "Epoch: 0110 loss = 0.666144\n",
            "Epoch: 0120 loss = 0.672475\n",
            "Epoch: 0130 loss = 0.684500\n",
            "Epoch: 0140 loss = 0.690448\n",
            "Epoch: 0150 loss = 0.689220\n",
            "Epoch: 0160 loss = 0.711759\n",
            "Epoch: 0170 loss = 0.691851\n",
            "Epoch: 0180 loss = 0.673173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNLzQo66YCQh"
      },
      "source": [
        "# 测试"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu4vOJExYB3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7590f66-8be7-4284-fe67-eb5fd8bd494a"
      },
      "source": [
        "# Predict mask tokens ans isNext\r\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = batch[0]\r\n",
        "print(text)\r\n",
        "print([idx2word[w] for w in input_ids if idx2word[w] != '[PAD]'])\r\n",
        "\r\n",
        "logits_lm, logits_clsf = model(torch.LongTensor([input_ids]), \\\r\n",
        "                 torch.LongTensor([segment_ids]), torch.LongTensor([masked_pos]))\r\n",
        "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\r\n",
        "print('masked tokens list : ',[pos for pos in masked_tokens if pos != 0])\r\n",
        "print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\r\n",
        "\r\n",
        "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\r\n",
        "print('isNext : ', True if isNext else False)\r\n",
        "print('predict isNext : ',True if logits_clsf else False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello, how are you? I am Romeo.\n",
            "Hello, Romeo My name is Juliet. Nice to meet you.\n",
            "Nice meet you too. How are you today?\n",
            "Great. My baseball team won the competition.\n",
            "Oh Congratulations, Juliet\n",
            "Thank you Romeo\n",
            "Where are you going today?\n",
            "I am going shopping. What about you?\n",
            "I am going to visit my grandmother. she is not very well\n",
            "['[CLS]', 'i', 'am', 'going', '[MASK]', 'what', '[MASK]', 'you', '[SEP]', 'hello', 'romeo', 'my', 'name', 'is', 'juliet', 'nice', 'to', 'meet', '[MASK]', '[SEP]']\n",
            "masked tokens list :  [5, 34, 30]\n",
            "predict masked tokens list :  [5, 34, 30]\n",
            "isNext :  False\n",
            "predict isNext :  True\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}