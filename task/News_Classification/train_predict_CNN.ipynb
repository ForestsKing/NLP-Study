{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_predict",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwOQ1qKgGP_b"
      },
      "source": [
        "# 导包"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAQVNliZLmLt"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.nn import init\r\n",
        "import torch.optim as optim\r\n",
        "import torch.utils.data as Data\r\n",
        "\r\n",
        "from torchtext import data\r\n",
        "from torchtext.vocab import Vectors\r\n",
        "from torchtext.data import Iterator, BucketIterator\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "import csv\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "import time\r\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whwpOSSVLm49",
        "outputId": "7e8f728d-7e5f-4e45-a660-c0ba693428da"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzuDsLPsGhpR"
      },
      "source": [
        "# 定义超参数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2pMkRniLoy8"
      },
      "source": [
        "BATCH_SIZE = 8\r\n",
        "EPOCHS = 10\r\n",
        "FIX_LENGTH = 2500\r\n",
        "N_CLASS=14\r\n",
        "LR=0.001\r\n",
        "OUT_CHANNEL=1000\r\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktfhcdRFGma_"
      },
      "source": [
        "# 划分训练集验证集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXLm92ppLqvu"
      },
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/data/News classification/train_set.csv', sep='\\t')\r\n",
        "\r\n",
        "train, val = train_test_split(train_df, test_size=0.1)\r\n",
        "train.to_csv(\"/content/drive/MyDrive/data/News classification/tmp/train.csv\", index=False)\r\n",
        "val.to_csv(\"/content/drive/MyDrive/data/News classification/tmp/val.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ziehWKFGsSp"
      },
      "source": [
        "# 构建Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWXPqhPJLspF",
        "outputId": "99011571-f22b-4ff1-f1cd-49750713fa08"
      },
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/data/News classification/tmp/train.csv')\r\n",
        "valid_data = pd.read_csv('/content/drive/MyDrive/data/News classification/tmp/val.csv')\r\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/data/News classification/test_a.csv\")\r\n",
        "\r\n",
        "tokenize = lambda x: x.split()\r\n",
        "# fix_length指定了每条文本的长度，截断补长\r\n",
        "TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, fix_length=FIX_LENGTH)\r\n",
        "LABEL = data.Field(sequential=False, use_vocab=False)\r\n",
        "\r\n",
        "def get_dataset(csv_data, text_field, label_field, test=False):\r\n",
        "\t# id数据对训练在训练过程中没用，使用None指定其对应的field\r\n",
        "    fields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\r\n",
        "                 (\"text\", text_field), (\"label\", label_field)]       \r\n",
        "    examples = []\r\n",
        "\r\n",
        "    if test:\r\n",
        "        # 如果为测试集，则不加载label\r\n",
        "        for text in tqdm(csv_data['text']):\r\n",
        "            examples.append(data.Example.fromlist([None, text, None], fields))\r\n",
        "    else:\r\n",
        "        for text, label in tqdm(zip(csv_data['text'], csv_data['label'])):\r\n",
        "            examples.append(data.Example.fromlist([None, text, label], fields))\r\n",
        "    return examples, fields\r\n",
        "\r\n",
        "\r\n",
        "# 得到构建Dataset所需的examples和fields\r\n",
        "train_examples, train_fields = get_dataset(train_data, TEXT, LABEL)\r\n",
        "valid_examples, valid_fields = get_dataset(valid_data, TEXT, LABEL)\r\n",
        "test_examples, test_fields = get_dataset(test_data, TEXT, None, test=True)\r\n",
        "\r\n",
        "# 构建Dataset数据集\r\n",
        "train = data.Dataset(train_examples, train_fields)\r\n",
        "valid = data.Dataset(valid_examples, valid_fields)\r\n",
        "test = data.Dataset(test_examples, test_fields)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "180000it [00:56, 3171.83it/s]\n",
            "20000it [00:05, 3639.63it/s]\n",
            "100%|██████████| 50000/50000 [00:17<00:00, 2933.10it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYbPl4QKGvGJ"
      },
      "source": [
        "# 构建迭代器"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYE9qqHZLvLM"
      },
      "source": [
        "train_iter, val_iter = BucketIterator.splits(\r\n",
        "        (train, valid),\r\n",
        "        batch_sizes=(BATCH_SIZE, BATCH_SIZE),\r\n",
        "        device = DEVICE, # 如果使用gpu，此处将-1更换为GPU的编号\r\n",
        "        sort_key=lambda x: len(x.text), # the BucketIterator needs to be told what function it should use to group the data.\r\n",
        "        sort_within_batch=False,\r\n",
        "        repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\r\n",
        ")\r\n",
        "\r\n",
        "test_iter = Iterator(test, batch_size=BATCH_SIZE, device=DEVICE, sort=False, train=False, sort_within_batch=False, repeat=False) # train=False可以保证顺序不变"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6Fe04JlG7hx"
      },
      "source": [
        "# 加载词向量"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVjENZKuLxsu",
        "outputId": "8efc0fd1-a5a6-4c87-d0ee-c9b4b46779fd"
      },
      "source": [
        "vectors = Vectors(name='/content/drive/MyDrive/data/News classification/tmp/mymodel.txt')\r\n",
        "TEXT.build_vocab(train, vectors=vectors)\r\n",
        "weight_matrix = TEXT.vocab.vectors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/6151 [00:00<?, ?it/s]Skipping token b'6151' with 1-dimensional vector [b'100']; likely a header\n",
            " 96%|█████████▌| 5894/6151 [00:00<00:00, 18770.32it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MviFc5zdG3_S"
      },
      "source": [
        "# 定义模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwx1sFJaLzzH"
      },
      "source": [
        "class CNN(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(CNN, self).__init__()\r\n",
        "        self.embedding = nn.Embedding(len(TEXT.vocab), 100)     \r\n",
        "        self.embedding.weight.data.copy_(weight_matrix)\r\n",
        "\r\n",
        "        self.conv = nn.Sequential(\r\n",
        "            # (8, 1, 2500, 100)\r\n",
        "            nn.Conv2d(1, OUT_CHANNEL, (2, 100)), # input_channel(=1), output_channel, (filter_height, filter_width), stride=1]\r\n",
        "            # (8, 1000, 2499, 100)\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.MaxPool2d((2499, 1)), # ((filter_height, filter_width))\r\n",
        "            # (8, 1000, 1, 1)\r\n",
        "        )\r\n",
        "        self.out = nn.Linear(OUT_CHANNEL, N_CLASS)\r\n",
        "\r\n",
        "    def forward(self, X): # X: [fix_length, batch_size]\r\n",
        "        embeds = self.embedding(X)  # [fix_length, batch_size, embedding_size]  \r\n",
        "        embeds = self.embedding(X).transpose(0, 1) # [batch_size, sequence_length, embedding_size]\r\n",
        "        embeds = embeds.unsqueeze(1) # [batch, channel(=1), sequence_length, embedding_size]\r\n",
        "        \r\n",
        "        conved = self.conv(embeds)\r\n",
        "        flatten = conved.view(BATCH_SIZE, -1)\r\n",
        "        y = self.out(flatten)\r\n",
        "        return y\r\n",
        "\r\n",
        "model = CNN().to(DEVICE)\r\n",
        "loss_fc = nn.CrossEntropyLoss().to(DEVICE)\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn2RO4WtHApK"
      },
      "source": [
        "# 训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSUkPCc-L2DN",
        "outputId": "0239491b-51e3-451d-f8be-0ec75388a5c0"
      },
      "source": [
        "start = time.perf_counter()\r\n",
        "for epoch in range(EPOCHS):\r\n",
        "    for i, batch in enumerate(train_iter):\r\n",
        "        optimizer.zero_grad()\r\n",
        "        pred = model(batch.text)\r\n",
        "\t\r\n",
        "        loss = loss_fc(pred, batch.label)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        if (i+1) % 1250 == 0:\r\n",
        "            end = time.perf_counter()\r\n",
        "            print('Epoch: ', epoch, '| batch: ', (i+1)*8, '| train loss: %.4f' % loss, '| time: %.2f s' % (end-start))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0 | batch:  10000 | train loss: 0.6136 | time: 31.84 s\n",
            "Epoch:  0 | batch:  20000 | train loss: 0.1008 | time: 63.23 s\n",
            "Epoch:  0 | batch:  30000 | train loss: 0.9332 | time: 94.60 s\n",
            "Epoch:  0 | batch:  40000 | train loss: 0.2081 | time: 125.95 s\n",
            "Epoch:  0 | batch:  50000 | train loss: 0.1780 | time: 157.31 s\n",
            "Epoch:  0 | batch:  60000 | train loss: 0.9344 | time: 188.67 s\n",
            "Epoch:  0 | batch:  70000 | train loss: 0.0056 | time: 220.07 s\n",
            "Epoch:  0 | batch:  80000 | train loss: 0.6538 | time: 251.66 s\n",
            "Epoch:  0 | batch:  90000 | train loss: 0.0562 | time: 283.16 s\n",
            "Epoch:  0 | batch:  100000 | train loss: 0.0129 | time: 314.52 s\n",
            "Epoch:  0 | batch:  110000 | train loss: 0.2322 | time: 345.95 s\n",
            "Epoch:  0 | batch:  120000 | train loss: 0.0278 | time: 377.38 s\n",
            "Epoch:  0 | batch:  130000 | train loss: 0.8838 | time: 408.77 s\n",
            "Epoch:  0 | batch:  140000 | train loss: 0.3172 | time: 440.15 s\n",
            "Epoch:  0 | batch:  150000 | train loss: 0.3538 | time: 471.53 s\n",
            "Epoch:  0 | batch:  160000 | train loss: 0.0929 | time: 502.95 s\n",
            "Epoch:  0 | batch:  170000 | train loss: 0.5129 | time: 534.31 s\n",
            "Epoch:  0 | batch:  180000 | train loss: 0.1764 | time: 565.70 s\n",
            "Epoch:  1 | batch:  10000 | train loss: 0.0004 | time: 597.31 s\n",
            "Epoch:  1 | batch:  20000 | train loss: 0.0124 | time: 628.72 s\n",
            "Epoch:  1 | batch:  30000 | train loss: 0.0203 | time: 660.12 s\n",
            "Epoch:  1 | batch:  40000 | train loss: 0.1322 | time: 691.54 s\n",
            "Epoch:  1 | batch:  50000 | train loss: 0.2701 | time: 722.94 s\n",
            "Epoch:  1 | batch:  60000 | train loss: 0.0040 | time: 754.35 s\n",
            "Epoch:  1 | batch:  70000 | train loss: 0.0815 | time: 785.77 s\n",
            "Epoch:  1 | batch:  80000 | train loss: 0.3577 | time: 817.25 s\n",
            "Epoch:  1 | batch:  90000 | train loss: 0.5343 | time: 848.67 s\n",
            "Epoch:  1 | batch:  100000 | train loss: 0.0239 | time: 880.13 s\n",
            "Epoch:  1 | batch:  110000 | train loss: 0.0222 | time: 911.59 s\n",
            "Epoch:  1 | batch:  120000 | train loss: 0.0266 | time: 943.03 s\n",
            "Epoch:  1 | batch:  130000 | train loss: 0.0006 | time: 974.45 s\n",
            "Epoch:  1 | batch:  140000 | train loss: 0.0078 | time: 1005.91 s\n",
            "Epoch:  1 | batch:  150000 | train loss: 0.0320 | time: 1037.28 s\n",
            "Epoch:  1 | batch:  160000 | train loss: 0.0538 | time: 1068.72 s\n",
            "Epoch:  1 | batch:  170000 | train loss: 1.7839 | time: 1100.12 s\n",
            "Epoch:  1 | batch:  180000 | train loss: 0.2389 | time: 1131.53 s\n",
            "Epoch:  2 | batch:  10000 | train loss: 0.0014 | time: 1163.23 s\n",
            "Epoch:  2 | batch:  20000 | train loss: 0.0186 | time: 1194.68 s\n",
            "Epoch:  2 | batch:  30000 | train loss: 0.0409 | time: 1226.14 s\n",
            "Epoch:  2 | batch:  40000 | train loss: 0.3058 | time: 1257.55 s\n",
            "Epoch:  2 | batch:  50000 | train loss: 0.0714 | time: 1289.00 s\n",
            "Epoch:  2 | batch:  60000 | train loss: 0.2287 | time: 1320.46 s\n",
            "Epoch:  2 | batch:  70000 | train loss: 0.0350 | time: 1351.85 s\n",
            "Epoch:  2 | batch:  80000 | train loss: 0.3318 | time: 1383.27 s\n",
            "Epoch:  2 | batch:  90000 | train loss: 0.1740 | time: 1414.76 s\n",
            "Epoch:  2 | batch:  100000 | train loss: 0.1154 | time: 1446.15 s\n",
            "Epoch:  2 | batch:  110000 | train loss: 0.0025 | time: 1477.64 s\n",
            "Epoch:  2 | batch:  120000 | train loss: 0.0339 | time: 1509.10 s\n",
            "Epoch:  2 | batch:  130000 | train loss: 0.5222 | time: 1540.62 s\n",
            "Epoch:  2 | batch:  140000 | train loss: 0.7053 | time: 1572.08 s\n",
            "Epoch:  2 | batch:  150000 | train loss: 0.0275 | time: 1603.60 s\n",
            "Epoch:  2 | batch:  160000 | train loss: 0.0002 | time: 1635.09 s\n",
            "Epoch:  2 | batch:  170000 | train loss: 0.0095 | time: 1666.59 s\n",
            "Epoch:  2 | batch:  180000 | train loss: 0.0208 | time: 1698.09 s\n",
            "Epoch:  3 | batch:  10000 | train loss: 0.0049 | time: 1729.88 s\n",
            "Epoch:  3 | batch:  20000 | train loss: 0.0170 | time: 1761.36 s\n",
            "Epoch:  3 | batch:  30000 | train loss: 0.0071 | time: 1792.87 s\n",
            "Epoch:  3 | batch:  40000 | train loss: 0.1169 | time: 1824.32 s\n",
            "Epoch:  3 | batch:  50000 | train loss: 0.0279 | time: 1855.81 s\n",
            "Epoch:  3 | batch:  60000 | train loss: 0.4270 | time: 1887.31 s\n",
            "Epoch:  3 | batch:  70000 | train loss: 0.0020 | time: 1918.86 s\n",
            "Epoch:  3 | batch:  80000 | train loss: 0.0956 | time: 1950.36 s\n",
            "Epoch:  3 | batch:  90000 | train loss: 0.0007 | time: 1981.83 s\n",
            "Epoch:  3 | batch:  100000 | train loss: 0.0285 | time: 2013.38 s\n",
            "Epoch:  3 | batch:  110000 | train loss: 0.0069 | time: 2044.89 s\n",
            "Epoch:  3 | batch:  120000 | train loss: 0.2615 | time: 2076.43 s\n",
            "Epoch:  3 | batch:  130000 | train loss: 0.0285 | time: 2107.94 s\n",
            "Epoch:  3 | batch:  140000 | train loss: 0.0043 | time: 2139.46 s\n",
            "Epoch:  3 | batch:  150000 | train loss: 0.0962 | time: 2170.96 s\n",
            "Epoch:  3 | batch:  160000 | train loss: 0.0662 | time: 2202.48 s\n",
            "Epoch:  3 | batch:  170000 | train loss: 0.0894 | time: 2234.02 s\n",
            "Epoch:  3 | batch:  180000 | train loss: 0.6817 | time: 2265.50 s\n",
            "Epoch:  4 | batch:  10000 | train loss: 0.0010 | time: 2297.24 s\n",
            "Epoch:  4 | batch:  20000 | train loss: 0.0001 | time: 2328.73 s\n",
            "Epoch:  4 | batch:  30000 | train loss: 0.6130 | time: 2360.22 s\n",
            "Epoch:  4 | batch:  40000 | train loss: 0.0024 | time: 2391.72 s\n",
            "Epoch:  4 | batch:  50000 | train loss: 0.0015 | time: 2423.21 s\n",
            "Epoch:  4 | batch:  60000 | train loss: 0.1914 | time: 2454.75 s\n",
            "Epoch:  4 | batch:  70000 | train loss: 0.4003 | time: 2486.23 s\n",
            "Epoch:  4 | batch:  80000 | train loss: 0.2225 | time: 2517.72 s\n",
            "Epoch:  4 | batch:  90000 | train loss: 0.2650 | time: 2549.27 s\n",
            "Epoch:  4 | batch:  100000 | train loss: 0.0122 | time: 2580.76 s\n",
            "Epoch:  4 | batch:  110000 | train loss: 0.0357 | time: 2612.25 s\n",
            "Epoch:  4 | batch:  120000 | train loss: 0.1591 | time: 2643.69 s\n",
            "Epoch:  4 | batch:  130000 | train loss: 0.0006 | time: 2675.12 s\n",
            "Epoch:  4 | batch:  140000 | train loss: 0.0004 | time: 2706.56 s\n",
            "Epoch:  4 | batch:  150000 | train loss: 0.0011 | time: 2738.00 s\n",
            "Epoch:  4 | batch:  160000 | train loss: 0.0298 | time: 2769.48 s\n",
            "Epoch:  4 | batch:  170000 | train loss: 0.0086 | time: 2800.95 s\n",
            "Epoch:  4 | batch:  180000 | train loss: 0.6096 | time: 2832.40 s\n",
            "Epoch:  5 | batch:  10000 | train loss: 0.1479 | time: 2864.19 s\n",
            "Epoch:  5 | batch:  20000 | train loss: 0.0178 | time: 2895.63 s\n",
            "Epoch:  5 | batch:  30000 | train loss: 0.4015 | time: 2927.12 s\n",
            "Epoch:  5 | batch:  40000 | train loss: 0.1612 | time: 2958.64 s\n",
            "Epoch:  5 | batch:  50000 | train loss: 0.0001 | time: 2990.11 s\n",
            "Epoch:  5 | batch:  60000 | train loss: 0.1007 | time: 3021.58 s\n",
            "Epoch:  5 | batch:  70000 | train loss: 0.0424 | time: 3053.07 s\n",
            "Epoch:  5 | batch:  80000 | train loss: 0.0003 | time: 3084.55 s\n",
            "Epoch:  5 | batch:  90000 | train loss: 0.0022 | time: 3116.04 s\n",
            "Epoch:  5 | batch:  100000 | train loss: 0.0149 | time: 3147.51 s\n",
            "Epoch:  5 | batch:  110000 | train loss: 0.0009 | time: 3179.04 s\n",
            "Epoch:  5 | batch:  120000 | train loss: 0.0499 | time: 3210.53 s\n",
            "Epoch:  5 | batch:  130000 | train loss: 0.0100 | time: 3241.99 s\n",
            "Epoch:  5 | batch:  140000 | train loss: 0.1160 | time: 3273.44 s\n",
            "Epoch:  5 | batch:  150000 | train loss: 0.0410 | time: 3304.89 s\n",
            "Epoch:  5 | batch:  160000 | train loss: 0.5909 | time: 3336.37 s\n",
            "Epoch:  5 | batch:  170000 | train loss: 0.0100 | time: 3367.85 s\n",
            "Epoch:  5 | batch:  180000 | train loss: 0.0000 | time: 3399.30 s\n",
            "Epoch:  6 | batch:  10000 | train loss: 0.0130 | time: 3431.04 s\n",
            "Epoch:  6 | batch:  20000 | train loss: 0.0010 | time: 3462.55 s\n",
            "Epoch:  6 | batch:  30000 | train loss: 0.0001 | time: 3494.08 s\n",
            "Epoch:  6 | batch:  40000 | train loss: 0.0003 | time: 3525.69 s\n",
            "Epoch:  6 | batch:  50000 | train loss: 0.0030 | time: 3557.19 s\n",
            "Epoch:  6 | batch:  60000 | train loss: 0.1945 | time: 3588.76 s\n",
            "Epoch:  6 | batch:  70000 | train loss: 0.2161 | time: 3620.25 s\n",
            "Epoch:  6 | batch:  80000 | train loss: 1.8806 | time: 3651.76 s\n",
            "Epoch:  6 | batch:  90000 | train loss: 0.0000 | time: 3683.30 s\n",
            "Epoch:  6 | batch:  100000 | train loss: 0.2078 | time: 3714.75 s\n",
            "Epoch:  6 | batch:  110000 | train loss: 0.0008 | time: 3746.20 s\n",
            "Epoch:  6 | batch:  120000 | train loss: 0.0063 | time: 3777.73 s\n",
            "Epoch:  6 | batch:  130000 | train loss: 0.0045 | time: 3809.18 s\n",
            "Epoch:  6 | batch:  140000 | train loss: 0.0004 | time: 3840.64 s\n",
            "Epoch:  6 | batch:  150000 | train loss: 0.0000 | time: 3872.15 s\n",
            "Epoch:  6 | batch:  160000 | train loss: 0.1294 | time: 3903.65 s\n",
            "Epoch:  6 | batch:  170000 | train loss: 0.0017 | time: 3935.08 s\n",
            "Epoch:  6 | batch:  180000 | train loss: 0.0002 | time: 3966.55 s\n",
            "Epoch:  7 | batch:  10000 | train loss: 0.0004 | time: 3998.28 s\n",
            "Epoch:  7 | batch:  20000 | train loss: 0.0048 | time: 4029.74 s\n",
            "Epoch:  7 | batch:  30000 | train loss: 0.0003 | time: 4061.22 s\n",
            "Epoch:  7 | batch:  40000 | train loss: 0.0410 | time: 4092.74 s\n",
            "Epoch:  7 | batch:  50000 | train loss: 0.0000 | time: 4124.25 s\n",
            "Epoch:  7 | batch:  60000 | train loss: 0.0015 | time: 4155.72 s\n",
            "Epoch:  7 | batch:  70000 | train loss: 0.0001 | time: 4187.19 s\n",
            "Epoch:  7 | batch:  80000 | train loss: 0.0026 | time: 4218.68 s\n",
            "Epoch:  7 | batch:  90000 | train loss: 0.0000 | time: 4250.18 s\n",
            "Epoch:  7 | batch:  100000 | train loss: 0.0002 | time: 4281.64 s\n",
            "Epoch:  7 | batch:  110000 | train loss: 0.3910 | time: 4313.13 s\n",
            "Epoch:  7 | batch:  120000 | train loss: 0.0004 | time: 4344.59 s\n",
            "Epoch:  7 | batch:  130000 | train loss: 0.3101 | time: 4376.04 s\n",
            "Epoch:  7 | batch:  140000 | train loss: 0.0744 | time: 4407.51 s\n",
            "Epoch:  7 | batch:  150000 | train loss: 1.4169 | time: 4438.91 s\n",
            "Epoch:  7 | batch:  160000 | train loss: 0.0001 | time: 4470.41 s\n",
            "Epoch:  7 | batch:  170000 | train loss: 0.0033 | time: 4501.87 s\n",
            "Epoch:  7 | batch:  180000 | train loss: 0.0130 | time: 4533.34 s\n",
            "Epoch:  8 | batch:  10000 | train loss: 0.0000 | time: 4565.02 s\n",
            "Epoch:  8 | batch:  20000 | train loss: 0.0061 | time: 4596.48 s\n",
            "Epoch:  8 | batch:  30000 | train loss: 0.0000 | time: 4627.91 s\n",
            "Epoch:  8 | batch:  40000 | train loss: 0.0006 | time: 4659.33 s\n",
            "Epoch:  8 | batch:  50000 | train loss: 0.1709 | time: 4690.73 s\n",
            "Epoch:  8 | batch:  60000 | train loss: 0.0000 | time: 4722.25 s\n",
            "Epoch:  8 | batch:  70000 | train loss: 0.7933 | time: 4753.70 s\n",
            "Epoch:  8 | batch:  80000 | train loss: 0.5573 | time: 4785.16 s\n",
            "Epoch:  8 | batch:  90000 | train loss: 0.7202 | time: 4816.68 s\n",
            "Epoch:  8 | batch:  100000 | train loss: 0.1341 | time: 4848.20 s\n",
            "Epoch:  8 | batch:  110000 | train loss: 0.0001 | time: 4879.71 s\n",
            "Epoch:  8 | batch:  120000 | train loss: 1.2519 | time: 4911.16 s\n",
            "Epoch:  8 | batch:  130000 | train loss: 0.0002 | time: 4942.60 s\n",
            "Epoch:  8 | batch:  140000 | train loss: 0.0741 | time: 4974.06 s\n",
            "Epoch:  8 | batch:  150000 | train loss: 0.0378 | time: 5005.55 s\n",
            "Epoch:  8 | batch:  160000 | train loss: 0.0005 | time: 5037.04 s\n",
            "Epoch:  8 | batch:  170000 | train loss: 0.6506 | time: 5068.50 s\n",
            "Epoch:  8 | batch:  180000 | train loss: 0.0095 | time: 5099.93 s\n",
            "Epoch:  9 | batch:  10000 | train loss: 0.0099 | time: 5131.65 s\n",
            "Epoch:  9 | batch:  20000 | train loss: 0.1180 | time: 5163.10 s\n",
            "Epoch:  9 | batch:  30000 | train loss: 0.3713 | time: 5194.59 s\n",
            "Epoch:  9 | batch:  40000 | train loss: 0.0001 | time: 5226.01 s\n",
            "Epoch:  9 | batch:  50000 | train loss: 0.0002 | time: 5257.46 s\n",
            "Epoch:  9 | batch:  60000 | train loss: 0.0001 | time: 5288.85 s\n",
            "Epoch:  9 | batch:  70000 | train loss: 0.4027 | time: 5320.25 s\n",
            "Epoch:  9 | batch:  80000 | train loss: 0.0001 | time: 5351.71 s\n",
            "Epoch:  9 | batch:  90000 | train loss: 0.5788 | time: 5383.09 s\n",
            "Epoch:  9 | batch:  100000 | train loss: 0.2804 | time: 5414.44 s\n",
            "Epoch:  9 | batch:  110000 | train loss: 0.0001 | time: 5445.85 s\n",
            "Epoch:  9 | batch:  120000 | train loss: 0.0003 | time: 5477.26 s\n",
            "Epoch:  9 | batch:  130000 | train loss: 0.9429 | time: 5508.68 s\n",
            "Epoch:  9 | batch:  140000 | train loss: 0.0000 | time: 5540.08 s\n",
            "Epoch:  9 | batch:  150000 | train loss: 0.1837 | time: 5571.48 s\n",
            "Epoch:  9 | batch:  160000 | train loss: 0.0023 | time: 5602.89 s\n",
            "Epoch:  9 | batch:  170000 | train loss: 0.0014 | time: 5634.32 s\n",
            "Epoch:  9 | batch:  180000 | train loss: 0.0117 | time: 5665.81 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wTIg4icHDo4"
      },
      "source": [
        "# 评估"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UWvl_k1L4Es",
        "outputId": "8a5926da-effb-415c-8854-880c6811c49e"
      },
      "source": [
        "true = 0.0\r\n",
        "all = 0.0\r\n",
        "for i, val_batch in enumerate(val_iter):\r\n",
        "    pred_y = torch.max(model(val_batch.text), 1)[1].cpu().data.numpy()\r\n",
        "    real_y = val_batch.label.cpu().data.numpy()\r\n",
        "    true += float((pred_y == real_y).astype(int).sum())\r\n",
        "    all += float(len(real_y))\r\n",
        "accuracy = true / all\r\n",
        "print(accuracy)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.93295\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyUmSVbeHGXT"
      },
      "source": [
        "# 预测"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSLxbjUBL7Tm"
      },
      "source": [
        "result=[['label']]\r\n",
        "for i, test_batch in enumerate(test_iter):\r\n",
        "    for label in torch.max(model(test_batch.text), 1)[1].cpu().data.numpy():\r\n",
        "       result.append([label]) \r\n",
        "with open(\"/content/drive/MyDrive/data/News classification/tmp/result.csv\", \"a\", newline='', encoding='utf-8') as file:\r\n",
        "    writer = csv.writer(file ,delimiter=',')\r\n",
        "    writer.writerows(result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}