{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 导包"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport torch.optim as optim\nimport torch.utils.data as Data\n\n!pip install torchtext==0.4\nfrom torchtext import data\nfrom torchtext.vocab import Vectors\nfrom torchtext.data import Iterator, BucketIterator\n\nfrom sklearn.model_selection import train_test_split\n\nimport csv\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport zipfile\nimport os\nimport shutil","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting torchtext==0.4\n  Downloading torchtext-0.4.0-py3-none-any.whl (53 kB)\n\u001b[K     |████████████████████████████████| 53 kB 129 kB/s eta 0:00:01     |████████████▍                   | 20 kB 2.7 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.4) (4.55.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchtext==0.4) (1.7.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from torchtext==0.4) (1.15.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext==0.4) (2.25.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext==0.4) (1.19.5)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.4) (1.26.2)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.4) (2020.12.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.4) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.4) (2.10)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torchtext==0.4) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->torchtext==0.4) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch->torchtext==0.4) (0.6)\nInstalling collected packages: torchtext\n  Attempting uninstall: torchtext\n    Found existing installation: torchtext 0.8.0a0+cd6902d\n    Uninstalling torchtext-0.8.0a0+cd6902d:\n      Successfully uninstalled torchtext-0.8.0a0+cd6902d\nSuccessfully installed torchtext-0.4.0\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 固定种子"},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(2021)","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 定义超参数"},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 8\nEPOCHS = 2\nFIX_LENGTH = 40\nN_CLASS=2\nLR=0.001\nOUT_CHANNEL=20\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 划分训练集验证集"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\n\ntrain, val = train_test_split(train_df, test_size=0.2)\ntrain.to_csv(\"./train.csv\", index=False)\nval.to_csv(\"./val.csv\", index=False)","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 构建Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('./train.csv')\nvalid_data = pd.read_csv('./val.csv')\ntest_data = pd.read_csv('../input/quora-insincere-questions-classification/test.csv')\n\ntokenize_text = lambda x: x.split()\n# fix_length指定了每条文本的长度，截断补长\nTEXT = data.Field(sequential=True, tokenize=tokenize_text, lower=True, fix_length=FIX_LENGTH)\nLABEL = data.Field(sequential=False, use_vocab=False)\n\ndef get_dataset(csv_data, id_field, text_field, label_field, test=False):\n\t# id数据对训练在训练过程中没用，使用None指定其对应的field\n    fields = [(\"id\", None), (\"text\", text_field), (\"label\", label_field)]       \n    examples = []\n\n    if test:\n        # 如果为测试集，则不加载label\n        for text in tqdm(csv_data['question_text']):\n            examples.append(data.Example.fromlist([None, text, None], fields))\n    else:\n        for text, label in tqdm(zip(csv_data['question_text'], csv_data['target'])):\n            examples.append(data.Example.fromlist([None, text, label], fields))\n    return examples, fields\n\n\n# 得到构建Dataset所需的examples和fields\ntrain_examples, train_fields = get_dataset(train_data, None, TEXT, LABEL)\nvalid_examples, valid_fields = get_dataset(valid_data, None, TEXT, LABEL)\ntest_examples, test_fields = get_dataset(test_data, None, TEXT, None, test=True)\n\n# 构建Dataset数据集\ntrain = data.Dataset(train_examples, train_fields)\nvalid = data.Dataset(valid_examples, valid_fields)\ntest = data.Dataset(test_examples, test_fields)","execution_count":5,"outputs":[{"output_type":"stream","text":"1044897it [00:18, 58004.90it/s]\n261225it [00:04, 56780.91it/s]\n100%|██████████| 375806/375806 [00:05<00:00, 65624.24it/s]\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"# 构建迭代器"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_iter, val_iter = BucketIterator.splits(\n        (train, valid),\n        batch_sizes=(BATCH_SIZE, BATCH_SIZE),\n        device = DEVICE, # 如果使用gpu，此处将-1更换为GPU的编号\n        sort_key=lambda x: len(x.text), # the BucketIterator needs to be told what function it should use to group the data.\n        sort_within_batch=False,\n        repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n)\n\ntest_iter = Iterator(test, batch_size=BATCH_SIZE, device=DEVICE, sort=False, train=False, sort_within_batch=False, repeat=False) # train=False可以保证顺序不变","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 加载词向量"},{"metadata":{"trusted":true},"cell_type":"code","source":"def un_zip(file_name):\n    \"\"\"unzip zip file\"\"\"\n    zip_file = zipfile.ZipFile(file_name)\n    if os.path.isdir('./'+file_name.split('/')[-1].split('.')[0]):\n        pass\n    else:\n        os.mkdir('./'+file_name.split('/')[-1].split('.')[0])\n    for names in zip_file.namelist():\n        zip_file.extract(names, './'+file_name.split('/')[-1].split('.')[0])\n    zip_file.close()","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"un_zip('../input/quora-insincere-questions-classification/embeddings.zip')\nshutil.rmtree('./embeddings/wiki-news-300d-1M')\nshutil.rmtree('./embeddings/GoogleNews-vectors-negative300')\nshutil.rmtree('./embeddings/paragram_300_sl999')","execution_count":8,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-9038376aaf7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mun_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/quora-insincere-questions-classification/embeddings.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-9db558674517>\u001b[0m in \u001b[0;36mun_zip\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, member, path, pwd)\u001b[0m\n\u001b[1;32m   1617\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1619\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1621\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m              \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1691\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;34m\"\"\"copy data from file-like object fsrc to file-like object fdst\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mZIP_DEFLATED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMIN_READ_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m             self._eof = (self._decompressor.eof or\n\u001b[1;32m   1008\u001b[0m                          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectors = Vectors(name='./embeddings/glove.840B.300d/glove.840B.300d.txt')\nTEXT.build_vocab(train, vectors=vectors)\nweight_matrix = TEXT.vocab.vectors","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 定义模型"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.embedding = nn.Embedding(len(TEXT.vocab), 300)     \n        self.embedding.weight.data.copy_(weight_matrix)\n\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, OUT_CHANNEL, (2, 300)), # input_channel(=1), output_channel, (filter_height, filter_width), stride=1\n            nn.ReLU(),\n            nn.MaxPool2d((FIX_LENGTH-1, 1)), # ((filter_height, filter_width))\n        )\n        self.out = nn.Linear(OUT_CHANNEL, N_CLASS)\n\n    def forward(self, X): # X: [fix_length, batch_size]\n        batch_size = np.shape(X)[1]\n        embeds = self.embedding(X)  # [fix_length, batch_size, embedding_size]  \n        embeds = self.embedding(X).transpose(0, 1) # [batch_size, sequence_length, embedding_size]\n        embeds = embeds.unsqueeze(1) # [batch, channel(=1), sequence_length, embedding_size]\n        \n        conved = self.conv(embeds)\n        flatten = conved.view(batch_size, -1)\n        y = self.out(flatten)\n        return y\n\nmodel = CNN().to(DEVICE)\nloss_fc = nn.CrossEntropyLoss().to(DEVICE)\noptimizer = optim.Adam(model.parameters(), lr=LR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 训练"},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(EPOCHS):\n    for batch in tqdm(train_iter):\n        optimizer.zero_grad()\n        pred = model(batch.text)\n\n        loss = loss_fc(pred, batch.label)\n        loss.backward()\n        optimizer.step()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 评估"},{"metadata":{"trusted":true},"cell_type":"code","source":"true = 0.0\nall = 0.0\nfor val_batch in tqdm(val_iter):\n    pred_y = torch.max(model(val_batch.text), 1)[1].cpu().data.numpy()\n    real_y = val_batch.label.cpu().data.numpy()\n    true += float((pred_y == real_y).astype(int).sum())\n    all += float(len(real_y))\naccuracy = true / all\n\nprint(accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 预测"},{"metadata":{"trusted":true},"cell_type":"code","source":"result=[]\nfor i, test_batch in enumerate(test_iter):\n    for j, label in enumerate(torch.max(model(test_batch.text), 1)[1].cpu().data.numpy()):\n            result.append([test_data['qid'][i*BATCH_SIZE+j],label])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"header=['qid','prediction']\nwith open(\"./result.csv\", \"a\", newline='', encoding='utf-8') as file:\n    writer = csv.writer(file)\n    writer.writerow(header)\n    writer.writerows(result)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}