# 1 n元语法

- 语言模型：语句 $s = w_1 w_2 … w_l $的先验概率：
  $$
  p(s) = p(w_1)\times p(w_2|w_1)\times p(w_3|w_1w_2)\times… \times p(w_l|w_1…w_{l-1}) =\prod ^l_{i=1}p(w_i|w_1…w_{i-1})
  $$
  当 i=1 时，$p(w_1|w_0) = p(w_1)$。
  - $w-i $可以是字、词、短语或词类等等，称为统计基元。通常以“词”代之。
  - $w_i$的概率由$ w_1, …, w_{i-1} $决定，由特定的一组$w_1, …, w_{i-1} $构成的一个序列，称为$ w_i $的历史。
  
- n元语法（n元文法）：

  - 将 $w_1 w_2 … w_{i-1} $映射到等价类 $E(w_1 w_2 … w_{i-1})$，使等价类的数目远远小于原来不同历史基元的数目。则有：
  $$
  p(w_i|w_1 ,w_2 ,…, w_{i-1})= p(w_i|E(w_1 ,w_2 ,…, w_{i-1})
  $$
  - 划分等价类：将两个历史映射到同一个等价类，当且仅当这两个历史中的最近 n-1 个基元相同
  
  - - 当 n=1 时，即出现在第 i 位上的基元$ w_i $独立于历史。一元文法也被写为 uni-gram 或 monogram； 
    - 当 n=2 时, 2-gram (bi-gram) 被称为1阶马尔可夫链；
    - 当 n=3 时, 3-gram(tri-gram)被称为2阶马尔可夫链
    
  - 为了保证条件概率在 i=1 时有意义，同时为了保证句子内所有字符串的概率和为 1，即$\sum_s p(s)=1$ ，可以在句子首尾两端增加两个标志: $<BOS> w_1 w_2 … w_m <EOS>$。不失一般性，对于n>2 的 n-gram，p(s) 可以分解为:
    $$
    p(s)=\prod_{i=1}^{m+1}p(w_i|w_{i-n+1}^{i-1})
    $$
    其中，$w_i^j$ 表示词序列 $w_i … w_j$，$w_{i-n+1}$ 从$ w_0 $开始，$w_0$ 为 <BOS>，$w_{m+1}$为 <EOS>。
  
  - $$
    p(w_i|w_{i-1})=\frac{c(w_{i-1}w_i)}{\sum_{w_i}c(w_{i-1}w_i)}
    $$
    $$
    p(w_i|w^{i-1}_{i-n+1})=\frac{c(w_{i-n+1}^i)}{\sum_{w_i}c(w_{i-n+1}^i)}
    $$
    求和表达式$\sum_{w_i}c(w_{i-n+1}^i)$等于计算历史$c(w_{i-n+1}^{i-1})$的数目。
  
    概率由训练语料获得
    
  - 例题：
  ![在这里插入图片描述](https://img-blog.csdnimg.cn/20210210115637507.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)


# 2 语言模型性能评价

- 概率：对于一个平滑过的概率为$p(w_i|w_{i-n+1}^{i-1})$的n元语法模型，句子的概率$p(s)=\prod_{i=1}^{l+1}p(w_i|w_{i-n+1}^{i-1})$，测试集的概率$p(T)=\prod^{l_T}_{i=1}p(t_i)$
- 交叉熵：$H_p(T)=-\frac{1}{W_T}log_2p(T)$
- 困惑度：$PP_T(T)=2^{H_p(T)}$

# 3 数据平衡

## 3.1 问题的提出

- 生词会让概率变为0
- 基本思想：调整最大似然估计的概率值,使零概率增值，使非零概率下调，“劫富济贫”，消除零概率，改进模型的整体正确率。
- 基本目标：测试样本的语言模型困惑度越小越好。
- 基本约束：$\sum_{w_i}p(w_i|w_1,w_2,…,w_{i-1})=1$

## 3.2 加法平滑方法

$$
p_{add}(w_i|w^{i-1}_{i-n+1})=\frac{\delta+c(w_{i-n+1}^i)}{\delta·|V|+\sum_{w_i}c(w_{i-n+1}^i)}
$$

假设每一个n元语法发生的次数比实际多发生$\delta$次，$0\leq\delta\leq1$

$|V|$为词汇表单词量

## 3.3 古德-图灵(Good-Turing)估计法

基本思想：修改训练样本中事件的实际计数，使样本中(实际出现的)不同事件的概率之和小于1，剩余的概率量分配给未见概率（均分）。

假设 N 是原来训练样本数据的大小， $n_r$ 是在样本中正好出现 r 次的事件的数目(此处事件为 n-gram)，即出现 1 次的n-gram有$ n_1$个，出现 2 次的 n-gram 有 $n_2$个, ……，出现 r 次的有 $n_r $个。

那么，$N=\sum^{\infty}_{r=1}n_rr$，由于$N=\sum^{\infty}_{r=0}n_rr^*=\sum^{\infty}_{r=0}n_{r+1}{(r+1)}$，则
$$
p_r=\frac{r^*}{N}
$$

## 3.4 Katz平滑方法

基本思想：当某一事件在样本中出现的频率大于阈值K (通常取 K 为0 或1)时，运用最大似然估计的减值法来估计其概率，否则，使用低阶的，即 (n-1)gram 的概率替代 n-gram 概率，而这种替代需受归一化因子$\alpha$的作用    (对于每个计数 r > 0 的n元文法的出现次数减值, 把因减值而节省下来的剩余概率根据低阶的 (n-1)gram 分配给未见事件)   。

对于一个出现次数为 $r=c(w_{i-1}^i)$的 2元语法 ，使用如下公式计算修正的概率：

![<img src="D:\picture\image-20210206094231876.png" alt="image-20210206094231876" style="zoom: 67%;" />](https://img-blog.csdnimg.cn/20210210115726746.png#pic_center)

$$
\alpha(w_{i-1})=\frac{1-\sum_{w_{i:r>0}}p_{katz}(w_i|w_{i-1})}{\sum_{w_{i:r=0}}p_{ML}(w_i)}
$$
其中，$p_{ML}(w_i)$表示 $w_i $的最大似然估计概率。这个公式的意思是，所有具有非零计数 r 的 2元语法都根据折扣率$d_r(0<d_r<1)$被减值了，折扣率 $d_r$近似等于 r*/r，减值由Good-Turing估计方法测。

## 3.5 绝对减值法

基本思想：从每个计数 r 中减去同样的量，剩余的概率量由未见事件均分

设 R 为所有可能事件的数目(当事件为 n-gram 时，如果统计基元为词，且词汇集的大小为 L, 则 $R=L^n$ )那么，样本出现了 r 次的事件的概率可以由如下公式估计：

![<img src="D:\picture\image-20210206095312015.png" alt="image-20210206095312015" style="zoom:67%;" />](https://img-blog.csdnimg.cn/20210210115744704.png#pic_center)


其中，$n_0 $为样本中未出现的事件的数目。b 为减去的常量，$b \leq 1$。$b(R - n_0)/N $是由于减值而产生的剩余概率量。N 为样本中出现了r 次的事件总次数：$n_r\times r$。

b 为自由参数，可以通过留存数据方法求得 b 的上限为

$$
b\leq\frac{n_1}{n_1+2n_2}<1
$$

## 3.6 算法总结

- Good-Turing 法：对非0事件按公式削减出现的次数，节留出来的概率均分给0概率事件。
- Katz 后退法：对非0事件按Good-Turing法计算减值，节留出来的概率按低阶分布分给0概率事件。
- 绝对减值法：对非0事件无条件削减某一固定的出现次数值，节留出来的概率均分给0概率事件