# 0 摘要
最先进的命名实体识别系统在很大程度上依赖手工制作的功能和特定领域的知识，以便从可用的受监督的小型培训资料库中有效学习。在本文中，我们介绍了两种新的神经体系结构：一种基于双向LSTM和条件随机字段，另一种采用受移位减少解析器启发的基于过渡的方法构造和标记片段。我们的模型依赖于两个有关单词的信息资源：从监督语料库学习的基于字符的单词表示形式和从无注释语料库学习的无监督单词表示形式。我们的模型无需使用任何特定于语言的知识或资源（例如，地名词典），就可以使用四种语言在NER中获得最新的性能。

# 1 介绍
命名实体识别（NER）是一个具有挑战性的学习问题。一方面，在大多数语言和领域中，只有很少量的监督培训数据可用。另一方面，对于可以作为名称的单词种类几乎没有限制，因此很难从这种小的数据样本中进行概括。结果，精心构造的正交特征和特定于语言的知识资源（例如地名词典）被广泛用于解决此任务。不幸的是，在新的语言和新的领域中开发特定于语言的资源和功能非常昂贵，这使得NER难以适应。从无注释语料库中进行无监督学习提供了另一种策略，可以从少量监督中获得更好的泛化。但是，即使是广泛依赖于无监督功能的系统（也使用这些功能来增强而不是替换手工设计的功能（例如，有关特定语言中的大写字母样式和字符类的知识）和专门的知识资源（例如，地名词典）。

在本文中，我们介绍了针对NER的神经体系结构，除了少量的有监督的训练数据和未标记的语料库之外，它不使用任何语言特定的资源或功能。我们的模型旨在捕获两种直觉。首先，由于名称通常由多个标记组成，因此对每个标记的标记决策进行联合推理非常重要。我们在这里比较两个模型
- 双向LSTM，其上有一个顺序条件随机层（LSTM-CRF;第2节）
- 一个新的模型，该模型使用一种算法来构造和标记输入句子的块，该算法的灵感来自基于转换的解析，其状态由堆栈LSTM表示(S-LSTM;第3节)。

其次，“作为一个名称”的标记级证据包括正字法证据(标记为名称的单词看起来像什么?)和分布证据(标记的单词在语料库中往往出现在哪里?)为了捕获正字法灵敏度，我们使用基于字符的单词表示模型来捕获分布灵敏度，我们将这些表示与分布表示相结合。我们的词汇表示将这两者结合起来，而dropout训练被用来鼓励模型学习信任这两种证据来源(第4节)。

在英语、荷兰语、德语和西班牙语中进行的实验表明，我们能够使用荷兰语、德语和西班牙语中的LSTM-CRF模型获得最先进的NER性能，在英语中也非常接近最先进的性能，而不需要任何手工设计的函数或地名辞典(第5节)。基于迁移的算法同样在几种语言中超过了以前发布的最佳结果，尽管它的性能不如LSTM-CRF模型。

# 2 LSTM-CRF模型
我们提供了LSTMs和CRFs的简要描述，并提出了一个混合标记体系结构。
## 2.1 LSTM
循环神经网络(RNNs)是一类操作顺序数据的神经网络。它们将向量序列$(x_1,x_2,...,x_n)$作为输入。并返回另一个序列$(h_1,h_2,...,h_n)$表示输入中每一步序列的一些信息。虽然从理论上讲，RNN可以学习长期依赖关系，但在实践中它们不能这样做，而且倾向于偏向于它们在序列中最近的输入。长期短期记忆网络(LSTMs)被设计成通过整合记忆单元来解决这个问题，并且已经被证明可以捕获长期依赖关系。他们使用几个门来控制输入到记忆细胞的比例，以及先前状态遗忘的比例。我们使用以下实现:
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221100412883.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
其中$\sigma$是元素的sigmoid函数，$\Theta$是元素的乘积。

对于一个给定的句子$(x_1,x_2,...,x_n)$包含n个词，每个词都用一个$d$维向量表示，LSTM在每个词$t$处计算句子左上下文的表示$h_t^{\rightarrow}$。当然，生成右上下文的表示$h_t^{\leftarrow}$也应该添加有用的信息。这可以使用第二个LSTM来实现，它反向读取相同的序列。我们将前者称为正向LSTM，将后者称为反向LSTM。这是两个不同的网络，参数不同。这对正向和反向LSTM被称为双向LSTM 。

使用该模型的词的表示是通过连接词的左右上下文表示，$h_t=[h_t^{\rightarrow};h_t^{\leftarrow}]$。这些表示有效地包括上下文中的单词表示，这对许多标记应用程序都很有用。

## 2.2 CRF标记模型
一个非常简单但非常有效的标记模型是使用$h_t$作为特征，为每个输出$y_t$做出独立的标记决定。尽管该模型在POS标签等简单问题上取得了成功，但当输出标签之间存在很强的依赖性时，其独立的分类决策就会受到限制。NER就是这样一个任务，因为描述标签可解释序列的“语法”强加了几个硬约束(例如，I-PER不能跟在B-LOC后面;具体请参见2.4)，这将不可能以独立假设进行建模。

因此，我们不是独立建模标记决策，而是使用条件随机场联合建模它们。对于输入句子
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221101228697.png#pic_center)
我们认为$P$是双向LSTM网络输出的分数矩阵。$P$的大小为$n \times k$，其中$k$为不同标签的个数，$P_{i,j}$为该句子中第$i$个单词的第$j$个标签的得分。为了一系列的预测
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221101512264.png#pic_center)
我们将其得分定义为

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221101529490.png#pic_center)
其中$A$是过渡得分的矩阵，因此$A_{i,j}$表示从标签$i$到标签$j$的过渡得分。 $y_0$和$y_n$是句子的开始和结束标记，我们将其添加到可能的标记集中。因此，$A$是大小为$k + 2$的方阵。

所有可能的标签序列上的softmax产生序列y的概率：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221101717173.png#pic_center)
在训练过程中，我们会最大化正确标签序列的对数概率
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221101752473.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
其中，$Y_X$表示句子$X$的所有可能的标签序列（甚至是那些不验证IOB格式的标签序列）。从上面的表述中，很明显，我们鼓励我们的网络生成有效的输出标签序列。在解码时，我们预测输出序列，该序列将获得以下最大分数：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221101857221.png#pic_center)
由于我们只是对输出之间的二进制图交互进行建模，因此等式1中的总和和等式2中的最大后序列y∗都可以使用动态规划进行计算。

## 2.3 参数化和训练
与每个标记（即$P_{i,j}$）的每决策相关联的分数被定义为使用双向LSTM计算的词法上下文嵌入之间的点积，并结合了双语法例兼容性分数（即$A_{y,y'}$）。此体系结构如图1所示。圆圈表示观察到的变量，菱形是其父代的确定性函数，双圆圈是随机变量。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221102242218.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 图1 ：网络的主要架构。单词嵌入被提供给双向LSTM。 $l_i$代表单词$i$及其左侧上下文，$r_i$代表单词$i$及其右侧上下文。将这两个向量串联起来，就可以得出词$i$在其上下文$c_i$中的表示。

因此，该模型的参数是双字母组兼容性分数A的矩阵，以及产生矩阵P的参数，即双向LSTM的参数，线性特征权重和词嵌入。与第2.2部分一样，让$x_i$表示标记句子中每个单词的单词嵌入顺序，并为其附加标签。我们将在第4节中讨论如何对嵌入流程进行建模。单词嵌入的序列作为双向LSTM的输入给出，该LSTM返回每个单词的左右上下文的表示形式，如2.1中所述。

这些表示是连接($c_i$)和线性投影到一个层，其大小等于不同标签的数量。我们不使用该层的softmax输出，而是使用前面描述的CRF来考虑相邻标签，从而为每个单词$y_i$产生最终预测。此外，我们观察到在$c_i$层和CRF层之间添加一个隐藏层略微改善了我们的结果。该模型报告的所有结果都包含了这个外层。对参数进行训练，使在给定观察到的单词的注释语料库中观察到的NER标签序列的等式1最大。

## 2.4 标签计划
命名实体识别的任务是为句子中的每个单词分配一个命名实体标签。单个命名实体可以在一个句子中跨越多个标记。句子通常用IOB格式表示(内部、外部、开始)，其中每个标记如果是命名实体的开始，则被标记为B-label，如果是在命名实体内部但不是命名实体中的第一个标记，则被标记为I-label，否则为O。然而,我们决定使用IOBES标签计划,IOB的变种常用的命名实体识别,编码单信息实体(S)和明确的结束标志着命名实体(E)。使用这种方案,标记一个单词是$I-label$高信任度缩小了随后的词$I-label$或$E-label$的选择,然而，IOB 仅仅是能够确定后续的词不能内部的另一个标签。使用像 IOBES 这样更具表现力的标记方案可以略微提高模型性能。然而，我们没有观察到 IOB 标记方案有显著的改善。

# 3 基于过渡的分块模型
作为上一节中讨论的LSTM-CRF的替代方法，我们探索一种新的体系结构，该体系结构使用类似于基于过渡的依存关系分析的算法对输入序列进行分块和标记。此模型直接构造多令牌名称的表示形式（例如，名称Mark Watney组成一个表示形式）。

该模型依赖于堆栈数据结构来增量构造输入块。为了获得用于预测后续动作的堆栈的表示，我们使用Stack-LSTM。 其中LSTM增强了“堆栈指针”。当顺序LSTM从左到右建模序列时，堆栈LSTM允许嵌入一堆对象，这些对象既可以添加到（使用推入操作），也可以删除（使用弹出操作）。这使Stack-LSTM可以像一个堆栈一样工作，以保持其内容的“摘要嵌入”。为了简单起见，我们将此模型称为Stack-LSTM或S-LSTM模型。

最后，我们将感兴趣的读者带到原始论文以获取有关StackLSTM模型的详细信息，因为在本文中，我们仅通过下一节介绍的基于过渡的新算法使用相同的体系结构。

## 3.1 分块算法
我们设计了一个过渡清单，如图2所示，该清单的灵感来自基于过渡的解析器，尤其是标准弧解析器。在该算法中，我们利用两个堆栈（分别表示完成的块和暂存空间的指定输出和堆栈）和一个包含尚未处理的单词的缓冲区。转换清单包含以下转换：SHIFT转换将一个字从缓冲区移至堆栈，OUT转换将一个字从缓冲区直接移至输出堆栈，而REDUCE（y）转换则从缓冲区顶部弹出所有项。堆栈创建一个“块”，并用标签y标记该块，并将该块的表示形式压入输出堆栈。当堆栈和缓冲区均为空时，该算法完成。图2中描述了该算法，该图显示了处理句子$Mark\ Watney\  visited\ Mars.$所需的操作顺序。

给定堆栈，缓冲区和输出的当前内容以及所采取操作的历史记录，可以通过定义每个时间步上各个操作的概率分布来对模型进行参数化。我们使用堆栈LSTM来计算它们中的每一个的固定维嵌入，并对它们进行串联以获得完整的算法状态。该表示用于定义每个时间步长可能采取的行动的分布。训练模型以在给定输入句子的情况下最大化参考动作序列（从标记的训练语料库中提取）的条件概率。为了在测试时标记一个新的输入序列，贪婪地选择最大概率动作，直到算法达到终止状态为止。尽管不能保证找到全局最优值，但实际上它是有效的。由于每个令牌要么直接移至输出（1个动作），要么先移至堆栈，然后再移至输出（2个动作），所以长度为n的序列的动作总数最大为2n。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221103717315.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 3：使用Stack-LSTM模型处理$Mark\ Watney\  visited\ Mars.$的过渡序列。

值得注意的是这种算法的本质该模型使其与所使用的标记方案无关，因为它直接预测标记的块。

## 3.2 代表标记的块
当执行REDUCE（y）操作时，该算法将一系列标记（连同其向量嵌入）从堆栈移至输出缓冲区，作为一个完整的块。为了计算该序列的嵌入，我们在其组成标记的嵌入以及代表所标识的块类型（即y）的标记上运行了双向LSTM。该函数以$g（u，...，v，r_y）$的形式给出，其中$r_y$是标签类型的学习型嵌入。因此，对于每个生成的带标签的块，无论其长度如何，输出缓冲区都包含单个矢量表示。

# 4 输入字词嵌入
我们两个模型的输入层都是单个单词的矢量表示。从有限的NER训练数据中学习单词类型的独立表示是一个困难的问题：太多的参数无法可靠地估计。由于许多语言都具有正字法或形态学证据，即某物是一个名称（或不是名称），因此我们需要对单词的拼写敏感的表示形式。因此，我们使用一种模型来构造单词的表示形式，该单词表示形式是由单词组成的字符表示形式（4.1）。我们的第二个直觉是，名称可能会变化很大，它们在大型语料库的常规上下文中出现。因此，我们使用从大型语料库中学到的对词序敏感的嵌入（4.2）。最后，为了防止模型过于依赖一个表示或另一个表示，我们使用了 dropout 训练，发现这对于良好的泛化性能至关重要（4.3）。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221104308860.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 图4：“Mars”一词的字符嵌入已提供给双向LSTM。我们将它们的最后输出连接到查找表的嵌入中，以获得该单词的表示形式。

## 4.1 基于字符的单词模型
我们的工作与大多数以前的方法的一个重要区别是，我们在训练的同时学习字符级功能，而不是手工设计有关单词的前缀和后缀信息。学习字符级嵌入的优势是可以学习特定于手头任务和领域的表示形式。已经发现它们对于形态丰富的语言很有用，并且对于语音部分标记和语言建模或依赖项解析之类的任务，可以解决词汇不足的问题。

图4描述了我们的体系结构，该体系结构从单词的字符生成单词嵌入。随机初始化的字符查找表包含每个字符的嵌入。对应于单词中每个字符的字符嵌入以向前和向后LSTM的正反顺序给出。从单词的字符派生的单词的嵌入是双向LSTM的向前和向后表示的串联。然后，此字符级表示形式与单词查找表中的单词级表示形式连接在一起。在测试过程中，在查询表中没有嵌入的单词将映射到UNK嵌入。为了训练UNK嵌入，我们用UNK嵌入替换单例，概率为0.5。在我们所有的实验中，向前和向后字符LSTM的隐藏维数均为25，这导致我们基于字符的单词表示的维数为50。

像RNN和LSTM这样的循环模型能够编码很长的序列，但是，它们的表示偏向于最新输入。结果，我们期望正向LSTM的最终表示形式是单词后缀的准确表示形式，而反向LSTM的最终状态则是其前缀的更好表示形式。已经提出了替代方法（最著名的是卷积网络）来从字符中学习单词的表示。但是，卷积网络旨在发现其输入的位置不变特征。虽然这适用于许多问题，例如图像识别（猫可以出现在图片中的任何地方），但我们认为重要信息是位置相关的（例如，前缀和后缀编码的信息与词干不同），这使LSTM成为先验者用于对单词及其字符之间的关系进行建模的函数类。

## 4.2 预训练的嵌入
我们使用预训练的词嵌入来初始化我们的查询表。我们观察到使用预训练词嵌入技术对随机初始化的词嵌入技术有明显的改进。嵌入是使用skip-n-gram（进行预训练的，后者是word2vec的变体，它说明了词的顺序。这些嵌入在训练期间进行了微调。

使用西班牙语Gigaword版本3，莱比锡语料库集合，2010年机器翻译研讨会的德语单语培训数据和英语Gigaword版本4（包括LA Times和NY Times部分）对西班牙语，荷兰语，德语和英语的单词嵌入进行训练对于英语，我们使用嵌入尺寸为100，对于其他语言，我们使用嵌入尺寸为64，最小词频截止为4，窗口大小为8。

## 4.3 Dropout训练
初步实验表明，与预训练的单词表示形式结合使用时，字符级嵌入不会改善我们的整体性能。为了鼓励模型同时依赖于两种表示，我们使用丢包训，在图1中双向LSTM的输入之前，将丢包掩码应用于最终的嵌入层。使用dropout 模型后我们模型的性能（请参见表5）。

# 5 实验
本节介绍了我们用于训练模型的方法，在各种任务上获得的结果以及网络配置对模型性能的影响。

## 5.1 训练
对于所介绍的两个模型，我们使用反向传播算法训练网络，每个算法都使用反向梯度更新算法来更新我们的参数，一次使用随机梯度下降（SGD），学习速率为0.01，梯度修剪为5.0。已经提出了几种提高SGD性能的方法，例如Adadelta或Adam。尽管我们观察到使用这些方法的收敛速度更快，但是它们都没有使用梯度裁剪的SGD更好。

我们的LSTM-CRF模型对尺寸设置为100的向前和向后LSTM使用单层。调整此尺寸不会显着影响模型性能。我们将dropout率设置为0.5。使用较高的费率会对我们的结果产生负面影响，而使用较低的费率则会导致更长的培训时间。

堆栈LSTM模型为每个堆栈使用两层，每层的尺寸为100。合成函数中使用的动作的嵌入每个都有16个维度，输出嵌入的维度是20个维度。我们尝试了不同的dropout率，并使用每种语言的最佳dropout率报告了分数。这是一个适用的贪婪模型局部最优动作直到整个句子被处理为止，可以通过波束搜索（或探索训练）获得进一步的改进。
## 5.2 数据集
我们在不同的数据集上测试模型以进行命名实体识别。为了证明我们的模型能够泛化为不同语言的能力，我们在CoNLL-2002和CoNLL2003数据集上展示了结果，这些数据集包含英语，西班牙语，德语的独立命名实体标签和荷兰人。所有数据集都包含四种不同类型的命名实体：不属于先前三个类别中任何一个的位置，人员，组织和其他实体。尽管POS标签可用于所有数据集，但我们并未将其包含在模型中。除了将英语NER数据集中的每个数字替换为零之外，我们没有执行任何数据集预处理。

## 5.3 结果
表1列出了我们与其他英语命名实体识别模型的比较。为了使我们的模型与其他模型之间的比较合理，我们报告了使用或不使用外部标记数据（例如，地名词典和知识库）的其他模型的得分。我们的模型不使用地名词典或任何外部标签资源。我们的LSTM-CRF模型优于所有其他系统，包括使用外部标记数据（例如地名词典）的系统。除了Chiu和Nichols（2015）提出的模型之外，我们的StackLSTM模型还优于以前没有整合外部特征的所有模型。

表2、3和4分别展示了我们与其他模型相比在NER上针对德国，荷兰和西班牙文的结果。在这三种语言上，LSTM-CRF模型的性能明显优于所有以前的方法，包括使用外部标记数据的方法。唯一的例外是荷兰语，其中Gillick等人的模型可以通过利用其他NER数据集中的信息来取得更好的效果。与不使用外部数据的系统相比，Stack-LSTM还始终如一地提供最新（或接近）结果。

从表中可以看出，Stack-LSTM模型更依赖于基于字符的表示来获得竞争性能。我们假设LSTM-CRF模型不需要正交信息，因为它可以从双向LSTM中获得更多上下文信息。但是，Stack-LSTM模型逐个消耗单词，并且在对单词进行分块时仅依赖于单词表示。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221110224398.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 表1：英语NER结果（CoNLL-2003测试集）。 *表示使用外部标签数据进行训练的模型


![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221110238548.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 表2：德语NER结果（CoNLL-2003测试集）。 *表示使用外部标签数据进行训练的模型
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221110250192.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 表3：荷兰语NER（CoNLL-2002测试仪）。 *表示使用外部标签数据进行训练的模型


![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221110303224.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 表4：西班牙语NER（CoNLL-2002测试集）。 *表示使用外部标签数据进行训练的模型



## 5.4 网络架构
我们的模型包含几个组件，我们可以对其进行调整以了解它们对整体性能的影响。我们探索了CRF，字符级别表示形式，我们的预训练单词嵌入，dropout 在我们的LSTM-CRF模型中的影响。我们观察到，对词嵌入进行预训练可以使我们在F1中的整体表现获得最大的改善，即+7.31。 CRF层使我们增加了+1.79，而使用dropout 导致相差+1.17，最后学习字符级单词嵌入会导致大约+0.74的增加。对于Stack-LSTM，我们执行了一组类似的实验。表5给出了不同体系结构的结果。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221110541549.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 表5：我们使用不同配置的模型得出的英语NER结果。 “预训练”是指包括预训练单词嵌入的模型，“字符”是指包括基于字符的单词建模的模型，“dropout”是指包括dropout率的模型。


# 6 结论
本文介绍了两种用于序列标记的神经体系结构，即使与使用外部资源（例如地名词典）的模型相比，它们也可以提供标准评估设置中报告的最佳NER结果。

我们模型的一个关键方面是，它们可以通过简单的CRF体系结构，或使用基于过渡的算法来显式构造和标记输入块，从而对输出标签依赖性进行建模。单词表示形式对于成功也至关重要。我们既使用预训练的单词表示形式，又使用“基于字符的”表示形式来捕获形态学和正字法信息。为了防止学习者过分地依赖一个表示形式类，使用了dropout。