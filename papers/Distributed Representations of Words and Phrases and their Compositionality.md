# 0 摘要
最近推出的SG模型是一种学习高质量分布式向量表示的有效方法，能够捕捉大量精确的句法和语义词关系。在本文中，我们提出了几个扩展，既提高了向量的质量和训练速度。通过对频繁单词的子抽样，我们获得了显著的加速，也学习了更多规则的单词表示。我们还描述了一种简单的替代方案，称为负抽样的分层softmax。

单词表示的固有局限性是它们对单词顺序的漠不关心以及它们无法表示惯用语。例如，“加拿大”和“航空”的含义不能轻易组合以获得“加拿大航空”。受此示例的启发，我们提出了一种用于在文本中查找短语的简单方法，并表明学习数百万个短语的良好矢量表示形式是可行的。

# 1 导言
向量空间中单词的分布式表示有助于学习算法通过对相似单词进行分组来在自然语言处理任务中实现更好的性能。此想法此后已成功应用于统计语言建模。后续工作包括自动语音识别和机器翻译以及各种NLP任务的应用。

最近，引入了Skip-gram模型，这是一种从大量非结构化文本数据中学习单词的高质量矢量表示的有效方法。与大多数以前使用的用于学习单词向量的神经网络体系结构不同，Skipg-ram模型的训练不涉及密集矩阵乘法。这使培训非常高效：经过优化的单机实现可以在一天中培训超过1000亿个单词。

使用神经网络计算的单词表示形式非常有趣，因为所学习的向量显式地编码了许多语言规律性和模式。令人惊讶的是，许多这些模式都可以表示为线性平移。例如，向量计算的结果（“马德里”）-（“西班牙”）+（“法国”）比任何其他单词向量都更接近（“巴黎”）。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221082819913.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)

> 图1：Skip-gram模型架构。训练目标是学习擅长预测附近单词的单词向量表示。



在本文中，我们介绍了原始Skip-gram模型的几个扩展。我们表明，在训练过程中对频繁出现的单词进行二次采样会显着提高速度（大约2倍-10倍），并提高了不太频繁出现的单词表示的准确性。此外，我们提出了一种噪声对比估计（NCE）的简化变体，用于训练Skip-gram模型，与先前工作中使用的更复杂的分层softmax相比，该算法可更快地训练频繁词并提供更好的矢量表示形式。

单词表示受其不能表示不是单个单词组成的惯用短语的限制。例如，“波士顿环球报”是报纸，因此它不是“波士顿”和“地球”含义的自然组合。因此，使用向量来表示整个短语可以使Skip-gram模型更具表现力。旨在通过组合单词向量来表示句子含义的其他技术（例如递归自动编码器）也将从使用短语向量而不是单词向量中受益。

从基于单词的模型到基于短语的模型的扩展相对简单。首先，我们使用数据驱动的方法识别大量的短语，然后在训练过程中将这些短语视为单独的标记。为了评估短语向量的质量，我们开发了一个包含单词和短语的类比推理任务测试集。我们的测试集中有一个典型的类比对是“蒙特利尔”：“蒙特利尔加拿大人” ：：“多伦多”：“多伦多枫叶”。如果与（“蒙特利尔加拿大人”）-（“蒙特利尔”）+ （“多伦多”）最接近的表示形式是（“多伦多枫叶”），则认为已正确回答。

最后，我们描述了Skip-gram模型的另一个有趣的特性。我们发现简单的向量加法通常可以产生有意义的结果。例如，（“俄罗斯”）+ （“河”）接近（“伏尔加河”），而（“德国”）+ （“资本”）接近（“柏林”） 。这种组合性表明，可以通过对词向量表示形式使用基本的数学运算来获得对语言的理解程度。

# 2 Skip-gram
Skip-gram模型的训练目标是找到可用于预测句子或文档中周围单词的单词表示形式。更正式地，给定训练词$w_1,w2,w3,...,w_T$的序列，Skip-gram模型的目标是使平均对数概率最大化
![在这里插入图片描述](https://img-blog.csdnimg.cn/2021022108361888.png#pic_center)

其中$c$是训练上下文的大小（可以是中心单词$w_t$的函数）。 $c$越大，产生的训练示例越多，因此可以导致更高的准确性，却会浪费训练时间。基本的Skip-gram公式使用softmax函数定义$p(w_{t + j} | w_t)$：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221083744804.png#pic_center)

其中$v_w$和 $v_w'$感知$w$的“输入”和“输出”矢量表示，$W$是词汇表中单词的数量。这种公式不切实际，因为计算$\nabla log\ p(w_O | w_I)$的成本与$W$成正比，而$W$通常很大($10^5-10^7$项)。

## 2.1 分层Softmax

完整softmax的计算有效近似是分层softmax。主要优势在于，无需评估神经网络中的$W$个输出节点以获得概率分布，而是仅需要评估约$log_2(W)$个节点。

分级softmax使用输出层的二叉树表示，以$W$字作为叶，对于每个节点，显式地表示其子节点的相对概率。它们定义了一个随机遍历，将概率赋给单词。

更精确地，每个词$w$可以通过从树的根部开始的适当路径到达。令$n(w，j)$为从根到$w$的路径上的第$j$个节点，令$L(w)$为该路径的长度，因此$n(w，1)=根$，$n(w，L (w))= w$。另外，对于任何内部节点$n$，令$ch(n)$为$n$的任意固定子项；如果$x$为true，则令$[[x]]$为1，否则为-1。然后，分层softmax定义$p(w_O | w_I)$如下：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221084649188.png#pic_center)

其中$\sigma(x)= 1 /(1 + exp(-x))$。可以验证$\sum^W_{w=1}p(w | w_I)=1$。这意味着计算$log\ p(w_O | w_I)$和$\nabla log\ p(w_O | w_I)$的成本与$L(w_O)$成正比，而平均而言，它不是大于$logW$。而且，与skip-gram的标准softmax公式不同，该标准softmax公式为每个单词$w$分配两个表示$v_w$和 $v'_w$，而分层softmax公式具有针对每个单词$w$的一个表示$v_w$和针对二叉树的每个内部节点$n$的一个表示$v'_n$。

分层softmax使用的树的结构对性能有很大影响。在我们的工作中，我们使用二进制霍夫曼树，因为它为频繁出现的单词分配了短代码，从而可以进行快速训练。之前已经观察到，通过频率将单词分组在一起是一种非常简单的基于神经网络的语言模型加速技术。

## 2.2 负采样
它是分层softmax的另一种选择，它是噪声对比估计(NCE)。 NCE认为，好的模型应该能够通过逻辑回归将数据与噪声区分开。这类似于hinge损失，他们通过对数据进行高于噪声的排序来训练模型。

虽然可以证明NCE使softmax的对数概率近似最大化，但是Skip-gram模型仅关注学习高质量矢量表示，因此只要矢量表示保持其质量，我们就可以自由简化NCE。我们通过目标定义负采样（NEG）
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221085541931.png#pic_center)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221085557988.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 2：国家及其首都城市的1000维Skip-gram矢量的二维PCA投影。该图说明了该模型自动组织概念并隐式学习概念之间的关系的能力，因为在培训期间我们没有提供有关首都含义的任何监督信息。

它用于替换Skip-gram目标中的每个$log\ P(w_O | w_I)$项。因此，任务是使用逻辑回归从噪声分布$P_n(w)$中区分出目标词$w_O$和图，其中每个数据样本有$k$个负样本。我们的实验表明，$k$值在5–20的范围内对于小型训练数据集很有用，而对于大型数据集，$k$可以小至2–5。负采样与NCE之间的主要区别在于NCE需要采样和噪声分布的数值概率，而负采样仅使用采样。虽然NCE大约使softmax的对数概率最大化，但是此属性对于我们的应用并不重要。

NCE和NEG都将噪声分布$P_n(w)$作为自由参数。我们研究了$P_n(w)$的许多选择，发现提高到3/4次幂的unigram分布$U（w）$（即$U（w）^{3/4} / Z$）明显优于unigram和均匀分布，对于NCE和NEG，我们尝试了包括语言建模在内的所有任务（此处未报告）。

## 2.3 常用词的二次采样
在非常大型的语料库中，最常见的词很容易出现数亿次（例如，“in”，“the”和“a”）。此类单词通常比稀有单词提供更少的信息价值。例如，虽然 Skip-gram 模型受益于观察“法国”和“巴黎”的同时出现，但它受益于观察“法国”和“ the”的频繁同时出现，因为几乎每个单词经常在带有“the”的句子中出现。这个想法也可以反方向应用。在训练了数百万个示例后，常用词的向量表示形式不会发生明显变化。

为了解决稀有词和常见词之间的不平衡问题，我们使用了一种简单的子采样方法：将训练集中的每个词$w_i$丢弃，其概率由公式计算：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221090221520.png#pic_center)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221090236412.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)

> 表1：类似推理任务的各种Skip-gram 300维模型的准确性。 NEG-k代表负采样，每个正样本有k个负样本； NCE代表噪声对比估计，HS-Huffman代表带有基于频率的Huffman码的分层Softmax。

其中$f（w_i）$是单词$w_i$的频率，$t$是选定的阈值，通常在10-5附近。我们之所以选择这种二次采样公式，是因为它在保留频率排名的同时，主动对频率大于$t$的单词进行二次采样。尽管此次采样公式是通过启发式方式选择的，但我们发现它在实践中效果很好。它将加速学习，甚至显着提高了稀有单词的学习向量的准确性，如以下各节所示。

# 3 实验结果
在本节中，我们评估训练词的分层Softmax（HS），噪声对比估计，负采样和二次采样。任务包含类似类比，例如$“ Germany”:“ Berlin”::“ France”:？$，可以通过找到向量x来解决，使得(x)最接近$(“ Berlin”)-(“ Germany ”)+ (“France”)$根据余弦距离（我们从搜索中丢弃输入的单词）。如果x为“ Paris”，则认为该特定示例已正确回答。该任务分为两大类：句法类比（例如“快速”：“迅速” ：：“慢”：“缓慢”）和语义类比（例如，国家与首都城市的关系）。

为了训练Skip-gram模型，我们使用了由各种新闻报道组成的大型数据集（一个内部Google数据集，有10亿个单词）。我们从词汇表中丢弃了在训练数据中出现少于5次的所有单词，从而形成了692K大小的词汇表。表1列出了词类比测试集上各种Skip-gram模型的性能。该表显示，在类比推理任务上，负采样的性能优于分层Softmax，其性能甚至比噪声对比估计稍好。频繁单词的二次采样可将训练速度提高数倍，并使单词表示更加准确。

可以说，skip-gram模型的线性使它的向量更适合于这种线性类比推理。标准S形递归神经网络（高度非线性）所学习的向量随着训练数据量的增加而在此任务上得到了显着改善，这表明非线性模型也偏爱单词表示的线性结构。

# 4 学习短语
如前所述，许多短语的含义不是其各个单词含义的简单组合。为了学习短语的矢量表示，我们首先找到经常出现且在其他情况下很少出现的单词。例如，“New York Times”和“Toronto Maple Leafs”在训练数据中被替换为唯一标记，而二元组“ this is”将保持不变。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221090946702.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 表2：短语的类比推理任务示例（完整的测试集有3218个示例）。目标是使用前三个计算第四个短语。我们的最佳模型在该数据集上的准确性达到72％。

这样，我们可以形成许多合理的短语，而不会大大增加词汇量；从理论上讲，我们可以使用所有n-gram训练Skip-gram模型，但这会占用大量内存。先前已经开发出许多技术来识别文本中的短语。但是，将它们进行比较超出了我们的工作范围。我们决定使用一种简单的数据驱动方法，其中基于单字和二元组计数形成短语，使用
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221091059651.png#pic_center)
$\delta$用作折现系数，可防止形成太多由非常少见的单词组成的短语。然后将分数高于所选阈值的二元词用作短语。通常，我们以递减的阈值对训练数据进行2-4次传递，从而允许形成由多个单词组成的较长短语。我们使用涉及短语的新类比推理任务评估短语表示的质量。表2显示了此任务中使用的五类类比的示例。该数据集可在$web^2$上公开获得。
## 4.1 词组Skip-Gram结果
从与先前实验相同的新闻数据开始，我们首先构建基于短语的训练语料库，然后使用不同的超参数训练几个Skip-gram模型。和以前一样，我们使用向量维数300和上下文大小5。此设置在短语数据集上已经实现了良好的性能，并允许我们快速比较“负采样”和“分层Softmax”，无论是否对频繁标记进行二次采样。结果总结在表3中。

结果表明，即使在k = 5的情况下，负采样也能获得可观的精度，而k = 15时却能获得更好的性能。令人惊讶的是，虽然我们发现在不进行二次采样的情况下进行训练时，分层Softmax会降低性能，但是当我们对频繁出现的单词进行下采样时，它成为性能最佳的方法。这表明，至少在某些情况下，二次采样可以提高训练速度，并可以提高准确性。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221091355400.png#pic_center)
> 表3：短语类比数据集上的Skip-gram模型的准确性。对模型进行了新闻数据集中约10亿个单词的训练。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221091425984.png#pic_center)
> 表4：使用两个不同的模型，最接近给定短语的实体的示例。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221091451624.png#pic_center)
> 表5：使用逐元素加法的扇区组成。使用最佳Skip-gram模型，显示了两个向量之和最接近的四个标记。

为了最大程度地提高短语类比任务的准确性，我们通过使用约330亿个单词的数据集来增加训练数据的数量。我们使用层次化softmax，维数为1000以及整个句子作为上下文。这导致模型的准确性达到72％。当我们将训练数据集的大小减小到6B个字时，我们获得了66％的较低准确性，这表明大量的训练数据至关重要。

为了进一步了解不同模型所学习的表示形式有何不同，我们使用各种模型手动检查了不常用短语的最近邻居。在表4中，我们显示了这种比较的示例。与先前的结果一致，似乎短语的最佳表示是通过具有分层softmax和二次采样的模型学习的。

# 5 附加成分
我们证明了通过Skip-gram模型学习的单词和短语表示呈现线性结构，这使得使用简单的向量算法执行精确的类比推理成为可能。有趣的是，我们发现Skip-gram表示呈现出另一种线性结构，这种结构使得有意义地组合单词成为可能，方法是对它们的向量表示进行元素式加法。表5说明了这一现象。

向量的可加性可以通过检测训练目标来解释。词向量与softmax非线性的输入呈线性关系。当单词向量被训练来预测句子中周围的单词时，这些向量可以被看作是一个单词出现在上下文中的分布。这些值与输出层计算的概率以对数形式相关，因此两个词向量的总和与两个上下文分布的乘积相关。这个乘积在这里作为AND函数:被两个词向量分配高概率的词将有高概率，而其他的词将有低概率。因此，如果“Volga River”与“Russian”和“River”频繁地出现在同一句话中，那么这两个词向量相加就会得到一个与“Volga River”的向量接近的特征向量。

# 6 与已发表的词表示的比较
许多以前致力于基于神经网络的词汇表示的作者已经发表了他们的结果模型，以供进一步使用和比较。我们从网上下载了他们的词向量。已经在单词类比任务上评估了这些单词表示，其中Skip-gram模型以最大的幅度实现了最佳性能。

![在这里插入图片描述](https://img-blog.csdnimg.cn/2021022109201376.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 表6：给出了各种众所周知的模型的最接近标记的示例以及使用超过300亿个训练词对短语进行训练的Skip-gram模型。空单元格意味着单词不在词汇表中。

为了更深入地了解所学习向量的质量差异，我们通过在表6中显示不常见单词的最近邻居来提供经验比较。这些示例表明，在大型语料库上训练的大型Skip-gram模型明显胜过其他所有语言学习表示的质量模型。这可以部分归因于以下事实：该模型已经训练了约300亿个单词，这比先前工作中使用的典型大小多了大约两个到三个数量级的数据。有趣的是，尽管训练集要大得多，但是Skip-gram模型的训练时间只是以前模型体系结构所需时间复杂度的一小部分。

# 7 结论
这项工作有几个关键贡献。我们展示了如何使用Skip-gram模型训练单词和短语的分布式表示形式，并演示了这些表示形式具有线性结构，这使得精确的类比推理成为可能。本文介绍的技术还可以用于训练的CBOW模型。

由于计算效率高的模型架构，我们成功地对模型进行了比以前发布的模型多几个数量级的数据训练。这极大地提高了所学单词和短语表示的质量，特别是对于稀有实体。我们还发现，频繁单词的二次采样不仅可以加快训练速度，而且可以显着改善不常见单词的表示方式。我们论文的另一个贡献是负采样算法，它是一种非常简单的训练方法，可以学习准确的表示形式，尤其是对于频繁单词。

训练算法的选择和超参数的选择是特定于任务的决策，因为我们发现不同的问题具有不同的最佳超参数配置。在我们的实验中，影响性能的最关键的决定是模型架构的选择，向量的大小，子采样率和训练窗口的大小。

这项工作的一个非常有趣的结果是，仅使用简单的向量加法就可以在某种程度上有意义地组合词向量。学习本文介绍的短语表示的另一种方法是简单地用单个标记表示短语。这两种方法的组合提供了一种强大而简单的方法，即如何在不使计算复杂性最小的情况下表示更长的文本。因此，我们的工作可以看作是对尝试使用递归矩阵矢量运算表示短语的现有方法的补充。

我们基于本文描述的技术使用于训练单词和短语向量的代码可作为一个开源项目使用。