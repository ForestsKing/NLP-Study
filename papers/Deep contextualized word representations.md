# 0 摘要
我们介绍了一种新型的深层上下文化词表示形式，该模型既可以建模单词使用的复杂特征（例如语法和语义），又可以建模这些用法在语言上下文之间的变化方式（即建模多义性）。我们的词向量是深度双向语言模型（biLM）内部状态的学习功能，双向语言模型已在大型文本语料库上进行了预训练。我们表明，可以很容易地将这些表示形式添加到现有模型中，并通过六个具有挑战性的NLP问题（包括问题回答，文本蕴含和情感分析）显着改善现有技术。我们还提出了一项分析，表明暴露出预训练网络的深层内部至关重要，从而允许下游模型混合不同类型的半监督信号。


# 1 介绍
预训练的单词表示是许多神经语言理解模型的关键组成部分。但是，学习高质量的表示形式可能会充满挑战。理想情况下，他们应该同时建模单词使用的复杂特征（例如语法和语义），以及这些使用如何在语言环境中变化（即建模多义性）。在本文中，我们介绍了一种新型的深层上下文词表示形式，它可以直接解决这两个挑战，可以轻松地集成到现有模型中，并在各种考虑到的情况下，在一系列具有挑战性的语言理解问题中，都可以显着改善现有技术。

我们的表示形式与传统单词类型嵌入的区别在于，为每个标记分配了一个表示形式，该表示形式取决于整个输入句子。我们使用从双向LSTM派生的向量，该双向LSTM在大型文本语料库上以耦合语言模型（LM）目标进行训练。因此，我们称它们为ELMo（语言模型的嵌入）表示形式。与以前的学习语境化词向量的方法不同，ELMo表示很深，因为它们是biLM所有内部层的函数。更具体地说，我们为每个最终任务学习了堆叠在每个输入词上方的向量的线性组合，与仅使用顶层LSTM层相比，这显着提高了性能。

以这种方式组合内部状态允许非常丰富的单词表示。使用内在评估，我们表明较高级别的LSTM状态捕获了单词含义的上下文相关方面（例如，可以不经修改就可以使用它们在监督的单词义消歧任务上表现良好），而较低级别的状态则对语法方面进行了建模（例如，它们可以用来进行词性标记）。同时暴露所有这些信号是非常有益的，允许学习的模型选择对每个最终任务最有用的半监督类型。

大量的实验表明，ELMo表示在实践中非常有效。我们首先表明，可以轻松地将它们添加到现有模型中，以解决六种多样且具有挑战性的语言理解问题，包括文本蕴涵，问题回答和情感分析。在每种情况下，单独添加ELMo表示都可以显着改善现有技术，包括减少多达20％的相对误差。对于可以直接比较的任务，ELMo优于CoVe，后者使用神经机器翻译编码器计算上下文表示。最后，对ELMo和CoV e的分析显示，深层表示优于arXiv，它们仅来自LSTM顶层。我们训练有素的模型和代码可公开获得，并且我们期望ELMo将为许多其他NLP问题带来类似的收益。
# 2 ELMo：语言模型的嵌入
不同于最广泛使用的词嵌入，ELMo词表示是整个输入句子的功能，如本节所述。它们是在具有字符卷积的两层biLM上计算的（第2.1节），是内部网络状态的线性函数（第2.2节）。这种设置使我们可以进行半监督学习，其中对biLM进行了大规模的预训练（第2.4节），并且可以轻松地将其纳入各种现有的神经NLP体系结构中（第2.3节）。

## 2.1 双向语言模型
给定N个令牌序列$(t_1,t_2,...,t_N)$，前向语言模型通过对历史$(t_1,...,t_{k-1})$中令牌$t_k$的概率进行建模来计算序列的概率：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221112724559.png#pic_center)
最近的最新神经语言模型计算上下文无关的令牌表示$x^{LM}_ k$（通过令牌嵌入或CNN字符），然后通过前LSTM的L层。在每个位置 k 处，每个LSTM层输出一个上下文相关的表示${h^{LM}_{k,j}}^{\rightarrow}$，其中$j = 1,...,L$顶层LSTM输出${h^{LM}_{k,L}}^{\rightarrow}$，L用于预测带有Softmax层的下一个标记$t_{k + 1}$。

后向LM与前向LM相似，不同之处在于后向LM反向运行序列，并在给定将来上下文的情况下预测前一个令牌：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221113130655.png#pic_center)
可以以类似于前向LM的方式来实现，其中L层深度模型中的每个后向LSTM层j都会产生$t_k$($t_{k+1},...,t_N$)的表示${h^{LM}_{k,j}}^{\leftarrow}$

biLM结合了前向和后向LM。我们的公式共同使向前和向后方向的对数可能性最大化：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221113407957.png#pic_center)
我们将标记表示$（\Theta_x$）和Softmax层$（\Theta_s）$的参数沿正向和反向绑定，同时在每个方向上为LSTM保留单独的参数。我们在方向之间共享一些权重，而不是使用完全独立的参数。在下一部分中，我们通过引入一种新的方法来学习单词表示，这是biLM层的线性组合，这与以前的工作有所不同。

## 2.2 ELMo
ELMo是biLM中中间层表示形式的任务特定组合。对于每个令牌$t_k$，L层biLM计算一组2L +1个表示形式
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221113649831.png#pic_center)
对于每个biLSTM层，其中$h_{k,0}^{LM}$是令牌层，$h^{LM}_{k,j}=[{h^{LM}_{k,j}}^{\rightarrow};{h^{LM}_{k,j}}^{\leftarrow}]$。

为了包含在下游模型中，ELMo将R中的所有层折叠为一个向量$ELMo_k = E(R_k;\Theta_e)$。在最简单的情况下，ELMo仅选择顶层E$E(R_k)=h^{LM}_{k,j}$更一般而言，我们计算所有biLM层的特定于任务的权重：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221114118899.png#pic_center)
其中，$s^{task}$是softmax归一化的权重，标量参数$\gamma^{task}$允许任务模型缩放整个ELMo向量。 $\gamma$对于协助优化过程具有实际重要性。考虑到每个biLM层的激活具有不同的分布，因此在某些情况下还有助于在加权之前将层归一化（应用于每个biLM层。

## 2.3 将biLM用于受监督的NLP任务

给定预训练的biLM和目标NLP任务的受监督体系结构，使用biLM改进任务模型是一个简单的过程。我们只需运行biLM并记录每个单词的所有层表示。然后，让最终任务模型学习这些表示的线性组合，如下所述。

首先考虑没有biLM的监督模型的最低层。大多数受监督的NLP模型在最低层共享一个通用体系结构，从而使我们能够以一致，统一的方式添加ELMo。给定一系列标记$（t_1,...,t_N）$，通常使用预先训练的单词嵌入和可选的基于字符的表示为每个标记位置形成上下文无关的标记表示$x_k$。然后，该模型通常使用双向RNN，CNN或前馈网络形成上下文相关的表示形式$h_k$。

为了将ELMo添加到监督模型中，我们首先冻结biLM的权重，然后将ELMo矢量$ELMo^{task}_k$与$x_k$连接起来，然后将ELMo增强表示$[x_k; ELMo^{task}_k]$传递到任务RNN中。对于某些任务（例如SNLI，SQuAD），通过引入另一组输出特定线性权重并将$h_k$替换为$[h_k; ELMo^{task}_k]$，我们还观察到了进一步的改进，将ELMo包含在任务RNN的输出中。由于监督模型的其余部分保持不变，因此这些添加可能会在更复杂的神经模型的背景下发生。其中双关注层紧跟在biLSTM之后，或者是共参考分辨率实验，其中聚类模型位于biLSTM之上。

最后，我们发现向ELMo添加适量的dropout是有益的，在某些情况下，通过在损失中添加$\lambda ||w||_2^2$来规范ELMo权重。这会在ELMo权重上施加电感性偏差，以保持接近所有biLM层的平均值。


## 2.4 预训练的双向语言模型架构
本文中经过预训练的biLM进行了修改以支持两个方向的联合训练，并在LSTM层之间添加了剩余连接。我们专注于这项工作中的大型biLM。强调了使用biLM而非前瞻性LM和大规模训练的重要性。

为了在总体语言模型困惑与模型大小和下游任务的计算要求之间保持平衡，同时保持纯粹基于字符的输入表示形式，我们将CNN-BIG-LSTM中的所有嵌入和隐藏尺寸减半。最终模型使用L = 2 biLSTM层，具有4096个单位和512个尺寸的投影以及从第一层到第二层的剩余连接。上下文不敏感类型表示使用2048个字符的n-gram卷积过滤器，后跟两个高速公路层和线性投影直到512表示。结果，biLM为每个输入令牌提供了三层表示，包括由于纯字符输入而在训练集之外的那些表示。相反，传统的词嵌入方法仅在固定词汇表中为令牌提供一层表示。

在1B Word Benchmark上训练了10个时期后，平均前向和后向困惑为39.7，而前向CNN-BIG-LSTM为30.0。通常，我们发现向前和向后的困惑程度大致相等，而向后的困惑程度则略低。

经过预训练后，biLM可以计算任何任务的表示形式。在某些情况下，在特定于域的数据上对biLM进行微调会导致困惑度显着下降和下游任务性能的提高。这可以看作是biLM的一种域转移。结果，在大多数情况下，我们在下游任务中使用了经过微调的biLM。

# 3 评估
表1显示了ELMo在六组基准NLP任务中的性能。在考虑的每项任务中，只需添加ELMo即可建立新的最新结果，相对于强大的基础模型而言，相对误差减少了6-20％。这是跨多种集合模型体系结构和语言理解任务的非常普遍的结果。在本节的其余部分中，我们将提供各个任务结果的简要概述。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221115315307.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 表1：在六个基准NLP任务中，ELMo增强型神经模型与最新的单个模型基线的测试集比较。性能指标因任务而异-SNLI和SST-5的准确性； SQuAD，SRL和NER的$F_1$； Coref的平均$F_1$。由于NER和SST-5的测试量较小，因此我们报告了五次使用不同随机种子的运行的平均值和标准差。 “increase”列列出了相对于基线的绝对改善和相对改善。

- 问题回答
  斯坦福问答数据集（SQuAD）包含100K +人群来源的问答对，其中答案是给定Wikipedia段落中的跨度。我们的基线模型是双向注意力流模型的改进版本。它在双向注意组件之后添加了一个自我注意层，简化了一些合并操作，并用LSTM代替了门控循环单元。将ELMo添加到基线模型后，测试集F1从81.1％提高到了85.8％，提高了4.7％，相对基线降低了24.9％的相对误差，并且将整个单一模型的最新技术水平提高了1.4％。由11名成员组成的团队将$F_1$提升到87.4，这是提交排行榜时的最新水平。ELMo的增长率为4.7％，也比将CoVe添加到基线模型的1.8％的提高要大得多。
- 文字蕴含
  文本蕴含是在给定“前提”的情况下确定“假设”是否正确的任务。斯坦福大学自然语言推理（SNLI）语料库提供了大约550K假设/前提对。我们的基线是ESIM序列模型。使用biLSTM编码前提和假设，然后使用矩阵关注层，局部推理层，另一个biLSTM推理组成层，最后在输出层之前进行池化操作。总体而言，在ESIM模型中添加ELMo可以使五个随机种子平均提高0.7％的准确性。五人合奏将整体准确度提高到89.3％，超过了之前的合奏最好的88.9％。
- 语义角色标签
  语义角色标记（SRL）系统为句子的谓词-自变量结构建模，通常被描述为回答“谁对谁做了什么”。将SRL建模为BIO标签问题，并使用了8层深biLSTM，前后方向交错。如表1所示，当将ElMo添加重新实现中时，单一模型测试集$F_1$从81.4％跃升了3.2％至84.6％，这是OntoNotes基准上的最新技术，甚至比之前的最佳合奏结果提高了1.2 ％。
- 共指解析
  共指解析是将文本中提及相同底层现实世界实体的提及聚类的任务。我们的基线模型是基于端到端跨度的神经模型。它使用biLSTM和注意力机制来首先计算跨度表示形式，然后应用softmax提及排名模型来查找共指链。在我们使用CoNLL 2012共享任务中的OntoNotes共指注进行的实验中，添加ELMo将平均$F_1$值从67.2提高到70.4，提高了3.2％，建立了一个新的技术水平，再次优于先前的最佳$F_1$的总成绩为1.6％。
- 命名实体提取
CoNLL 2003 NER任务由来自路透社RCV1语料库的新闻通讯社组成，并用四种不同的实体类型（PER，LOC，ORG，MISC）标记。遵循最新的最先进系统，基线模型使用了预训练的词嵌入，基于字符的CNN表示，两个biLSTM层和条件随机电场（CRF）损失。如表1所示，我们的ELMo增强型biLSTM-CRF在五次运行中的$F_1$平均达到92.22％。我们的系统与Peters等人先前的技术水平之间的主要区别是，我们允许任务模型学习所有biLM层的加权平均值，而Peters等人仅使用顶层biLM层。使用所有层而不只是最后一层可以提高跨多个任务的性能。
- 情绪分析
  斯坦福情感树库中的细粒度情感分类任务（SST-5）涉及从电影评论中选择五个标签中的一个（从非常否定到非常肯定）来描述一个句子。句子包含各种语言现象（例如习语）和复杂的句法构造（例如，否定词），模型难以学习。我们的基线模型是的双注意力分类网络（BCN）。 当使用CoV e嵌入进行增强时，它也保持了先前的最新技术成果。在BCN模型中用ELMo替换CoV e可使绝对精度比现有技术提高1.0％。


# 4 分析
本节提供了一种消融分析，以验证我们的主要主张并阐明ELMo表示的一些有趣方面。4.1显示，在下游任务中使用深层上下文表示相对于仅使用顶层的先前工作可提高性能，而不管它们是由biLM还是MT编码器产生的，并且ELMo表示可提供最佳的整体性能。4.3探索了在biLM中捕获的不同类型的上下文信息，并使用两次内在评估表明语法信息在较低的层上更好地表示，而语义信息在较高的层上更好地表示，这与MT编码器一致。这也表明我们的biLM始终提供比CoV e更丰富的表示形式。此外，我们分析了在任务模型中包含ELMo的敏感度（4.2节），训练集大小（4.4节），并可视化了任务中ELMo学习的权重（4.5节）。

## 4.1 替代层加权方案
等式1有许多替代方案可用于组合biLM层。先前有关上下文表示的工作仅使用最后一层，无论是来自biLM还是MT编码器。正则化参数$\lambda$的选择也很重要，因为诸如$\lambda＝ 1$之类的大值有效地将加权函数减小到整个层上的简单平均值，而较小的值（例如，$\lambda＝ 0.001$）允许层权重变化。

表2比较了SQuAD，SNLI和SRL的这些替代方案。与仅使用最后一层相比，包括所有层的表示可以提高总体性能，而包括最后一层的上下文表示可以提高整个基线的性能。例如，在SQuAD的情况下，仅使用最后一个biLM层可使开发$F_1$比基线提高3.9％。平均所有biLM层而不是仅使用最后一层可将$F_1$再提高0.3％（将“仅最后一个”与$\lambda＝ 1$列进行比较），并允许任务模型学习单个层权重将$F_1$再提高0.2％（$\lambda＝ 1$与$\lambda＝ 0.001$）。在大多数情况下，对于ELMo，较小的$\lambda$是首选，尽管对于NER（一项训练集较小的任务），结果对$\lambda$不敏感（未显示）。

总体趋势与CoV e相似，但与基线相比增长幅度较小。对于SNLI，与仅使用最后一层相比，对所有$\lambda＝ 1$的层进行平均可以将开发精度从88.2提高到88.7％。与仅使用最后一层相比，对于$\lambda＝ 1$情况，SRL $F_1$的边际收益增加了0.1％，达到82.2。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221120557177.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 表2：使用BiLM的所有层（具有不同的正则化强度$\lambda$选择）仅将顶层与SQuAD，SNLI和SRL的开发集性能进行比较。


## 4.2 哪里包括ELMO？
本文中的所有任务体系结构都仅将词嵌入作为最低层biRNN的输入。但是，我们发现在特定任务架构中的biRNN输出中包含ELMo可以改善某些任务的总体结果。如表3所示，在SNLI和SQuAD的输入层和输出层都包括ELMo可以仅在输入层上有所改善，但对于SRL（以及共参考分辨率，未显示），当仅将其包含在输入层时，性能最高。对此结果的一种可能解释是SNLI和SQuAD架构都在biRNN之后使用了注意层，因此在这一层引入ELMo可使模型直接参与biLM的内部表示。就SRL而言，特定于任务的上下文表示可能比biLM中的上下文表示更为重要。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221120607692.png#pic_center)
> 表3：在受监管模型的不同位置包含ELMo时，SQuAD，SNLI和SRL的开发集性能。

## 4.3 biLM的表示可以捕获哪些信息？
由于添加ELMo可以比单独的单词向量提高任务性能，因此biLM的上下文表示形式必须对通常不用于单词向量的NLP任务有用的信息进行编码。直观上，biLM必须使用上下文来消除单词的含义。考虑“ play”，一个高度多义的单词。表4的顶部列出了使用GloVe向量“play”的最近邻居。它们分布在语音的多个部分（例如，“played”，“plating”为动词，以及“player”，“game”为名词），但集中在与运动有关的“play”中。相反，最下面的两行使用biLM在源语句中的“play”上下文表示形式，显示了SemCor数据集（参见下文）中最接近的相邻语句。在这些情况下，biLM可以消除源句中的语音部分和词义。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221121558445.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 表4：使用GloVe和biLM中的上下文嵌入来“play”的最近邻居。

可以使用与Belinkov等类似的上下文表示的内在评估来量化这些观察结果。为了隔离由biLM编码的信息，这些表示用于直接进行细粒度词义消歧（WSD）任务和POS标记任务的预测。使用这种方法，还可以与CoVe进行比较，并跨越每个单独的层。

**单词词义消歧**对于一个句子，我们可以使用biLM表示来预测目标单词的意义，使用简单的1 nearest neighbor方法。为此，我们首先使用biLM来计算我们的训练语料库SemCor 3.0中所有单词的表示，然后取每种感觉的平均表示。在测试时，我们再次使用biLM来计算给定目标词的表示，并从训练集中获取最近邻的意义，对于训练期间未观察到的引理，则返回到WordNet的第一个意义。

表5比较了Raganato et al. (2017b)使用Raganato et al. (2017a)的评估框架在Raganato et al. (2017a)的同一套四组测试集上的WSD结果。总体而言，biLM顶层表示的$F_1$为69.0，在WSD上优于第一层。这与使用手工制作特性的最先进的特定于WSD的监督模型和使用辅助粗粒度语义标签和POS标签训练的特定于任务的biLSTM 竞争。CoVe biLSTM层遵循与biLM层相似的模式(与第一层相比，第二层的整体性能更高);然而，我们的biLM优于CoVe biLSTM，后者落后于WordNet第一感觉基线。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221122122766.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 表5:全词细粒度WSD $F_1$。对于CoVe和biLM，我们报告第一层和第二层biLSTMs的分数。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221124843268.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 表6：PTB的测试集POS标记准确性。对于CoVe和biLM，我们报告第一层和第二层biLSTM的得分。


**POS标记**为了检查biLM是否捕获基本语法，我们使用上下文表示作为线性分类器的输入，该分类器使用Penn树库(PTB)的华尔街日报部分预测POS标记。由于线性分类器只增加了少量的模型容量，这是对biLM表示的直接测试。与WSD类似，biLM表示与仔细调整的、任务特定的biLSTMs竞争。然而，与WSD不同的是，使用第一层biLM的精度高于顶层，与深度biLSTMs在多任务训练中的结果一致和MT 。CoVe POS标记的准确性遵循与biLM相同的模式，而且就像WSD一样，biLM比CoVe编码器实现了更高的准确性。

这些实验证实了biLM中的不同层代表不同类型的信息，并解释了为什么包括所有的biLM层对下游任务的最高性能是重要的。此外，与CoVe相比，biLM的表述对水务署和POS标签的适用性更强，这有助于说明为什么ELMo在下游任务中表现优于CoVe。

## 4.4 样品效率
将ELMo添加到模型中，无论是为了达到最新性能的参数更新次数，还是整个训练集的大小，都大大提高了采样效率。例如，在没有ELMo的486个训练周期之后，SRL模型达到了最大发展$F_1$。添加ELMo之后，该模型在第10个阶段超过了基线最大值，达到相同水平的表现所需的更新数量相对减少了98％。

此外，与没有ELMo的模型相比，增强型ElMo的模型更有效地使用较小的训练集。图1比较了带有和不带有ELMo的基线模型的性能，因为整个训练集的百分比在0.1％到100％之间变化。 ELMo的改进最大程度地适用于较小的训练集，并显着减少了达到给定性能水平所需的训练数据量。在SRL情况下，训练集为1％的ELMo模型的$F_1$与训练集为10％的基线模型的$F_1$大致相同。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221125020198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 图1：随着训练集大小从0.1％到100％的变化，SNLI和SRL的基线性能与ELMo性能的比较。
## 4.5 学习权重的可视化
图2展示了softmax归一化学习层权重。在输入层，任务模型倾向于第一个biLSTM层。对于共指和SQuAD，强烈建议使用此方法，但是对于其他任务，分布不会达到峰值。输出层权重相对平衡，较低层略有偏爱。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221125125116.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 图2：跨任务和ELMo位置的softmax归一化biLM层权重的可视化。小于1/3的归一化权重用水平线标出，而大于2/3的归一化权重用斑点表示。

# 5 结论
我们介绍了一种从biLM中学习高质量的深层上下文相关表示的通用方法，并且在将ELMo应用于各种NLP任务时显示出了很大的改进。通过消融和其他受控实验，我们还确认了biLM层有效地编码了有关上下文中单词的不同类型的句法和语义信息，并且使用所有层都可以提高总体任务性能。