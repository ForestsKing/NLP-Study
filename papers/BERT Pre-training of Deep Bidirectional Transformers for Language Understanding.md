# 0 摘要
我们介绍了一种称为BERT的新语言表示模型，该模型表示“Bidirectional Encoder Representations from Transformers”。与最近的语言表示模型不同，BERT被设计为通过在所有层的左，右上下文上共同进行条件预训练来自未标记文本的深度双向表示。结果，仅需一个额外的输出层就可以对经过预训练的BERT模型进行微调，以创建适用于各种任务（例如问题解答和语言推论）的最新模型，而无需进行大量特定于任务的体系结构修改。

BERT在概念上很简单，在经验上也很强大。它在11种自然语言处理任务上获得了最新的最新结果，包括将GLUE得分提高到80.5％（绝对提高7.7％），MultiNLI准确度达到86.7％（绝对提高4.6％），SQuAD v1.1问题答案测试F1达到93.2（绝对值提高1.5分）和SQuAD v2.0测试F1达到83.1（绝对值提高5.1点）。
# 1 介绍
语言模型预训练已被证明可以有效地改善许多自然语言处理任务。这些包括句子级任务，例如自然语言推理和释义，旨在通过对句子之间的关系进行整体分析来预测句子之间的关系。以及诸如命名实体识别和问题回答之类的令牌级任务，其中需要模型以在令牌级产生细粒度的输出。

有两种将预训练的语言表示应用于下游任务的现有策略：基于特征和微调。基于特征的方法（例如ELMo）使用特定于任务的架构，其中包括预训练的表示形式作为附加特征。精细调整方法（如Generative Pre-trained Transformer（OpenAI GPT））引入了最小的任务特定参数，并通过简单地微调所有预训练参数来对下游任务进行训练。两种方法在预训练期间具有相同的目标功能，它们使用单​​向语言模型学习通用语言表示形式。

我们认为，当前的技术限制了预训练表示的能力，特别是对于微调方法。主要限制是标准语言模型是单向的，这限制了可以在预训练期间使用的体系结构的选择。例如，在OpenAI GPT中，作者使用从左到右的体系结构，其中每个令牌只能参与Transformer的自注意层中的先前令牌。这样的限制对于句子级任务不是最理想的，并且在将基于微调的方法应用于令牌级任务（例如问题回答）时非常有害，在这种情况下，必须从两个方向合并上下文。

在本文中，我们通过提出BERT：来自Transformer的双向编码器表示法，改进了基于微调的方法。 BERT通过使用Cloze任务启发的“屏蔽语言模型”（MLM）预训练目标，减轻了前面提到的单向性约束。屏蔽语言模型从输入中随机屏蔽了某些标记，目的是仅根据其上下文预测被屏蔽单词的原始词汇ID。与从左到右的语言模型预训练不同，MLM目标使表示形式能够融合左右上下文，这使我们能够预训练深层双向Transformer。除了屏蔽语言模型外，我们还使用“下一个句子预测”任务来联合预训练文本对表示。本文的贡献如下：

- 我们证明了双向预训练对语言表示的重要性。与Radford等人不同，它使用单向语言模型进行预训练，BERT使用掩码语言模型来启用预训练的深度双向表示。这也与彼得斯等人相反，它使用了独立训练的从左到右和从右到左的LM的浅层连接。
- 我们表明，经过预训练的表示形式减少了对许多精心设计的任务特定体系结构的需求。 BERT是第一个基于微调的表示模型，可在一大堆句子级和令牌级任务上实现最先进的性能，优于许多特定于任务的体系结构。
- BERT推动了11个NLP任务的发展。可以在[https://github.com/google-research/bert](https://github.com/google-research/bert)上找到代码和预先训练的模型。

# 2 相关工作
训练通用语言表示形式已有很长的历史，我们将简要回顾本节中使用最广泛的方法。
## 2.1 无监督的基于特征的方法
数十年来，学习广泛适用的单词表示法一直是研究的活跃领域，包括非神经和神经方法。预先训练的词嵌入是现代NLP系统不可或缺的一部分，与从头开始学习的嵌入相比有显着改进。为了预训练单词嵌入向量，已经使用了从左到右的语言建模目标，以及在左右上下文中区分正确单词和错误单词的目标。

这些方法已推广到更粗粒度，例如句子嵌入或段落嵌入。为了训练句子表示，先验工作使用目标对候选的下一句进行排序，从左到右生成下一句单词，给出前一句的表示形式，或对自动编码器衍生的目标进行降噪。

ELMo及其前身从不同维度概括了传统词嵌入研究。他们从左到右和从右到左的语言模型中提取上下文相关的功能。每个标记的上下文表示是从左到右和从右到左表示的串联。当将上下文单词嵌入与现有的特定于任务的体系结构集成时，ELMo改进了几种主要的NLP基准的技术水平，其中包括问答，情感分析，命名实体识别。提出了通过一项任务来学习上下文表示，以使用LSTM从左右上下文中预测单个单词。与ELMo相似，它们的模型是基于特征的，并且不是双向的。 Fedus等显示了cloze任务可用于提高文本生成模型的鲁棒性。
## 2.2 无监督的微调方法
与基于特征的方法一样，第一个方法仅在未标记文本的预训练单词嵌入参数上才可以使用。

最近，产生上下文标记表示的句子或文档编码器已经从未标记的文本中进行了预训练，并针对有监督的下游任务进行了微调。这些方法的优点是几乎不需要从头学习参数。至少部分由于此优势，OpenAI GPT在GLUE基准测试中的许多句子级任务上获得了之前的最新结果。从左至右的语言建模和自动编码器目标已用于预训练此类模型。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221162642620.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 图1：BERT的总体预训练和微调过程。除输出层外，相同的体系结构还用于预训练和微调。相同的预训练模型参数用于初始化不同下游任务的模型。在微调期间，所有参数都将进行微调。 [CLS]是在每个输入示例前面添加的特殊符号，[SEP]是特殊的分隔符标记（例如，分隔问题/答案）。

## 2.3 从监督数据转移学习
也有工作显示从监督任务到大型数据集的有效转移，例如自然语言推理。计算机视觉研究还证明了从大型预训练模型进行转移学习的重要性，其中有效的方法是微调使用ImageNet进行预训练的模型。

# 3 BERT
在本节中，我们将介绍BERT及其详细实现。我们的框架有两个步骤：**预训练**和**微调**。在预训练期间，通过不同的预训练任务对未标记的数据进行模型训练。为了进行微调，首先使用预训练的参数初始化BERT模型，然后使用来自下游任务的标记数据对所有参数进行微调。每个下游任务都有单独的微调模型，即使它们使用相同的预训练参数进行了初始化。图1中的提问示例将作为本节的运行示例。

BERT的一个独特功能是其跨不同任务的统一体系结构。预训练的体系结构和最终的下游体系结构之间的差异很小。

**模型架构**：BERT的模型架构是多层双向Transformer编码器，由于使用Transformers已变得很普遍，并且我们的实现几乎与原始实现相同，因此我们将省略模型架构的详尽背景说明。

在这项工作中，我们将层（即，Transformer块）的数量表示为$L$，将隐藏的尺寸表示为$H$，并将自注意头的数量表示为$A$。我们主要报告两种模型尺寸的结果：$BERT_{BASE}$（L = 12，H = 768，A = 12，总参数= 110M）和$BERT_{LARGE}$（L = 24，H = 1024，A = 16，总参数= 340M）。

为了进行比较，选择$BERT_{BASE}$具有与OpenAI GPT相同的模型大小。但是，至关重要的是，BERT Transformer使用双向自我关注，而GPT变压器使用受限的自我关注，其中每个令牌只能关注其左侧的上下文。

**输入/输出表示**：为了使BERT处理各种下游任务，我们的输入表示能够明确地在一个令牌序列中表示单个句子和一对句子。在整个工作中，“句子”可以是任意连续文本，而不是实际的语言句子。 “序列”是指BERT的输入令牌序列，它可以是一个句子或两个句子包装在一起。

我们使用WordPiece嵌入和30,000个令牌词汇表。每个序列的第一个标记始终是特殊分类标记（[CLS]）。与此令牌对应的最终隐藏状态用作分类任务的聚合序列表示。句子对打包在一起形成单个序列。我们通过两种方式区分句子。首先，我们使用特殊令牌（[SEP]）将它们分开。其次，我们向每个标记添加学习的嵌入，以指示它是属于句子A还是句子B。如图1所示，我们将输入嵌入表示为E，将特殊[CLS]令牌的最终隐藏向量表示为$C\in R^H$， 第$i$个输入令牌的最终隐藏向量为$T_i \in R^H$。

对于给定的令牌，其输入表示形式是通过将相应的令牌，段和位置嵌入相加来构造的。这种结构的可视化效果如图2所示。

## 3.1 预训练BRET
我们没有使用传统的从左到右或从右到左的语言模型来预训练BERT。相反，我们使用本节中描述的两个无监督任务对BERT进行预训练。此步骤显示在图1的左侧。

- **任务1:屏蔽LM**：
  从直觉上讲，有理由相信，深层双向​​模型比左至右模型或左至右模型和从右至左模型的浅层连接更强大。不幸的是，标准条件语言模型只能从左到右或从右到左进行训练，因为双向条件将允许每个单词间接“看到自己”，并且该模型可以在多层结构中轻松预测目标单词语境。

  为了训练深度双向表示，我们简单地随机屏蔽一定百分比的输入令牌，然后预测这些屏蔽的令牌。尽管此过程在文献中通常被称为“Cloze”任务，但我们将此过程称为“Cloze LM”（MLM）。在这种情况下，如在标准LM中一样，将与掩码令牌相对应的最终隐藏向量馈送到词汇表上的输出softmax中。在所有实验中，我们随机屏蔽每个序列中所有WordPiece令牌的15％。与去噪自动编码器相反，我们仅预测被屏蔽的单词，而不重构整个输入。

  尽管这可以使我们获得双向的预训练模型，但缺点是我们在预训练和微调之间造成了不匹配，因为[MASK]令牌在微调过程中不会出现。为了减轻这种情况，我们并不总是用实际的[MASK]令牌替换“被屏蔽”的单词。训练数据生成器随机选择令牌位置的15％进行预测。如果选择了第$i$个令牌，我们将（1）80％的时间[MASK]令牌替换为第$i$个令牌，（2）10％的时间随机令牌（3）10％的时间第$i$个令牌不变。然后，将使用$T_i$来预测具有交叉熵损失的原始令牌。

- **任务2:下句预测(NSP)**
  许多重要的下游任务，如问答(QA)和自然语言推理(NLI)，都是基于对两个句子之间关系的理解，而语言建模并没有直接捕捉到这些关系。为了训练一个理解句子关系的模型，我们预先训练了一个可以从任何单语语料库中生成的二值化下一个句子预测任务。具体来说，当为每个训练示例选择句子A和B时，50%的情况下B是A后面的实际下一个句子(标记为IsNext)， 50%的情况下是语料库中的随机句子(标记为NotNext)。如图1所示，C用于下一个句子预测(NSP)。尽管它很简单，但我们在5.1节中证明了对这个任务的预训练对QA和NLI都是非常有益的。

  NSP任务与Jernite等人和Logeswaran和Lee中使用的表征学习目标密切相关。然而，在之前的工作中，只有句子嵌入被转移到下游任务，其中BERT转移所有参数来初始化最终任务模型参数。
  

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221164504278.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 图2:BERT输入表示。输入嵌入是标记嵌入、分割嵌入和位置嵌入的总和。



**预训练的数据**
训练前的程序在很大程度上遵循了现有的语言模式训练的文献。对于训练前语料库，我们使用图书语料库(8亿单词)和英语维基百科(2500万单词)。对于维基百科，我们只提取文本段落，忽略列表、表格和标题。为了提取长连续序列，使用文档级语料库而不是像十亿字基准语料库这样的句子级语料库是至关重要的。

## 3.2 微调BERT
微调是很简单的，因为Transformer中的自我注意机制允许BERT通过交换适当的输入和输出来模拟许多下游任务——无论它们涉及单个文本还是文本对。对于涉及文本对的应用，一种常见的模式是在应用双向交叉注意之前独立编码文本对。而BERT则利用自我注意机制将这两个阶段统一起来，因为对连接文本对进行自我注意编码有效地包含了两个句子之间的双向交叉注意。

对于每个任务，我们只需将特定于任务的输入和输出插入BERT中，并对所有参数进行端到端的微调。在输入时，预先训练的句子A和句子B类似于(1)释义时的句子对，(2)隐含时的假设-前提对，(3)问答时的问题-段落对，(4)文本分类或序列标注时的简并文本-$\emptyset$对。在输出时，令牌表示被送入输出层以完成令牌级任务，如序列标记或问题回答，而[CLS]表示被送入输出层以进行分类，如蕴涵或情感分析。

与预训练相比，微调相对便宜。从完全相同的预先训练过的模型出发，论文中的所有结果在单个云TPU上最多只需1小时，或在GPU上只需数小时。我们在第4节相应的小节中描述了特定于任务的细节。

# 4 实验
在本节中，我们将介绍11个NLP任务的BERT微调结果。
## 4.1 GLUE
通用语言理解评估(GLUE)基准是一个不同自然语言理解任务的集合。

为了在GLUE进行微调，我们将第3节中描述的输入序列(对于单个句子或句子对)表示出来，并使用与第一个输入令牌([CLS])对应的最终隐藏向量$C\in R^H$作为聚合表示。在微调过程中引入的新参数只有分类层权值$W\in R^{K \times H}$，其中$K$是标签的个数。我们用$C$和$W$计算标准分类损失，即$log(softmax(CW^T))$。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221165728527.png#pic_center)
>表1：由评估服务器（https://gluebenchmark.com/leaderboard）评分的GLUE测试结果。每个任务下面的数字表示培训示例的数量。 “平均”列与官方GLUE得分略有不同，因为我们排除了有问题的WNLI集。BERT和OpenAI GPT是单模型，单任务。报告QQP和MRPC的$F_1$得分，报告STS-B的Spearman相关性，报告其他任务的准确性得分。我们排除使用BERT作为其组件之一的条目。

对于所有GLUE任务，我们使用32的批处理大小并针对数据进行3个时期的微调。对于每个任务，我们在开发集上选择了最佳的微调学习率（在5e-5、4e-5、3e-5和2e-5中）。此外，对于$BERT_{LARGE}$，我们发现微调有时在小型数据集上不稳定，因此我们进行了几次随机重启，并在Dev集上选择了最佳模型。对于随机重启，我们使用相同的预训练检查点，但是执行不同的微调数据改组和分类器层初始化。

结果显示在表1中。$BERT_{BASE}$和$BERT_{LARGE}$在所有任务上的性能均优于所有系统，与现有技术相比，平均精度分别提高了4.5％和7.0％。请注意，除了注意掩盖之外，$BERT_{BASE}$和OpenAI GPT在模型架构方面几乎相同。对于最大且报告最广的GLUE任务MNLI，BERT获得4.6％的绝对准确度改进。在官方GLUEleaderboard上，$BERT_{LARGE}$的得分为80.5，而OpenAI GPT的得分为72.8。

我们发现，在所有任务中，尤其是训练数据很少的任务，$BERT_{LARGE}$的性能明显优于$BERT_{BASE}$。模型大小的影响将在5.2节中更全面地探讨。

## 4.2 SQuAD v1.1
斯坦福问答数据集（SQuAD v1.1）是10万个众包问题/答案对的集合。给定一个问题以及Wikipedia中包含答案的段落，任务是预测段落中的答案文本范围。

如图1所示，在问题解答任务中，我们将输入的问题和段落表示为单个打包序列，其中问题使用A嵌入，而段落使用B嵌入。在微调过程中，我们仅引入起始向量$S\in R^H$，结束向量$E\in R^H$。单词$i$作为答案跨度的开始的概率计算为$T_i$和 $S$之间的点积，然后是段落中所有单词上的softmax：$P_i=\frac {e^{S·T_i }} { \sum_j e^{S·T_j }}$。类似的公式用于答案范围的结尾。将从位置$i$到位置$j$的候选跨度的分数定义为$S·T_i + E·T_j$，并且将$j\geq i$的最大得分跨度用作预测。训练目标是正确的起点和终点位置的对数似然率的总和。我们以3e-5的学习率和3的批量大小微调3个时期。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221171224825.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 表2：SQuAD 1.1结果。 BERT集成是使用不同的预训练检查点和微调种子的7x系统。


表2显示了排行榜中排名靠前的条目以及排名靠前的系统的结果。 SQuAD排行榜的最佳结果没有最新的公共系统描述，并允许在培训他们的系统时使用任何公共数据。因此，我们首先在TriviaQA上进行微调，然后在SQuAD上进行微调，从而在系统中使用适度的数据增强。

我们最好的表现系统在整体表现上领先+1.5$F_1$，在整体表现上优于+1.3 $F_1$。实际上，就$F_1$分数而言，我们的单一BERT模型优于顶级整体系统。如果没有TriviaQA的微调数据，我们只会损失0.1-0.4 $F_1$，仍然远远胜过所有现有系统。


## 4.2 SQuAD v2.0
SQuAD 2.0任务通过允许在提供的段落中不存在简短答案的可能性扩展了SQuAD 1.1问题定义，从而使问题更加实际。

我们使用一种简单的方法来扩展SQuAD v1.1 BERT模型以完成此任务。我们将没有答案的问题视为答案范围以[CLS]令牌开头和结尾。起始和终止答案跨度位置的概率空间已扩展为包括[CLS]令牌的位置。为了进行预测，我们将无答案跨度的分数进行比较：$s_{null} = S·C + E·C$与最佳非零跨度的分数$s_{i,j}' =max_{j\geq i}S·T_i + E·T_j$。当$s_{i,j}'> s_{null} +\tau$时，我们预测一个非零答案，其中在dev集上选择阈值$\tau$以最大化$F_1$。我们没有为此模型使用TriviaQA数据。我们对2个纪元进行了微调，学习率为5e-5，批量大小为48。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221171317464.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 表3：SQuAD 2.0结果。我们排除使用BERT作为其组件之一的条目。

表3中显示了与先前排行榜条目和排名靠前的作品相比的结果，其中不包括使用BERT作为其组件之一的系统。与先前的最佳系统相比，我们观察到+1 $F_1$的改进。


## 4.4 SWAG
对抗代的情形(SWAG)数据集包含113k的句子对完成示例，用于评估基于常识的推理。给定一个句子，任务是在四个选项中选择最合理的延续。

大量数据集上微调时,我们构建四个输入序列,每个包含给定的句子的连接(句子)和一个可能的延续(句子B)。唯一的特定于任务的参数介绍了向量的点积(CLS)令牌表示C代表一个分数为每个选择与softmax层规范化。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221201152149.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 表4:SWAG Dev和测试精度。人体性能测量100个样品，SWAG论文报道。


我们以2e-5的学习率和16个批量为3个时期对模型进行微调。结果如表4所示。$BERT_{LARGE}$比作者的基线ESIM+ELMo系统高出27.1%，OpenAI GPT高出8.3%。

# 5 消融研究
在本节中，我们对BERT的多个面进行消融研究，以便更好地理解它们的相对重要性。


## 5.1 预训练任务的效果
我们通过使用完全相同的预训练数据、微调方案和与$BERT_{BASE}$完全相同的超参数来评估两个预训练目标，证明了BERT深度双向性的重要性：

**No NSP**：一种双向模型，使用“masked LM”(MLM)训练，但不使用“next sentence prediction”(NSP)任务。

**LTR & No NSP**：一个只使用左上下文的模型，它使用标准的从左到右(LTR) LM而不是MLM来训练。左侧约束也应用于微调，因为删除它会导致预训练/微调不匹配，从而降低下游性能。此外，该模型在没有NSP任务的情况下进行了预先训练。这可以直接与OpenAI GPT相比，但是使用的是我们更大的训练数据集、输入表示和微调方案。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221201646997.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 表5:使用$BERT_{BASE}$架构对预训练任务的处理。“No NSP”在没有下一个句子预测任务的情况下进行训练。“LTR & No NSP”被训练成一个从左到右的LM，没有下一个句子的预测，就像OpenAI GPT一样。“+ BiLSTM”在微调过程中在“LTR + No NSP”模型的顶部添加一个随机初始化的BiLSTM。


我们首先NSP任务带来的影响。在表5中，我们发现去除NSP会显著损害QNLI、MNLI和SQuAD 1.1的性能。接下来，我们通过比较“No NSP”和“LTR & No NSP”来评估训练双向表示的影响。在所有任务中，LTR模式的表现都比 MLM 模式差，MRPC和SQuAD都大幅下降。

对于SQuAD，我们可以直观地看到LTR模型在令牌预测方面表现不佳，因为令牌级别的隐藏状态没有右侧上下文。为了更好地加强LTR系统，我们在上面添加了一个随机初始化的BiLSTM。这确实可以显着改善SQuAD上的结果，但结果仍然比预训练的双向模型的结果差很多。 BiLSTM损害了GLUE任务的性能。

我们认识到，也有可能像ELMo一样训练单独的LTR和RTL模型并将每个令牌表示为两个模型的串联。但是：（a）这是单个双向模型的两倍昂贵； （b）对于QA这样的任务，这是不直观的，因为RTL模型将无法确定问题的答案； （c）这绝对不像深度双向模型那么强大，因为它可以在每一层使用左右上下文。

## 5.2 模型尺寸的影响

在本节中，我们探索模型大小对微调任务准确性的影响。我们训练了许多具有不同数量的层，隐藏单元和注意头的BERT模型，而其他方面则使用了与前面所述相同的超参数和训练过程。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210221202233679.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 表6:BERT模型尺寸的消融。#L =层数;#H =隐藏大小;#A =注意次数。“LM (ppl)”是隐藏的LM的困惑，提供的训练数据。

表6显示了选定的GLUE任务的结果。在此表中，我们报告了5次随机微调重新启动后的平均Dev Set准确性。我们可以看到，即使对于只有3,600个带标签的训练示例的MRPC，它与所有预训练任务也大不相同，较大的模型也会导致所有四个数据集的严格准确性提高。我们能够在相对于现有文献而言已经相当大的模型的基础上实现如此显着的改进，这也许也令人惊讶。例如，Vaswani等人探索了最大的Transformer。 是（L = 6，H = 1024，A = 16），编码器的参数为100M，而我们在文献中发现的最大的Transformer是（L = 64，H = 512，A = 2）和235M参数。相比之下，$BERT_{BASE}$包含110M参数，而$BERT_{LARGE}$包含340M参数。

众所周知，增加模型的大小将导致对诸如机器翻译和语言建模之类的大规模任务的持续改进，这一点在表6所示的持久性训练数据的LM困惑中得到了证明。但是，我们相信这是第一个有说服力的证明，只要模型已经过充分的预训练，缩放到极限模型大小也将导致非常小的规模任务的重大改进。彼得斯等对于将预训练的Bi-LM尺寸从两层增加到四层对下游任务的影响提出了不同的结果，Melamud等顺便提到了将隐藏尺寸从200增加到600有帮助，但进一步增加到1000并没有带来进一步的改进。这两个先前的工作都使用基于特征的方法-我们假设当直接在下游任务上微调模型并且仅使用很少数量的随机初始化的附加参数时，特定于任务的模型可以从更大，更具表达力的模型中受益。训练的表示形式，即使下游任务数据非常小也是如此。

## 5.3 基于特征的BERT方法
到目前为止，所有的BERT结果都使用了微调方法，其中将一个简单的分类层添加到预先训练的模型中，所有参数都在下游任务上进行联合微调。然而，基于特征的方法，即从预先训练的模型中提取固定的特征，具有一定的优势。首先，并不是所有的任务都可以通过Transformer 编码器体系结构轻松地表示，因此需要添加特定于任务的模型体系结构。第二，预先计算训练数据的一个昂贵的表示，然后在这个表示的基础上用便宜的模型进行多次实验，这对计算有很大的好处。

在本节中，我们通过将BERT应用于CoNLL-2003命名实体识别(NER)任务来比较这两种方法。在给BERT的输入中，我们使用保留大小写的文字模型，并包含数据提供的最大文档上下文。按照标准实践，我们将其定义为标记任务，但在输出层不使用CRF。我们使用第一个子标记的表示作为NER标签集上标记级分类器的输入。

为了消除这种微调方法，我们采用了基于特征的方法，即在不微调任何BERT参数的情况下从一个或多个层中提取激活。在分类层之前，将这些上下文嵌入作为随机初始化的两层768维BiLSTM的输入。

![在这里插入图片描述](https://img-blog.csdnimg.cn/2021022120261471.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)
> 表7：CoNLL-2003命名实体识别结果。使用开发集选择超参数。使用这些超参数在5次随机重启中平均报告的Dev和Test分数。

结果显示在表7中。$BERT_{LARGE}$用最新技术具有竞争力。最佳性能的方法将来自预训练的Transformer的顶部四个隐藏层的令牌表示连接在一起，这仅比微调整个模型低0.3$F_1$。这表明BERT对于微调和基于特征的方法均有效。

# 6 结论
由于使用语言模型进行了转移学习，最近的经验改进表明，丰富的，无监督的预培训是许多语言理解系统不可或缺的一部分。特别是，这些结果使即使是资源匮乏的任务也可以从深度单向体系结构中受益。我们的主要贡献是将这些发现进一步推广到深度双向体系结构，从而使相同的经过预训练的模型能够成功解决各种NLP任务。