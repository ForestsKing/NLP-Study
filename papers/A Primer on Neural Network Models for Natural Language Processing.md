# 1 导言

## 1.1 范围

## 1.2 术语说明

## 1.3 数学符号

# 2 神经网络架构

# 3 特征表示

## 3.1 密集向量与单点表示

## 3.2 特征变量数：连续词袋

## 3.3 距离和位置特征

## 3.4 特征组合

## 3.5 维度

## 3.6 病媒分享

## 3.7 网络的输出

## 3.8 历史记录

# 4 前馈神经网络

## 4.1 大脑暗示隐喻

## 4.2 数学符号

## 4.3 代表权力

## 4.4 常见的非线性

### 4.4.1 乙状结肠

### 4.4.2 双曲汤(TANH)

### 4.4.3 很硬的

### 4.4.4 整流器

## 4.5 输出变换

## 4.6 嵌入层

### 4.6.1 注记

### 4.6.2 关于稀疏与稀疏的注释。密集的特征

## 4.7 丧失功能

### 4.7.1 铰链（二进制）

### 4.7.2 铰链（多类）

### 4.7.3 日志丢失

### 4.7.4 范畴交叉熵损失

### 4.7.5 排名损失

# 5 字嵌入

## 5.1 随机初始化

## 5.2 监督特定任务的预训练

## 5.3 无监督的预训练

## 5.4 培训目标

## 5.5. 语境的选择

### 5.5.1 窗户通道

### 5.5.2 句子、段落或文件

### 5.5.3 句法窗口

### 5.5.4 多语种

### 5.5.5 基于字符和子词的表示

# 6 神经网络训练

## 6.1 随机梯度训练

## 6.2 计算图像抽象

### 6.2.1 前向计算

### 6.2.2 反向计算（导数、反向传播）

### 6.2.3 软件

### 6.2.4 实施配方

### 6.2.5 网络组成

## 6.3 优化问题

### 6.3.1 初始化

### 6.3.2 消失和梯度爆炸

### 6.3.3 饱和和死亡神经元

### 6.3.4 扭来扭去

### 6.3.5 学习率

### 6.3.6 迷你表

## 6.4 正规化

# 7 级联和多任务学习

## 7.1 模型级联

## 7.2 多任务学习

# 8 结构化输出预测

## 8.1 贪婪的结构化预测

## 8.2 基于搜索的结构化预测

### 8.2.1 概率目标（CRF）

### 8.2.2 打屁股

### 8.2.3 MEMM 和混合方法

# 9 卷积层

## 9.1 基本卷积+池化

## 9.2 动态、层次和k-max池

## 9.3 差异

# 10 递归神经网络-建模序列和堆栈

## 10.1 RNN抽象

## 10.2 RNN培训

### 10.2.1 接受者

### 10.2.2 编码器

### 10.2.3 传送器

### 10.2.4 编码器-解码器

## 10.3 多层（堆叠）RNN

## 10.4 双向RNN（biRNN）

## 10.5 代表堆栈的RNN

## 10.6 阅读文献的注释

# 11 混凝土RNN结构

## 11.1 简单RNN

## 11.2 磅

## 11.3 GRU

## 11.4 其他备选案文

# 12 建模树-递归神经网络

## 12.1 正式定义

## 12.2 扩展和变异

## 12.3 训练递归神经网络

# 13 结论



