# 0 摘要

过去几年，神经网络以强大的机器学习模型的形式重新出现，在图像识别和语音处理等领域产生了最先进的成果。最近，神经网络模型也开始应用于文本自然语言信号，同样获得了非常有前途的结果。本教程从自然语言处理研究的角度调查神经网络模型，试图让自然语言研究人员跟上神经技术的步伐。本教程涵盖了自然语言任务的输入编码，前馈网络，卷积网络，循环网络和递归网络，以及自动梯度计算的计算图抽象。

# 1 导言

十多年来，核心的自然语言处理技术被机器学习方法所主导，这些方法使用线性模型，如支持向量机或逻辑回归，在非常高维但非常稀疏的特征向量上训练。

最近，该领域在从稀疏输入的线性模型转换到密集输入的非线性神经网络模型方面取得了一些成功。虽然大多数神经网络技术都很容易应用，有时几乎作为旧线性分类器的临时替代品，但在许多情况下存在一个强大的进入障碍。在本教程中，我试图向NLP从业者(以及新手)提供基本的背景、术语、工具和方法论，让他们理解神经网络模型背后的原理，并将其应用到自己的工作中。本教程希望是独立的，同时在统一的符号和框架下介绍不同的方法。它重复了很多其他地方可以找到的材料。它还在适当的时候指向更高级主题的外部资源。

对于那些将继续发展神经网络机器的下一个进展的人来说，这本入门书并不打算作为一个全面的资源(尽管它可能作为一个很好的入门点)。相反，它的目标读者是那些有兴趣采取现有的，有用的技术，并应用它在有用的和创造性的方法，以他们最喜欢的自然语言处理问题。对于神经网络及其背后的理论、高级优化方法和其他高级主题的更深入、一般性的讨论，读者可以参考其他现有的资源。特别推荐本吉奥、古德费勒和考维尔(2015)的书。

## 1.1 范围

重点是神经网络在语言处理任务中的应用。但是，使用神经网络进行语言处理的一些子领域故意不在本教程的讨论范围之内。这些包括语言建模和声学建模的大量文献，用于机器翻译的神经网络的使用，以及结合语言和其他信号(如图像和视频)的多模态应用(如字幕生成)。对于高效运行时性能的缓存方法，使用大量输出词汇表和注意力模型进行高效训练的方法也没有讨论。对于词嵌入的讨论仅限于为了使用它们作为其他模型的输入而需要理解的范围。其他非监督方法，包括自动编码器和递归自动编码器，也不在此范围之内。虽然文中提到了神经网络在语言建模和机器翻译中的一些应用，但它们的处理并不全面。

## 1.2 术语说明

“特征”这个词是用来指一个具体的语言输入，比如一个单词、一个后缀或一个词性标签。例如，在一阶词性标注器中，特征可能是“当前词、前一个词、下一个词、前一个词性”。术语“输入向量”是用来指馈给神经网络分类器的实际输入。类似地，“输入向量条目”指的是输入的特定值。这与许多神经网络文献形成了鲜明对比，在这些文献中，“特征”一词在两种用法之间被重载，主要用于指输入向量条目。

## 1.3 数学符号

我用粗体大写字母表示矩阵$(X,  Y, Z)$，用粗体小写字母表示向量$(b)$。当有一系列相关的矩阵和向量时(例如，每个矩阵对应于网络中的不同层)，就使用上标索引$(W^1,  W^2)$。在极少数情况下，我们想要表示矩阵或向量的幂，在要取幂的项周围加一对方括号：$(W)^2，(W^3)^2$。除非另有说明，向量被假定为行向量。我们用$[v1;v2]$来表示向量串联。

选择使用行向量,这是右乘矩阵$(xW  + b)$，有点不标准。很多神经网络文献使用的列向量左乘矩阵$(Wx + b)$。我们相信读者能够适应阅读文献时的列向量表示法。

> 选择使用行向量表示法是受到以下好处的启发:
> - 它符合文献中经常绘制的输入向量和网络图的方式;
> - 它使网络的分层结构更加透明，并将输入作为最左边的变量，而不是嵌套;
> - 它的结果是全连接层的维度是$d_{in}\times d_{out}$而不是$d_{out}\times d_{in}$;
> - 而且它可以更好地映射到使用矩阵库(如$numpy$)在代码中实现网络的方式。

# 2 神经网络架构

神经网络是功能强大的学习模型。我们将讨论两种神经网络结构，它们可以是混合匹配前馈网络和循环 / 递归网络。前馈网络包括具有完全连接层的网络，例如多层感知器，以及具有卷积层和池化层的网络。所有的网络都起到了分类器的作用，但每个网络都有不同的优势。

完全连接的前馈神经网络(第4节)是非线性学习器，在大多数情况下，在使用线性学习器的任何地方都可以用作插入式替代。这包括二分类和多分类问题，以及更复杂的结构化预测问题(第8节)。网络的非线性，以及容易集成预先训练的单词嵌入的能力，往往导致优异的分类精度。一系列工作设法通过简单地用完全连接的前馈网络替换解析器的线性模型来获得改进的句法解析结果。前馈网络作为分类器替换的直接应用(通常与预先训练的词向量的使用相结合)也为CCG超标记、对话状态跟踪、统计机器翻译的预排序和语言建模提供了好处。多层前馈网络可以在情感分类和类事实问题回答方面提供有竞争力的结果。

具有卷积层和池化层的网络(第9节)对于分类任务很有用，在分类任务中，我们希望找到关于类成员关系的强本地线索，但这些线索可能会出现在输入的不同位置。例如，在文档分类任务中，单个关键短语(或$ngram$)可以帮助确定文档的主题。我们希望了解到，某些单词序列是主题的良好指示器，不一定关心它们在文档中出现在哪里。卷积层和池化层使模型能够学会找到这样的本地指标，而不考虑它们的位置。卷积和池化结构在文档分类、短文本分类、情感分类、实体间关系类型分类、事件检测、转述识别、语义角色标注、问答、根据评论家的评论预测电影的票房收入，对文本趣味性进行建模，对人物序列和词性标签之间的关系进行建模等方面取得了良好的效果。

在自然语言中，我们经常处理任意大小的结构化数据，例如序列和树。我们希望能够捕捉这些结构中的规律性，或者对这些结构之间的相似性进行建模。在许多情况下，这意味着将结构编码为固定宽度向量，然后我们可以将其传递给另一个统计学习器进行进一步处理。虽然卷积和池化体系结构允许我们将任意大项编码为固定大小的向量，以捕捉它们最显著的特征，但它们是通过牺牲大部分结构信息来做到这一点的。另一方面，循环(第10节)和递归(第12节)体系结构允许我们在保留大量结构信息的同时处理序列和树。循环网络是用来模拟序列的，而递归网络是循环网络的推广，可以处理树。我们还将讨论循环网络的扩展，使它们能够对堆栈进行建模。

循环模型已被证明在语言建模、序列标注、机器翻译、依存关系分析、情感分析、噪声文本归一化、对话状态跟踪、响应生成、以及对字符序列和词性标签之间的关系建模方面产生了非常强的结果。

递归模型被证明能够产生最先进或接近最先进的结果，用于选民和从属句法分析重新排序、话语分析、语义关系分类、基于句法分析树的政治意识形态检测、情感分类、目标相关情感分类和问题回答。

# 3 特征表示

在更深入地讨论网络结构之前，请注意如何表示特征，这一点很重要。目前，我们可以将前馈神经网络看作一个函数$NN(X)$，它以一个$d_{in}$维向量$x$作为输入，并产生一个$d_{out}$维的输出向量。该函数通常用作分类器，为输入$x$分配一个或多个$d_{out}$类中的隶属度。该函数可以是复杂的，并且几乎总是非线性的。此函数的常见结构将在第4节中讨论。在此，我们重点介绍输入$x$。当处理自然语言时，输入$x$对诸如单词、词性标签或其他语言信息等特征进行编码。当从稀疏输入的线性模型转移到基于神经网络的模型时，可能最大的概念跳跃是不再将每个特征表示为唯一的维度(所谓的$one-hot$表示)，而是将它们表示为密集向量。也就是说，每个核心特征被嵌入到$d$维空间中，并被表示为该空间中的向量。然后，可以像函数$NN$的其他参数一样训练嵌入(每个核心特征的向量表示)。图1显示了两种特征表示方法。

特征嵌入(每个特征的向量条目的值)被视为需要与网络的其他组件一起训练的模型参数。稍后将讨论训练(或获得)特征嵌入的方法。现在，考虑给定的特征嵌入。

基于前馈神经网络的NLP分类系统的一般结构如下：

- 提取一组与预测输出类相关的核心语言特征$f_1,…,f_k$。
- 对于每个感兴趣的特征$f_i$，检索相应的矢量$v(f_i)$。
- 将矢量(通过连接、求和或两者的组合)组合成输入矢量$x$。
- 将$x$输入非线性分类器(前馈神经网络)。

因此，输入中最大的变化是从稀疏表示法(其中每个特征都是其自己的维度)转变为密集表示法(其中每个特征被映射到一个向量)。另一个不同之处在于，我们只提取核心特征，而不提取特征组合。我们将简要阐述这两个变化。

## 3.1 密集向量与 one-hot 表示

将我们的特征表示为矢量而不是唯一ID有什么好处？我们是否应该始终将特征表示为密集向量？让我们考虑一下这两种表示：


![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204319473.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)


> 图1：稀疏特征表示与密集特征表示。信息的两种编码：$当前单词是“Dog”；前一个单词是“the”；前一个位置标签是“DET”$。
>
> - 稀疏特征向量。每个维度代表一个特征。特征组合会收到它们自己的尺寸。特征值是二进制的。维度非常高。
> - 密集的、基于嵌入的特征向量。每个核心特征被表示为一个矢量。每个特征对应于几个输入向量条目。没有对功能组合进行显式编码。维度较低。特征到矢量的映射来自嵌入表。

- one-hot​ 每个特征都是它自己的尺寸。
  - one-hot​ 向量的维度与不同特征的数量相同。
  - 功能之间是完全独立的。$“word\ is\ ‘dog’”$与$“word\ is\ ‘thinking’”$的区别，与$“word\ is\ ‘cat’”$的区别是一样的。
- 稠密的每个特征都是一个$d$维向量。
  - 向量的维数为$d$。
  - 模型训练会使相似的特征具有相似的向量——相似的特征之间共享信息。

使用密集和低维向量的一个好处是计算性：大多数神经网络工具包都不能很好地处理高维稀疏向量。然而，这只是一个技术障碍，可以通过一些工程努力来解决。

密集表示的主要好处是泛化能力：如果我们相信某些特征可能提供类似的线索，那么提供一种能够捕捉这些相似之处的表示是值得的。例如，假设我们在训练中观察了$“dog”$这个词很多次，但只观察了$“cat”$这个词几次，或者根本没有观察。如果每个单词都与它自己的维度相关联，那么$“dog”$的出现就不会告诉我们$“cat”$的出现。然而，在密集向量表示中，$“dog”$的学习向量可能类似于$“cat”$的学习向量，允许模型在两个事件之间共享统计强度。这个论证假设“好的”向量是给定的。第5节描述了获得这种向量表示的方法。

如果我们在类别中有相对较少的明显特征，并且我们认为不同特征之间没有相关性，我们可以使用 one-hot​ 表示。但是，如果我们相信每组中的不同特征之间有相关性(例如，对于词性标记，我们可能认为对我们的任务而言不同的动词词形变化$VB$和$VBZ$表现是一样的)通过共享参数让网络找出相关性和获得一些统计力量可能是值得。在某些情况下，当特征空间相对较小而训练数据丰富时，或者当我们不希望在不同的单词之间共享统计信息时，使用​ one-hot 表示可能会有好处。然而，这仍然是一个开放的研究问题，没有强有力的证据支持任何一方。大部分工作提倡对所有特征使用密集的、可训练的嵌入向量。

最后，重要的是要注意，将特征表示为密集向量是神经网络框架的一个组成部分，因此，使用稀疏和密集特征表示之间的差异比最初可能出现的要微妙。事实上，在训练神经网络时使用稀疏的、 one-hot​ 向量作为输入，相当于将网络的第一层专门用于根据训练数据为每个特征学习密集的嵌入向量。我们将在第4.6节讨论这个问题。

## 3.2 可变的特征数：连续词袋(CBOW)

前馈网络假设一个固定的维度输入。这可以很容易地适应提取固定数量特征的特征提取函数的情况：每个特征被表示为一个向量，并将向量连接起来。这样，得到的输入向量的每个区域对应一个不同的特征。然而，在某些情况下，特征的数量是事先不知道的(例如，在文档分类中，句子中的每个单词都是一个特征是很常见的)。因此，我们需要用一个固定大小的向量来表示无界数目的特征。实现这一目标的一种方法是通过所谓的连续词袋(CBOW)表示。CBOW非常类似于传统的单词袋表示，我们丢弃了订单信息，通过对相应特征的嵌入向量求和或平均来工作：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204350699.png#pic_center)

CBOW表示的一个简单变化是权值  CBOW，其中不同的向量获得不同的权值：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204358167.png#pic_center)

这里，每个特征$f_i$有一个相关联的权重$a_i$，表示特征的相对重要性。例如，在文档分类任务中，一个特征$f_i$可能对应于文档中的一个单词，而相关的权重$a_i$可能是该单词的TF-IDF分数。

## 3.3 距离和位置特征

句子中两个词之间的线性距离可以作为一种信息特征。例如，在事件提取任务中，可能会给我们一个触发器字和一个候选参数字，并要求我们预测参数字是否确实是触发器的参数。触发器和参数之间的距离(或相对位置)是这个预测任务的强信号。在“传统”的NLP设置中，距离通常通过将距离分成若干组(即1、2、3、4、5-10、10+)进行编码，并将每个库与 one-hot 向量关联。在一个神经体系结构中，输入向量不是由二进制指标特征组成的，将单个输入条目分配给距离特征似乎很自然，其中该条目的数值就是距离。然而，在实践中并没有采用这种方法。相反，距离特征的编码与其他特征类型相似：每个容器都与一个d维向量相关联，然后这些距离嵌入向量被训练成网络中的常规参数。

## 3.4 特征组合

注意，神经网络设置中的特征提取阶段只处理核心特征的提取。这与传统基于线性模型的NLP系统的特性设计了手动指定不仅感兴趣的核心功能,而且它们之间的相互作用(例如,不仅引入功能说明”这个词是X”和特性说明“标签是Y”还结合功能说明”字是X和标签是Y”甚至“单词是X,标记Y和之前的单词是Z”)。组合特征在线性模型中是至关重要的，因为它们为输入引入了更多维度，将其转换为一个数据点更接近于线性可分的空间。另一方面，可能组合的空间非常大，特征设计师必须花大量时间想出一组有效的特征组合。非线性神经网络模型的承诺之一是只需要定义核心特征。分类器的非线性(由网络结构定义)被期望用来寻找指示性特征组合，减轻对特征组合工程的需求。

核方法，特别是多项式核，也允许特征设计者只指定核心特征，而将特征组合方面留给学习算法。与神经网络模型相比，核方法是凸的，承认优化问题的精确解。然而，核方法分类的计算复杂度随着训练数据的大小呈线性增长，这使得它们对于大多数实际目的来说太慢，不适合在大数据集的训练。另一方面，使用神经网络进行分类的计算复杂度与网络的大小呈线性关系，而与训练数据的大小无关。

## 3.5 维度

我们应该为每个特征分配多少维度?不幸的是，在这个领域中没有理论界限，甚至没有既定的最佳实践。显然，维度应该随着类中成员的数量增加而增加(您可能想给单词嵌入分配比给词性嵌入分配更多的维度)，但是多少维度才足够呢?在目前的研究中，单词嵌入向量的维数在50到几百之间，在一些极端情况下，甚至达到数千。由于向量的维数对内存需求和处理时间有直接影响，一个好的经验法则是试验几种不同大小的向量，并在速度和任务精度之间做出权衡。

## 3.6 向量分享

考虑这样一种情况，您有一些共享相同词汇表的特性。例如，当将一个词性分配给一个给定的单词时，我们可能会考虑考虑上一个单词的一组特征，以及考虑下一个单词的一组特征。在构建分类器的输入时，我们将把前一个单词的向量表示与下一个单词的向量表示连接起来。分类器将能够区分这两个不同的指标，并区别对待它们。但这两个特征应该共享相同的向量吗?“上一个dog”的向量是否与“下一个dog”的向量相同?或者我们应该给它们分配两个不同的向量?这基本上也是一个经验问题。如果你相信言语行为不同,当他们出现在不同的位置(例如,在前面的位置字X像单词Y,但在后面位置X像Z)那么它可能是一个好主意使用两个不同的词汇表和分配一个不同的每个特征向量的集合类型。但是，如果您认为单词在两个位置的行为相似，那么可以通过为两种特性类型使用共享的词汇表来获得一些东西。

## 3.7 网络的输出

对于有k个类的多类分类问题，网络的输出是一个k维向量，其中每个维代表一个特定输出类的强度。也就是说，输出与传统的线性模型一样——离散集合中项目的标量分数。然而，正如我们将在第4节中看到的，有一个与输出层相关的$d \times k$矩阵。这个矩阵的列可以被认为是输出类的d维嵌入。k个类的向量表示之间的向量相似性表明模型学习到的输出类之间的相似性。

## 3.8 历史记录

$Bengio$等人(2003)在神经语言建模的背景下推广了将单词作为密集向量输入到神经网络的方法。在$Collobert,  Weston$和同事$(2008,2011)$的开创性工作中，引入了NLP任务。38在$Chen$和$Manning(2014)$之后，使用嵌入技术不仅可以表示单词，还可以表示任意的特征。

# 4 前馈神经网络

本节介绍前馈神经网络。它从触发它们的受欢迎的大脑启发的隐喻开始，但很快切换回使用数学符号。讨论了前馈神经网络的结构、表示幂以及常见的激活函数和损失函数。

## 4.1 灵感来自大脑的隐喻

顾名思义，神经网络的灵感来自于大脑的计算机制，由称为神经元的计算单元组成。在这个比喻中，神经元是一个具有标量输入和输出的计算单元。每个输入都有一个相关的权值。

神经元将每个输入乘以它的权值，然后将39个权值相加，对结果应用非线性函数，并将其传递给输出。神经元相互连接，形成一个网络:一个神经元的输出可以馈送给一个或多个神经元的输入。这种网络被证明是非常有能力的计算设备。如果权重设置正确，一个具有足够的神经元和非线性激活函数的神经网络可以近似非常广泛的数学函数(稍后我们将对此进行更精确的描述)。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204419840.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)


> 图2:有两个隐含层的前馈神经网络。

一个典型的前馈神经网络如图2所示。每个圆是一个神经元，输入的箭头是神经元的输入，输出的箭头是神经元的输出。每个箭头都有一个权重，反映其重要性(未显示)。神经元按层排列，反映了信息的流动。底层没有进入的箭头，是网络的输入。最顶层没有向外的箭头，是网络的输出。其他层次被认为是“隐藏的”。中间层神经元内部的S型形状代表一个非线性函数(即逻辑函数$1/(1  +e^{−xa}$))，在将其传递给输出前应用于神经元值。在图中，每个神经元都与下一层的所有神经元相连——这被称为全连接层或仿射层。

虽然大脑的比喻是性感和有趣的，但它也是分散注意力和笨拙的数学操作。因此，我们转而使用更简明的数学符号。网络中每一行神经元的值可以看作是一个向量。在图2中，输入层是一个4维向量$(x)$，它上面的层是一个6维向量$(h^1)$。全连接层可以看作是一个线性变换从4维到6维。全连接层实现了一个向量矩阵乘法,$h  = xW$，连接的权重是从在输入行中的第$i$个神经元到输出行的第$j$个神经元$W_{ij}$。被非线性函数g转化后的h值应用于每个值传递给下一个输入。从输入到输出的整个计算可以写成$(g(xW^1))W^2$，其中$w^1$为第一层的权值，$W^2$为第二层的权值。

## 4.2 数学符号

从这一点开始，我们将放弃大脑的比喻，只使用向量矩阵运算来描述网络。最简单的神经网络是感知器，它是感知器输入的线性函数:

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204431669.png#pic_center)


W是权重矩阵，b是偏置项。为了超越线性函数，我们引入了非线性隐层(图2中的网络有两个这样的层)，导致了带有一个隐层的多层感知器(MLP1)。带有一个隐含层的前馈神经网络的形式为:

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204441166.png#pic_center)


这里$w^1$和$b^1$是输入第一次线性变换的矩阵和偏置项，g是应用于元素的非线性函数(也称为非线性或激活函数)，$w^2$和$b^2$是第二次线性变换的矩阵和偏置项。

把它分解，$xW^1+b^1$是输入$x$从$d_{in}$维度到$d_1$维的线性变换。然后将g应用到每个$d_1$维上，然后使用矩阵$w^2$和偏置向量$b^2$将结果转换为$d_2$维输出向量。非线性激活函数g在网络表示复杂函数的能力中起着至关重要的作用。没有了g中的非线性，神经网络只能表示输入的线性变换。

我们可以添加额外的线性转换和非线性，从而产生具有两个隐藏层的MLP(图2中的网络就是这种形式):

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204452285.png#pic_center)


使用中间变量来编写更深层次的网络可能会更清晰:

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204459291.png#pic_center)

每个线性变换产生的向量被称为一个层。最外面的线性变换产生输出层，其他的线性变换产生隐藏层。每个隐藏层后面都有一个非线性的激活。在某些情况下，例如在我们的示例的最后一层，偏置向量被强制为0(“删除”)。

由线性变换产生的层通常被称为完全连通层或仿射层。存在其他类型的体系结构。特别是，图像识别问题可以从卷积层和池化层中获益。这些层在语言处理中也有用途，将在第9节中讨论。有几个隐藏层的网络被称为深度网络，因此得名深度学习。

当描述一个神经网络时，应该指定层的尺寸和输入。一个层将期望一个$d_{in}$维向量作为它的输入，并将它转换成一个$d_{out}$维向量。该层的维数被认为是其输出的维数。对于输入维数$d_{in}$输出维数$d_{out}$的全连通层$l(x)  = xW+b$, $x$的维数为$1 × d_{in}$, $W$的维数为$d_{in}× d_{out}$, $b$的维数为$1 × d_{out}$。

网络的输出是一个$d_{out}$维向量。如果$d_{out}=1$，网络的输出是一个标量。这种网络可以通过考虑输出值来进行回归(或评分)，也可以通过咨询输出的符号来进行二进制分类。可以使用$d_{out}= k > 1$的网络进行k类分类，将每个维与一个类关联，寻找具有最大值的维。类似地，如果输出向量条目是正的并且和为1，输出可以被解释为类分配的分布(这种输出规范化通常通过在输出层应用$softmax$变换来实现，见4.5节)。

定义线性变换的矩阵和偏差项是网络的参数。通常把所有参数的集合称为$\theta$。这些参数和输入一起决定了网络的输出。训练算法负责设置它们的值，以便网络的预测是正确的。训练将在第6节讨论。

## 4.3 表示能力

在表示能力方面，MLP1是通用逼近器–它可以以任何期望的非零误差来逼近一个函数家族，其中包括所有连续的$R^n$的封闭有界子集上的函数，以及从任何有限维离散空间到另一个空间的任何函数映射。这可能表明没有理由超越MLP1到更复杂的体系结构。但是，理论结果并未讨论神经网络的可学习性（它陈述了一种表示形式，但并未说明基于训练数据和特定的学习算法来设置参数的难易程度）。这也不能保证训练算法会找到生成训练数据的正确函数。最后，它没有说明隐藏层应该有多大。的确，存在着神经网络，其中神经网络的边界层数很多，除非它们的层数呈指数级增长，否则无法被具有较少层数的网络近似。

在实践中，我们使用局部搜索方法（例如随机梯度下降的变体）在相对少量的数据上训练神经网络，并使用相对中等大小（最多数千个）的隐藏层。由于通用逼近定理在这些非理想的真实条件下无法提供任何保证，因此尝试比MLP1更复杂的体系结构肯定会带来好处。但是，在许多情况下，MLP1确实可以提供出色的结果。

## 4.4 常见的非线性

非线性g可以采用多种形式。目前尚无关于在哪种条件下应用哪种非线性的良好理论，而为给定任务选择正确的非线性在很大程度上是一个经验问题。现在，我将回顾文献中常见的非线性：$sigmoid$，$tanh$，$hard\ tanh$ 和整流线性单位（$ReLU$）。一些NLP研究人员还尝试了其他形式的非线性，例如$cube$和$tanh-cube$。

### 4.4.1 sigmoid

sigmoid激活函数$\sigma（x）= 1 /（1 + e^{-x}）$，也称为逻辑函数，是一个S形函数，将每个值x转换为[0,1]范围。自神经网络成立以来，sigmoid一直是规范的非线性，但是目前认为它已被弃用在神经网络的内部层中，因为下面列出的选择在经验上证明效果更好。

### 4.4.2 tanh

双曲正切$tanh（x）= \frac {e^{2x}-1}{ e^{2x} + 1}$激活函数是一个S形函数，将值x转换为范围[-1,1]。

### 4.4.3 hard tanh

hard tanh激活函数是tanh函数的近似值，可以更快地计算并采用以下导数：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204529573.png#pic_center)

### 4.4.4 ReLU

整流器激活函数，也称为整流线性单元，是一种非常简单的激活函数，易于使用，已多次显示以产生出色的效果。ReLU单元会裁剪每个值$x<0$时为0。尽管它很简单，但是它可以很好地完成许多任务，尤其是在与 dropout 正则化技术结合使用时（请参见6.4节）。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204537606.png#pic_center)


根据经验，ReLU单元的性能优于tanh，tanh的性能优于sigmoid

## 4.5 输出变换

在许多情况下，输出层向量也会被转换。常见的转换是$softmax$：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204545691.png#pic_center)


结果是一个非负实数的向量，总和为1，使其成为k个可能结果的离散概率分布。

当我们对可能的输出类别上的概率分布建模感兴趣时，可以使用$softmax$输出转换。为使其有效，应与概率训练目标（例如交叉熵）结合使用（请参阅下面的4.7.4节）。

将$softmax$变换应用于没有隐藏层的网络的输出时，结果就是众所周知的多项式逻辑回归模型，也称为最大熵分类器。

## 4.6 嵌入层

到目前为止，讨论都忽略了$x$的来源，将其视为任意向量。在NLP应用程序中，$x$通常由各种嵌入向量组成。我们可以明确说明$x$的来源，并将其包含在网络的定义中。我们介绍$c（·）$，这是一个从核心特征到输入向量的函数。

对于c来说，提取与每个特征相关联的嵌入向量并将它们串联起来是很常见的：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204556695.png#pic_center)


c的另一个常见选择是对嵌入向量求和（假设嵌入向量都共享相同的维数）：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204610265.png#pic_center)


c的形式是网络设计的重要组成部分。在许多论文中，通常将c称为网络的一部分，并且同样将词嵌入$v（f_i）$视为“嵌入层”或“查找层”的结果。考虑$| V  |$的词汇单词，每个单词都作为d维向量嵌入。然后可以将向量的集合视为$| V |\times d$嵌入矩阵E，其中每行对应一个嵌入特征。设一个$| V  |$维向量，该向量除一个索引外全为零，与第$i$个特征的值相对应，其中第一个特征的值为1（这称为one-hot向量）。然后，乘法$f_iE$将“选择”  E的相应行。因此，可以根据E和$f_i$定义$v（f_i）$：

![在这里插入图片描述](https://img-blog.csdnimg.cn/2021022020462099.png#pic_center)

并且类似地：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204627232.png#pic_center)


然后，将网络的输入视为 one-hot 向量的集合。尽管这很优雅并且在数学上定义得很好，但是有效的实现通常涉及将基于哈希的数据结构将特征映射到其对应的嵌入向量，而无需经历一次性演示。

在本教程中，我们将c与网络体系结构分开：网络的输入始终是密集的实值输入向量，并且c在输入通过网络之前应用，类似于熟悉的线性模型术语中的“特征函数”  。但是，在训练网络时，输入向量x确实会记住它的构造方式，并且可以将错误梯度适当地传播回其组件嵌入向量（错误传播将在第6节中讨论）。

### 4.6.1 关于符号的注释

在描述将级联向量x，y和z作为输入的网络层时，一些作者使用显式级联$([x; y; z] W + b)$，而另一些作者则使用仿射变换$（xU + yV + zW +  b）$。如果仿射变换中的权重矩阵U，V，W彼此不同，则这两个符号相等。

### 4.6.2 关于稀疏特征与密集特征的注释

考虑一个网络，该网络的输入向量使用“传统”稀疏表示，并且没有嵌入层。假设所有可用特征的集合为$V$，我们有$k$个“ on”特征$f_1,...,f_k$，$f_i\in V$，网络的输入为：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204641581.png#pic_center)


因此，第一层（忽略非线性激活）是：

![在这里插入图片描述](https://img-blog.csdnimg.cn/202102202046515.png#pic_center)


该层选择与x中的输入特征相对应的W行并将其相加，然后添加一个偏差项。这与在特征上生成CBOW表示的嵌入层非常相似，其中矩阵W充当嵌入矩阵。主要区别在于引入了偏置矢量b，并且嵌入层通常不会发生非线性激活，而是直接传递到第一层这一事实。另一个区别是，这种情况迫使每个特征都接收一个单独的矢量（W行），而嵌入层提供了更大的灵活性，例如允许要素“下一个词是狗”和“上一个词是狗”共享相同的特征向量。

但是，这些差异很小而微妙。当涉及到多层前馈网络时，密集输入和稀疏输入之间的差异小于乍看之下的差异。

## 4.7 损失函数

训练神经网络时（与下面的第6节中的训练更多有关），就像训练线性分类器时一样，定义一个损失函数$L（y^*，y）$，指出当真实输出为$y$时预测$y^*$的损失。培训目标是使不同培训示例中的损失最小化。损失$L（y^*，y）$在给定真实预期输出$y$的情况下为网络的输出$y^*$分配一个数字分数（标量）。损失函数应从下面限制，只有在网络输出的情况下才能达到最小值是正确的。

然后设置网络的参数（矩阵$W^i$，偏置$b^i$和通常嵌入E），以使训练样本上的损失L最小化（通常，这是不同训练样本上的损失之和最小化）。

损失可以是将两个向量映射到标量的任意函数。出于优化的实际目的，我们将自己局限于可以轻松计算梯度（或子梯度）的函数。在大多数情况下，依赖于常见的损失函数而不是定义自己的损失函数就足够了并且值得建议。现在，我们讨论一些在NLP的神经网络中常用的损失函数。

### 4.7.1 hinge（二类）

对于二类分类问题，网络的输出为单个标量$y^*$，预期的输出$y\in \{+  1，−1\}$。分类规则为$sign（y^*）$，如果$y·y^*>0$，则认为分类正确，这意味着$y$和$y^*$共享相同的符号。hinge损耗（也称为余量损耗或SVM损耗）定义为：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204703447.png#pic_center)


当$y$和$y^*$共享相同的符号并且$|y^*|>1$ |时，损失为0。否则，损耗是线性的。换句话说，二元hinge损失试图获得正确的分类，其裕度至少为1



### 4.7.2 hinge（多类）

令$y^*  = y_1^*,...,y_n^*$代表网络的输出向量，并且$y$是正确输出类别的one-hot向量。

分类规则定义为选择得分最高的类别：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204713311.png#pic_center)


用$t = argmax_i\ y_i$表示正确的类别，用$k = argmax_{i\neq t}\ y^*$表示最高得分的类别，使得$k \neq t$。多类hinge损失定义为：
![在这里插入图片描述](https://img-blog.csdnimg.cn/2021022020472189.png#pic_center)



多类hinge损失尝试以至少1的余量在所有其他类别上得分高于其他类别。

二类和多类hinge损失都打算与线性输出层一起使用。每当我们需要严格的决策规则，并且不尝试对类成员资格概率进行建模时，hinge损失就很有用。

### 4.7.3 对数损失

对数损耗是hinge损耗的常见变化，可以看作是铰链损耗的“软”形式，具有无限的余量

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204729105.png#pic_center)


### 4.7.4 分类交叉熵损失

当需要对分数进行概率解释时，使用分类交叉熵损失（也称为负对数似然）。

令$y=y_1,...,y_n$表示在标签$1,...,n$上的真实多项式分布的向量。 。 。 ，n，令$y^*  = y_1^*,...,y_n^*$表示网络的输出，该结果由$softmax$激活函数进行了转换，并表示类成员条件分布$y_i^* = P（y = i | x）$。分类交叉熵损失度量的是真实标签分布$y$与预测标签分布$y^*$之间的差异，并定义为交叉熵：

![在这里插入图片描述](https://img-blog.csdnimg.cn/2021022020473961.png#pic_center)


对于每个训练示例都有一个正确的类别分配的硬分类问题，$y$是表示真实类别的ont-hot矢量。在这种情况下，交叉熵可以简化为：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204746712.png#pic_center)


其中t是正确的类分配。这试图将分配给正确类别 t 的概率质量设置为1。因为已使用$softmax$函数对分数$y^*$进行了变换并表示条件分布，所以增加分配给正确类别的质量意味着减少分配给所有其他类别的质量。

交叉熵损失在神经网络文献中非常常见，它产生了一个多类分类器，它不仅预测一个最好的类别标签，而且预测在可能标签上的分布。当使用交叉熵损失时，假设网络的输出使用$softmax$变换进行变换。

### 4.7.5 排名损失

在某些情况下，我们不会受到标签方面的监管，而是提供成对的正确和错误项目$x$和$x'$，我们的目标是对正确项目进行评分，使其高于错误项目。当我们只有积极的榜样时，就会出现这种训练情况，并通过破坏积极的榜样来产生负面的榜样。在这种情况下有用的损失是基于保证金的排名损失，为一对正确和不正确的示例定义：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204755647.png#pic_center)


其中$NN（x）$是网络为输入向量$x$分配的分数。目的是对正确的输入进行评分（对不正确的输入进行排名），保证金至少为1。

一个常见的变化是使用排名损失的对数版本：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204802319.png#pic_center)



在语言任务中使用排序hinge损失的例子包括使用用于导出预先训练的单词嵌入的辅助任务进行训练(参见第5节)，其中给我们一个正确的单词序列和一个损坏的单词序列，我们的目标是在损坏的单词序列之上给正确的序列打分。类似地，在选择偏好任务中使用了排序损失，在该任务中，网络被训练为将正确的动词-宾语对排序在不正确的、自动导出的动词-宾语对之上，人们训练了一个模型，在信息提取设置中，将正确的(头部、关系、线索)三元组评分在损坏的三元组之上。在工作中可以找到使用排序对数损失的例子。在工作中给出了一种允许负类和正类有不同余量的排序对数损失的变体。 

# 5 字嵌入

神经网络方法的主要组成部分是嵌入的使用-将每个特征表示为低维空间中的向量。但是向量来自哪里？本节将调查常见方法。

## 5.1 随机初始化

如果有足够的监督训练数据可用，则可以将特征嵌入与其他模型参数相同：将嵌入向量初始化为随机值，然后让网络训练过程将其调整为“良好”向量。

必须注意执行随机初始化的方式。有效的word2vec实现所使用的方法将字向量初始化为范围为$[-\frac{1}{ 2d}，\frac{1}{ 2d}  ]$，其中d是维数。另一种选择是使用$xavier$初始化（请参见6.3.1节），并使用$ −\frac{\sqrt6}{\sqrt d}，\frac{\sqrt6}{\sqrt d}$中的统一采样值进行初始化。

在实践中，人们经常会使用随机初始化方法来初始化常见特征（例如词性标签或单个字母）的嵌入矢量，同时使用某种形式的有监督或无监督预训练来初始化潜在的稀有特征，例如单个单词的特征。然后，可以将预训练向量在网络训练过程中视为固定的，或更常见的是像随机初始化的向量一样对待，然后进一步调整以适应当前的任务。

## 5.2 监督特定任务的预训练

如果我们对任务A感兴趣，那么对于该任务A我们只有有限数量的标记数据（例如，语法分析），但是对于辅助任务B（例如，词性标记），我们还有更多的任务标记的数据，我们可能需要预训练我们的词向量，以便它们能够很好地完成任务B的预测变量，然后将训练后的向量用于训练任务A。通过这种方式，我们可以利用大量的标记数据对于任务B。训练任务A时，我们可以将预先训练的向量视为固定的，也可以针对任务A对其进行进一步调整。另一种选择是针对两个目标共同训练，请参见第7节以获取更多详细信息。

## 5.3 无监督的预训练

常见的情况是，我们没有足够多的带注释数据的辅助任务（或者我们可能想帮助引导具有更好向量的辅助任务训练）。在这种情况下，我们求助于“无监督”方法，可以对大量无注释文本进行训练。

训练单词向量的技术本质上是监督学习的技术，但是我们不是监督我们关心的任务，而是从原始文本创建实际上无限数量的监督训练实例，希望我们创建的任务能够匹配（或足够接近）我们关心的最终任务。

无监督方法背后的关键思想是，人们希望“相似”词的嵌入向量具有相似向量。虽然单词的相似性很难定义，并且通常非常依赖于任务，但是当前的方法是从分布假设中得出的，指出如果单词在相似的上下文中出现，它们是相似的。所有不同的方法都创建有监督的训练实例，其中目标是根据单词的上下文预测单词，或者根据单词预测上下文。

在大量未注释的数据上训练单词嵌入的一个重要好处是，它为未出现在监督训练集中的单词提供矢量表示。理想情况下，这些单词的表示形式将与训练集中出现的相关单词的表示形式相似，从而使模型可以更好地概括未见事件。因此，期望由无监督算法学习的词向量之间的相似性捕获相似性的相同方面，这对于执行网络的预期任务是有用的。

常见的无监督词嵌入算法包括$word2vec$、$GloVe$ 和$Collobert and Weston$嵌入算法。这些模型受神经网络的启发，基于随机梯度训练。然而，它们与另一类在NLP和IR社区中发展起来的算法有着深刻的联系，这些算法基于矩阵分解。

可以论证的是，辅助问题的选择(预测什么，基于什么样的背景)对结果向量的影响要比用来训练它们的学习方法大得多。因此，我们将重点放在可用的辅助问题的不同选择上，而只略过训练方法的细节。有几个派生词向量的软件包可供使用，包括$word2vec$和$gensim$使用基于$word \ windows$的上下文实现$word2vec$模型，$word2vecf$(它是$word2vec$的修改版本，允许使用任意上下文)，以及实现$GloVe$模型的$glove$。许多预先训练好的词向量也可以在网上下载。



## 5.4 训练目标

给定一个单词w和它的上下文c，不同的算法制定不同的辅助任务。在所有情况下，每个单词都被表示为一个d维向量，该向量被初始化为一个随机值。训练该模型来完成辅助任务，可以得到与上下文相关的词的良好嵌入，从而使相似词的嵌入向量彼此相似。

受语言建模启发的方法，使用辅助任务，目标是预测给定上下文的单词。这是在一个概率设置中提出的，试图建模条件概率$P(w|c)$。

其他方法将问题简化为二值分类。除了观察到的单词上下文对的集合$D$之外，一个集合 $D' $是从随机单词和上下文对中创建的。二分类问题是:给定的$(w, c)$对是否来自于$D$ ?这些方法的不同之处在于集合$D'$是如何构造的，分类器的结构是什么，以及优化的目标是什么。可以采用基于边缘的二进制排序方法，训练前馈神经网络来对错误的$(w,  c)$对进行评分。也可以采用概率版本，训练一个对数双线性模型来预测概率$P((w, c)\in D|w,  c)$这一对来自语料库而不是随机样本。

## 5.5. 上下文的选择

在大多数情况下，一个单词的上下文被认为是它周围出现的其他单词，或者是在它周围的一个短窗口中，或者是在同一个句子、段落或文档中。在某些情况下，文本由语法解析器自动解析，上下文从自动解析树诱导的语法邻域派生。有时，单词和上下文的定义也会改变，包括单词的部分，如前缀或后缀。

神经词嵌入起源于语言建模的世界，在这个世界中，一个网络被训练来根据之前的单词序列预测下一个单词。在这里，文本被用来创建辅助任务，其目的是基于上下文中的k个先前单词来预测一个单词。虽然语言建模辅助预测问题的训练确实会产生有用的嵌入，但这种方法受到了语言建模任务的不必要限制，在该任务中，只允许查看前面的单词。如果我们不关心语言建模，而只关心所产生的嵌入，我们可以通过忽略这个约束并将上下文作为围绕焦点词的对称窗口来做得更好。

### 5.5.1 窗口法

最常见的方法是滑动窗口方法，其中通过查看$2k  +  1$个单词的序列来创建辅助任务。中间单词称为焦点单词，而每边的$k$个单词是上下文。然后，创建单个任务，目标是基于所有上下文词来预测焦点词（使用CBOW表示，或向量级联），或创建$2k$个不同的任务，每个任务将焦点词与另一个上下文词配对。 $2k$任务方法被称为跳跃文法模型(SG)。基于跳过图的方法显示出强大且有效的训练方法，并且经常产生最先进的结果。

- 窗口大小的影响

  滑动窗口的大小对所得向量相似性有很大影响。较大的窗口倾向于产生更多的主题相似性（例如，“狗”，“树皮”和“皮带”将被分组在一起，以及“行走”，“奔跑”和“行走”分组），而较小的窗口倾向于产生更多的功能性。和句法上的相似之处（例如“贵宾犬”，“比特犬”，“罗特韦勒”或“步行”，“奔跑”，“接近”）。

- 位置窗口

  当使用CBOW或SG上下文表示时，窗口内所有不同的上下文词均被同等对待。靠近焦点词的上下文词和远离焦点词的上下文词之间没有区别，同样，在焦点词之前出现的上下文词和在焦点词之后出现的上下文词之间也没有区别。可以通过使用位置上下文轻松地吸收此类信息：为每个上下文单词还指示其与焦点单词的相对位置（即，代替上下文单词“  the”为“  the：+2”，表示该单词出现焦点词右边的两个位置）。位置上下文和较小的窗口的使用往往会产生语法上更相似的相似性，并且很容易将共享一部分词性的单词组合在一起，并且在语义上在功能上相似。当位置向量用于初始化词性标记和语法相关性解析的网络时，它比基于窗口的向量更有效。

- 变体

  窗口方法的许多变体都是可能的。可以在学习之前对单词进行词素化，应用文本规范化，过滤太短或太长的句子或删除大写字母。可以对语料库的一部分进行子采样，以某种可能性跳过从焦点词太常见或太少的窗口创建任务。窗口大小可以是动态的，每次转动时使用不同的窗口大小。人们可能会以不同的方式衡量窗口中不同位置的权重，更多地关注于尝试正确预测接近的单词上下文对，而不是更远的单词上下文。这些选择中的每一个都会影响所得的向量。 

### 5.5.2 句子、段落或文章

使用SG（或CBOW）方法，可以将一个单词的上下文视为与同一句子，段落或文档中出现的所有其他单词。这等效于使用非常大的窗口大小，并且有望产生捕获主题相似性的词向量（来自同一主题的词，即一个人希望出现在同一文档中的词很可能会接收类似的向量）。

### 5.5.3 句法窗口

一些工作用句法代替句子中的线性语境。使用依赖关系解析器自动解析文本，并且单词的上下文被视为与在解析树中最接近的单词以及它们通过其连接的句法关系一起。这种方法产生了高度的功能相似性，将单词组合在一起，比起在句子中可以扮演相同角色的单词分组（例如颜色，学校名称，动词）。分组也是一种句法，将共享词形的词归为一组。

### 5.5.4 多语种

另一种选择是使用基于翻译的多语言语境。例如，在给定大量的句子对齐的平行文本的情况下，可以运行双语对齐模型，例如$IBM$模型1或模型2（即使用$GIZA  ++$软件），然后使用产生的对齐方式导出单词上下文。在此，单词实例的上下文是与其对齐的外语单词。这种比对倾向于导致同义词单词接收相似的向量。一些作者改为在句子对齐级别上工作，而不依赖于单词对齐或训练端到端机器翻译神经网络并使用生成的单词嵌入。一种吸引人的方法是将基于单语言窗口的方法与多语言方法相混合，以创建两种辅助任务。这很可能会产生类似于基于窗口的方法的矢量，但是会减少基于窗口的方法的神经网络入门中的不希望的影响，在该方法中反义词（例如，热和冷，高和低）倾向于接收相似的向量。

### 5.5.5 基于字符和子词的表示

一个有趣的工作尝试从组成单词的字符中得出单词的矢量表示。这种方法对于本质上是句法的任务可能特别有用，因为单词中的字符模式与其句法功能密切相关。这些方法还具有产生非常小的模型大小的优点（字母表中每个字符只需要存储一个向量以及少数小的矩阵），并且能够为可能遇到的每个单词提供嵌入向量。可以使用字符上的卷积网络（参见第9节）对单词的嵌入进行建模。或使用两个RNN（LSTM）编码器的最终状态的串联来建模单词的嵌入（第10节），一个从左到右读取字符，另一个从右到左读取字符。两者都为词性标记产生非常强的结果。 两个LSTM编码也有利于在形态丰富的语言的依赖解析中表示单词。

从单词的字符表示中派生单词的表示形式是受未知单词问题的驱使：当遇到一个没有嵌入向量的单词时，您会怎么做？由于可能的字符的词汇量比可能的单词的词汇量小得多，因此在字符级别上进行工作可在很大程度上缓解此问题。但是，在字符级别上工作非常具有挑战性，因为语言中的形式（字符）与功能（语法，语义）之间的关系非常松散。限制自己停留在字符级别可能是不必要的硬约束。一些研究人员提出了一个中间立场，其中一个词被表示为该词本身的向量与构成该词的子词单位的向量的组合。然后，子词嵌入有助于在具有相似形式的不同词之间共享信息，并在未观察到该词时允许退回到子词级别。同时，当足够多的单词观察结果可用时，模型不会被迫仅依赖于形式。  有人建议将单词的嵌入向量建模为特定词向量的总和（如果存在），并使用构成该词的不同形态成分的向量（这些成分使用$Morfessor$推导，一种无监督的形态学分割方法）。有人建议不仅将单词形式本身用作单词的核心特征，还将单词中每个字母的唯一特征（因此具有唯一的嵌入向量）用作核心特征。

# 6 神经网络训练

通过使用基于梯度的方法，通过尝试使训练集上的损失函数最小化，来完成神经网络训练。粗略地说，所有训练方法都是通过在数据集上重复计算误差的估计值，相对于误差的梯度进行计算，然后在梯度的相反方向上移动参数。在如何计算误差估计以及如何定义“沿梯度的相反方向移动”方面，模型有所不同。我们描述了基本算法，即随机梯度下降（SGD），然后简要介绍了其他带有指针的方法，以供进一步阅读。梯度计算对于该方法至关重要。可以在计算图上使用反向模式微分来高效且自动地计算梯度，这是用于自动计算任何网络和损失函数的梯度的通用算法框架，将在6.2节中讨论。

## 6.1 随机梯度训练

训练神经网络的常用方法是使用随机梯度下降（SGD）算法或它的一种变体。  SGD是一种通用的优化算法。它接收由$\theta$参数化的函数$f$，损失函数以及所需的输入和输出对。然后，尝试设置参数$\theta$，以使训练示例中的$f$损失较小。该算法的工作原理如下：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204841316.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)


该算法的目标是设置参数$\theta$，以使训练集上的总损失$\sum^n_{i=1}L(f(x_i;\theta),y_i)$最小。它的工作方式是反复采样一个训练示例，并计算该示例相对于参数$\theta$的误差梯度（第7行）假定输入和预期输出是固定的，并且将损失视为参数$\theta$的函数。然后，在梯度的相反方向上更新参数$\theta$，并按学习率$\eta_t$进行缩放（第8行）。学习率可以在整个训练过程中固定，也可以根据时间步长$t$进行衰减。有关设置学习率的更多讨论，请参见6.3节。

请注意，第6行中计算出的误差是基于单个训练示例，因此仅是我们旨在最大程度减少的全语料损失的粗略估计。损耗计算中的噪声可能会导致梯度不准确。减少这种噪声的常用方法是基于m个示例的样本来估计误差和梯度。这产生了minibatch  SGD算法：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204850982.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)


在第6  – 9行中，该算法基于最小批量估计语料损失的梯度。循环之后，$g^*$包含梯度估计，并且参数$\theta$向$g^*$更新。最小批量的大小可以在m = 1到m =  n之间变化。较高的值可以更好地估计整个语料库的梯度，而较小的值则可以更新更多，从而加快收敛速度。除了提高了梯度估计的准确性外，minibatch算法还提供了提高训练效率的机会。对于中等大小的m，某些计算架构（即GPU）允许在第6–9行中高效并行执行计算。在适当降低学习率的情况下，如果函数是凸的，则可以确保SGD收敛到全局最优。但是，它也可以用于优化非凸函数，例如神经网络。尽管不再保证找到全局最优值，但该算法被证明是健壮的并且在实践中表现良好。

在训练神经网络时，参数化函数$f$为神经网络，参数$\theta$为线性变换矩阵，偏差项，嵌入矩阵等。梯度计算是SGD算法以及所有其他神经网络训练算法中的关键步骤。那么问题是，如何根据参数计算网络误差的梯度。幸运的是，采用反向传播算法的形式存在一种简单的解决方案。反向传播算法是一个奇特的名字，用于使用链式法则有条理地计算复杂表达式的导数，同时缓存中间结果。更一般而言，反向传播算法是反向模式自动微分算法的特例。下一节将在计算图抽象的背景下描述反向模式自动微分。

### 6.1.1 超越SGD

尽管SGD算法可以并且通常确实可以产生良好的结果，但是也可以使用更高级的算法。  $SGD + Momentum$和$Nesterov \  Momentum$算法是SGD的变体，其中以前的梯度会累积并影响当前的更新。自适应学习率算法包括$AdaGrad$，$AdaDelta$，$RMSProp$和$Adam$，被设计为为每个小批量选择学习速率，有时在每个坐标的基础上，潜在地减轻了对学习速率调度的需要。由于许多神经网络软件框架提供了这些算法的实现，所以尝试不同的算法非常容易，有时也值得一试。

## 6.2 计算图像概念

虽然可以手工计算网络的各种参数的梯度，并在代码中实现它们，但这个过程是繁琐和容易出错的。在大多数情况下，最好使用自动工具进行梯度计算)。计算图抽象允许我们轻松地构造任意网络，评估它们对给定输入的预测(前向传递)，并计算它们的参数与任意标量损耗有关的梯度(后向传递)。

计算图是用图形表示任意数学计算的一种形式。它是一个有向无环图(DAG)，其中节点对应数学操作或(绑定)变量，边对应节点之间的中间值流。图结构根据不同组件之间的依赖关系定义了计算的顺序。图是DAG而不是树，因为一个操作的结果可以是多个延续的输入。例如，考虑一个计算$(a∗b  + 1)∗(a∗b + 2)$的图形:

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204914922.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)


$a  * b$的计算是共享的。我们只限于连接计算图的情况。

由于神经网络本质上是数学表达式，因此可以将其表示为计算图。


![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204924187.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)



> 图3：MLP1的计算图。（a）输入未绑定的图形。（b）有具体输入的图表。（c）带有具体输入，预期输出和损失节点的图形。

例如，图3a给出了具有一个隐藏层和$softmax$输出转换的MLP的计算图。用我们的符号表示，椭圆形节点表示数学运算或函数，阴影矩形节点表示参数（绑定变量）。网络输入被视为常量，并且在没有周围节点的情况下进行绘制。输入和参数节点没有输入弧，输出节点没有输出弧。每个节点的输出是一个矩阵，其维数指示在该节点上方。该图不完整：如果不指定输入，就无法计算输出。

图3b显示了一个MLP的完整图形，该图形以三个词作为输入，并预测了第三个词在词性标签上的分布。该图可用于预测，但不能用于训练，因为输出是矢量（非标量），并且该图未考虑正确的答案或损失项。

最后，图3c中的图形显示了特定训练示例的计算图，其中输入是单词“  the”，“ black”，“ dog”的（嵌入），预期输出是“  NOUN”（其索引为5）。选择节点执行索引操作，接收向量和索引（在本例中为5）并返回向量中的相应条目。

一旦建立了图形，就可以直接进行正向计算（计算结果）或反向计算（计算梯度），如下所示。构造图形可能看起来很艰巨，但是使用专用的软件库和API实际上非常容易。

### 6.2.1 前向计算

前向传递计算图中节点的输出。由于每个节点的输出仅取决于其自身及其传入边缘，因此通过按拓扑顺序遍历节点并在给定其前任节点已计算输出的情况下计算每个节点的输出来计算所有节点的输出是很简单的。

更正式地说，在$N$个节点的图中，我们根据节点的拓扑顺序将每个节点与索引$i$关联。设$f_i$是被节点$i$计算的函数（例如，乘法，加法...）。设$\pi（i）$为节点$i$的父节点，并且$\pi^{−1}（i）= \{j |  i\in\pi（j）\}$为节点$i$的子节点（这些是$f_i$的自变量）。用$v（i）$表示节点$i$的输出，即使用$f_i$表示其参数$\pi^{-1}（i）$的输出值。对于变量和输入节点，$f_i$是一个常数函数，并且$\pi^{-1}（i）$为空。前向算法为所有$i\in[1，N]$计算值$v（i）$。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204941978.png#pic_center)


### 6.2.2 反向计算（导数、反向传播）

反向传递通过将输出标量（1×1）的节点N指定为损耗节点开始，并进行向前计算直至该节点。反向计算会计算相对于该节点值的梯度。用$d（i）$表示数量$\frac{\partial\ N}{\partial\ i}$。反向传播算法用于计算所有节点$i$的值$d（i）$。

反向遍历填充表$d（i）$如下：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220204950632.png#pic_center)


$\frac{\partial f_j}{\partial i}$是$f_j（\pi^{-1}（j））$的偏导数，而不是自变量$i\in \pi^{-1}(j)$。此值取决于函数$f_j$和$v(a_1),...,v(a_m)(其中a_1,...,a_m =\pi^{-1}(j))$的值，这些参数是在前向传递中计算的。

因此，为了定义一种新型的节点，一种需要定义两种方法：一种用于基于节点输入来计算前向值$v（i）$，另一种用于针对每个$x\in\pi^{-1}(i)$计算$\frac{\partial f_j}{\partial x} $。

### 6.2.3 代码

一些软件包实现了计算图形模型，包括$Theano$，$Chainer$，$penne$和$CNN  /  pyCNN$。所有这些软件包都支持用于定义各种神经网络体系结构的所有基本组件（节点类型），涵盖了本教程及其他内容中描述的结构。通过使用运算符重载，图形创建几乎变得透明。该框架定义了一种表示图节点（通常称为表达式）的类型，构造用于输入和参数的节点的方法，以及一组将表达式作为输入并产生更复杂表达式的函数和数学运算。例如，使用$pyCNN$框架从图（3c）创建计算图的python代码为：

```python
import pycnn as pc
# model initialization.
model = pc.Model()
pW1 = model.add_parameters((20,150))
pb1 = model.add_parameters(20)
pW2 = model.add_parameters((17,20))
pb2 = model.add_parameters(17)
words = model.add_lookup_parameters((100, 50))
# Building the computation graph:
pc.renew_cg() # create a new graph.
# Wrap the model parameters as graph-nodes.
W1 = pc.parameter(pW1)
b1 = pc.parameter(pb1)
W2 = pc.parameter(pW2)
b2 = pc.parameter(pb2)
def get_index(x): return 1 # place holder
# Generate the embeddings layer.
vthe  = pc.lookup(words, get_index("the"))
vblack = pc.lookup(words, get_index("black"))
vdog  = pc.lookup(words, get_index("dog"))
# Connect the leaf nodes into a complete graph.
x = pc.concatenate([vthe, vblack, vdog])
output = pc.softmax(W2*(pc.tanh(W1*x)+b1)+b2)
loss = -pc.log(pc.pick(output, 5))
loss_value = loss.forward()
loss.backward() # the gradient is computed
# and stored in the corresponding
# parameters.
```

大多数代码都涉及各种初始化：第一个块定义了在不同计算图之间共享的模型参数（请注意，每个图都对应于一个特定的训练示例）。第二个块将模型参数转换为图节点（表达式）类型。第三块检索表达式的输入单词嵌入。最后，第四个块是创建图的位置。请注意，图形创建的透明性与创建图形与数学描述之间几乎一一对应。最后一块显示前进和后退。其他软件框架遵循类似的模式。

$Theano$涉及用于计算图的优化编译器，这既是福也是祸。一方面，大型图一旦编译，就可以在CPU或GPU上高效运行，因此非常适合具有固定结构的大型图，其中只有输入在实例之间改变。但是，编译步骤本身可能会很昂贵，并且使接口使用起来有些麻烦。相反，其他软件包则专注于构建大型动态计算图，并在不进行编译步骤的情况下“即时”执行它们。尽管$Theano$的优化版本的执行速度可能会受到影响，但这些软件包在使用第10、12节中所述的循环和递归网络以及第8节中所述的结构化预测设置时特别方便。

### 6.2.4 实施方案

使用计算图抽象，在算法5中给出了网络训练算法的伪代码。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205009815.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)

此处，构建计算图是用户定义的函数，可为给定的输入，输出和网络结构构建计算图，并返回单个损失节点。更新参数是优化器特定的更新规则。配方指定为每个训练示例创建一个新图。这适用于在训练示例（例如循环神经网络和递归神经网络）之间网络结构变化的情况，将在第10至12节中讨论。对于具有固定结构的网络（例如MLP），创建一个基础可能更有效计算图，并且仅在示例之间更改输入和预期输出。

### 6.2.5 网络组成

只要网络的输出是矢量（1×k矩阵），就可以通过将一个网络的输出作为另一个网络的输入，创建任意网络来构成网络。计算图抽象使该功能更加明确：计算图中的节点本身可以是具有指定输出节点的计算图。然后，人们可以设计任意深度和复杂的网络，并且可以借助自动前向和梯度计算轻松评估和训练它们。正如我们在第7节中讨论的那样，这使得为结构化输出和多目标训练定义和训练网络以及在第10-12节中讨论的复杂的循环和递归网络变得容易。

## 6.3 优化问题

一旦完成了梯度计算，就可以使用SGD或其他基于梯度的优化算法来训练网络。被优化的功能不是凸面的，长期以来，神经网络的训练被认为是一种“妖术”，只能由少数几个人完成。实际上，许多参数会影响优化过程，因此必须谨慎调整这些参数。虽然本教程并非旨在作为成功培训神经网络的全面指南，但我们在此确实列出了一些突出的问题。

### 6.3.1 初始化

损失函数的非凸性意味着优化过程可能会卡在局部最小值或鞍点中，并且从不同的初始点（例如参数的不同随机值）开始可能会导致不同的结果。因此，建议从不同的随机初始化开始进行几次训练重新开始，然后根据开发集选择最佳的训练。对于不同的网络公式和数据集，结果的方差量是不同的，并且无法预测进步。

随机值的大小对训练的成功有重要影响。  $xavier$初始化，建议初始化权重矩阵$W  \in       R^{d_{in} \times d_{out}}$ as：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205022918.png#pic_center)

其中$U [a，b]$是在$[a，b]$范围内的均匀采样随机值。该建议基于$tanh$激活函数的属性，在许多情况下效果很好，并且是许多人首选的默认初始化方法。

当使用$ReLU$非线性时，应通过从标准偏差为$\sqrt{\frac{2}{d_{in}}} $的零均值高斯分布中采样来初始化权重。  这种初始化在图像分类任务中比$xavier$初始化更好，特别是在涉及深层网络时。



### 6.3.2 梯度消失和梯度爆炸

在深度网络中，当误差梯度通过计算图传播回来时，它们通常消失（变得非常接近0）或爆炸（变得非常高）。这个问题在更深层的网络中变得更加严重，尤其是在递归和循环网络中。处理消失的梯度问题仍然是一个开放的研究问题。解决方案包括使网络变浅，逐步训练（首先根据一些辅助输出信号训练第一层，然后根据实际任务信号对它们进行修复并训练整个网络的上层），执行批标准化（对于每个小批量，将每个网络层的输入归一化以具有零均值和单位方差）或使用旨在帮助梯度流的专用架构（例如，针对递归网络的LSTM和GRU架构）  ，在第11节中讨论。处理爆炸梯度有一个简单但非常有效的解决方案：如果梯度的范数超过给定阈值，则将其剪切。

### 6.3.3 饱和神经元和死亡神经元

$tanh$和$sigmoid$激活的层可能会变得饱和，导致该层的输出值都接近于一（激活函数的上限）。饱和神经元的梯度非常小，应避免使用。具有$ReLU$激活的层不能饱和，但可以“消亡”，大多数或所有值均为负，因此对于所有输入均裁剪为零，从而导致该层的梯度为零。如果您的网络训练得不好，建议监视网络中有许多饱和或死亡神经元的层。饱和神经元是由于进入该层的值太大而引起的。可以通过更改初始化，缩放输入值的范围或更改学习率来进行控制。死亡的神经元是由进入该层的所有信号均为负信号引起的（例如，这可能在大梯度更新后发生）。降低学习率将在这种情况下有所帮助。对于饱和层，另一种选择是在激活后归一化饱和层中的值，即使用$g（h）= \frac{ tanh（h）}{||tanh（h）||}$代替$g（h）=  tanh（h）$。层归一化是抵消饱和的有效措施，但在梯度计算方面也很昂贵。一种相关技术是批处理归一化，其中对每一层的激活进行了归一化，以使它们在每个小批处理中的平均值为0，方差为1。批标准化技术成为有效培训计算机视觉深层网络的关键组件。在撰写本文时，它在自然语言应用程序中不太流行。

### 6.3.4 Shuffling

训练示例向网络展示的顺序很重要。上面的SGD公式指定每轮选择一个随机示例。实际上，大多数实现都按顺序遍历了培训示例。建议在每次传递数据之前先对训练示例进行洗牌。

### 6.3.5 学习率

学习率的选择很重要。太大的学习率将阻止网络收敛于有效的解决方案。太小的学习率将花费很长时间才能收敛。根据经验，应该以[0,1]范围内的一系列初始学习率进行实验，例如0.001、0.01、0.1、1。随着时间的推移，监控网络的损耗，并在损耗停止改善时降低学习速率。学习速率调度降低速率作为一个函数的观察小批量的数量。一个常见的计划是将初始学习率除以迭代数。推荐使用以下形式的学习速率:$\eta_t= \eta_0(1 +  \eta_0\lambda t)^{-1}$，其中，$\eta_0$是初始学习速率，$\eta_t$是$t$训练示例中使用的学习速率，$\lambda$是额外的超参数。在整个数据集上运行之前，应基于一小部分数据来确定一个好的$\eta_0$值。

### 6.3.6 小批量

每个训练示例(大小为1的小批)或每个k个训练示例都会更新参数。一些问题可以从更大的小批量训练中获益。在计算图抽象方面，可以为每个k个训练示例创建一个计算图，然后将k个损失节点连接到一个平均节点下，平均节点的输出是小批的损失。大量的小批量训练在GPU等专用计算架构上的计算效率方面也有好处，用矩阵矩阵运算代替向量矩阵运算。这超出了本教程的范围。

## 6.4 正则化

神经网络模型具有许多参数，并且过度拟合很容易发生。通过正则化可以在某种程度上减轻过度拟合的情况。常见的正则化方法是$L_2$正则化，通过在要最小化的目标函数上添加一个累加$\frac{\lambda}{2}||\theta||^2$来对具有大数值的参数进行平方惩罚，其中$\theta$是模型参数的集合，$||·||^2$是平方的$L_2 $基准（平方和值），而$\lambda$是控制正则化量的超参数。

最近提出的替代正则化方法是 dropout。dropout方法旨在防止网络学习依赖特定的权重。在每个训练示例中，通过随机删除（设置为0）网络（或特定层）中的一半神经元来工作。  

dropout技术是导致神经网络方法在图像分类任务上取得非常出色成果的关键因素之一，尤其是与$ReLU$激活单元结合使用时。dropout技术在神经网络的NLP应用中也很有效。



# 7 级联和多任务学习

在线训练方法与使用计算图抽象的自动梯度计算相结合，可以轻松实现模型级联，参数共享和多任务学习。

## 7.1 模型级联

是一种强大的技术，大型网络是由较小的组件网络组成的。例如，我们可能有一个前馈网络，根据相邻的单词或组成单词的字符来预测单词的词性。在流水线方法中，我们将使用这个网络来预测词性，然后将预测结果作为输入特征提供给神经网络，后者进行句法分块或解析。相反，我们可以把这个网络的隐藏层看作是一种编码，它捕获了预测词性的相关信息。在级联的方法中，我们把这个网络的隐藏层连接起来(而不是语音预测本身的部分)作为语法网络的输入。我们现在有了一个更大的网络，它接受单词和字符序列作为输入，并输出一个语法结构。计算图抽象允许我们很容易地将语法任务丢失的错误梯度传播到字符上。

为了解决深层网络消失的梯度问题，以及更好地利用可用的训练材料，可以通过在相关任务上分别对其进行培训来引导各个组件网络的参数，然后再将其插入较大的网络以进行进一步调整。例如，可以在将词性预测网络的隐藏层插入语法分析网络中之前，训练词性预测网络以在相对较大的带注释的语料库上准确地预测词性，对于该语法分析网络而言，训练数据较少。如果训练数据可以直接监督这两个任务，我们可以在训练期间使用它，方法是创建一个具有两个输出的网络，每个任务一个，为每个输出计算一个单独的损失，然后将这些损失汇总为一个节点从中我们反向传播误差梯度。

当使用卷积，循环和递归神经网络时，模型级联非常普遍，例如，循环网络用于将句子编码为固定大小的向量，然后将其用作另一个网络的输入。循环网络的监控信号主要来自上层网络，上层网络在输入时会消耗循环网络的输出。

## 7.2 多任务学习

当我们有相关的预测任务时，这些任务并不一定相互衔接，但我们相信，对一种预测有用的信息对其他一些任务也有用。例如，组块、命名实体识别(NER)和语言建模都是协同任务的例子。预测块边界、命名实体边界和句子中的下一个单词的信息都依赖于一些共享的底层语法语义表示。我们可以创建一个具有多个输出的网络，而不是为每个任务训练一个单独的网络。一种常见的方法是有一个多层前馈网络，它的最终隐藏层(或所有隐藏层的连接)然后被传递到不同的输出层。这样，网络的大部分参数就可以在不同的任务之间共享。从一个任务中获得的有用信息可以帮助消除其他任务的歧义。同样，通过计算每个可用的监控信号的单独损失，然后将损失相加为一个用于计算梯度的单一损失，计算图形抽象使构建这样的网络和为它们计算梯度非常容易。如果我们有几个语料库，每个语料库都有不同类型的监督信号(例如，我们有一个语料库用于NER，另一个语料库用于分块)，那么训练过程将洗牌所有可用的训练示例，针对每个回合的不同损失执行梯度计算和更新。

# 8 结构化输出预测

NLP中的许多问题都涉及到结构化输出：在这种情况下，所需的输出不是类标签或类标签上的分布，而是一个结构化对象，如序列、树或图。典型的例子是序列标记(例如词性标记)、序列切分(组块，NER)和句法分析。在本节中，我们将讨论如何将前馈神经网络模型用于结构化任务。在后面的章节中，我们将讨论用于处理序列(第10节)和树(第12节)的专门神经网络模型。

## 8.1 贪婪结构化预测

结构预测的贪婪方法是将结构预测问题分解为一系列局部预测问题，并训练一个分类器来执行每个局部决策。在测试时，以贪婪的方式使用训练好的分类器。这种方法的例子有从左到右的标签模型和基于转换的贪婪解析。通过简单地将本地分类器从线性分类器(如SVM或logistic回归模型)替换为神经网络，这些方法很容易适应使用神经网络。

贪婪的方法会遭受错误传播，早期决策中的错误会继续存在并影响后期决策。非线性神经网络分类器可实现整体上更高的准确性，从而在某种程度上弥补了这一问题。此外，还提出了一种训练技术来缓解错误传播问题，方法是尝试先进行较简单的预测，然后再进行较困难的预测，或者通过公开训练程序给可能产生错误的输入使训练条件与测试条件更相似。这些对训练贪婪神经网络模型也有效。

## 8.2 基于搜索的结构化预测

预测自然语言结构的常用方法是基于搜索的。这些技术可以很容易地应用于神经网络。在神经网络文献中，这些模型是在基于能量学习的框架下讨论的。在这里，它们使用了自然语言处理社区熟悉的设置和术语。

基于搜索的结构化预测被表述为对可能结构的搜索问题:

![在这里插入图片描述](https://img-blog.csdnimg.cn/2021022020505288.png#pic_center)


其中x是输入结构，y是x的输出（在典型示例中x是一个句子，y是该句子的标记分配或分析树），Y（x）是所有有效结构的集合在x上，我们正在寻找输出y，它将使x，y对的得分最大化。

评分函数定义为线性模型：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205101523.png#pic_center)

其中$\Phi$是特征提取函数，$w$是权重向量。

为了使对最优y的搜索易于处理，将结构y分解为多个部分，并根据这些部分定义特征函数，其中$\phi（p）$是局部特征提取函数：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205109796.png#pic_center)


每个部分分别评分，结构评分是组成部分评分的总和：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205116135.png#pic_center)



其中$p\in y$是$p\in parts（x，y）$的简写。  y分解为多个部分，使得存在一种推理算法，该算法允许在给定各个部分的分数的情况下有效搜索最佳得分结构。

现在，可以使用神经网络轻松替换零件上的线性评分函数：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205125628.png#pic_center)


其中c（p）将部分p映射为$d_{in}$维向量。

如果是一个隐藏层前馈网络：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205138677.png#pic_center)


$c（p）\in R^{d_{in}}，W^1\in R^{d_{in}\times d_1}，b^1\in R^{d_1}，w\in R^{d_1}$。结构化预测的一个共同目标是使金结构$y$得分高于其他任何结构$y'$，从而导致以下（广义感知器）损失：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205147518.png#pic_center)


在实现方面，这意味着：为每个可能的部分创建一个计算图$CG_p$，并计算其得分。然后，对得分部分进行推论，以找到最佳得分结构$y'$。将对应于黄金（预测）结构$y(y')$中各个部分的计算图的输出节点连接到求和节点$CG_y（CG_y'）$中。使用“减”节点$CG_l$连接$CG_y$和$CG_y'$，并计算梯度。

当训练结构化预测神经网络时，广义感知器损失可能不是一个好的损失函数，因为它没有裕度，因此基于裕度的 hinge 损失是首选：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205155522.png#pic_center)


修改上述实现以处理hinge损耗是微不足道的。

请注意，在两种情况下，我们都会失去线性模型的良好特性。特别是，模型不再是凸的。这是可以预料的，因为即使最简单的非线性神经网络也已经是非凸的。尽管如此，我们仍然可以使用标准的神经网络优化技术来训练结构化模型。

训练和推理较慢，因为我们必须评估神经网络（并采用梯度）$|  parts（x，y）|$次。

结构化预测是一个广阔的领域，超出了本教程的范围，但是可以轻松地将例如损失函数，正则化器和增强成本的解码，应用于神经网络或将其应用于神经网络框架。

### 8.2.1 概率目标（CRF）

在概率框架（条件随机字段，“CRF”）中，我们将各个部分的得分视为集团潜力，并定义每个结构的得分$y$成为：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205209516.png#pic_center)


计分函数定义条件分布$P（y  | x）$，我们希望设置网络参数，以使语料库条件对数似然性$\sum_{(x_i,y_i)\in training}logP（y_i | x_i）$最大化。

因此，给定训练示例$（x，y）$的损失为：$log \ score_{CRF}（x，y）$。相对于损失采取梯度与建立相关的计算图一样复杂。棘手的部分是分母（分区函数），它需要求和Y中可能呈指数形式的许多结构。但是，对于某些问题，存在一种动态编程算法，可以有效地求解多项式时间的求和（即，向前和向后维特比循环序列和CKY内外循环树结构）。当存在这样的算法时，它可以适用于创建多项式大小的计算图。

当没有足够有效的算法来计算分区函数时，可以使用近似方法。例如，可以使用波束搜索进行推断，并使用分区函数求和在波束中剩余的结构上而不是在指数大的Y（x）上求和。

### 8.2.2 重新排序

当搜索所有可能的结构是棘手的，效率低下或难以集成到模型中时，通常使用重新排序方法。在重排框架中，基本模型用于生成 k-best 评分结构的列表。然后训练一个更复杂的模型，以对k个最佳列表中的候选者进行评分，以使相对于黄金的最佳结构得分最高。由于现在搜索是在k个项目上而不是在指数空间上进行的，因此复杂模型可以根据打分结构的任意方面进行条件处理（从中提取特征）。重新排序方法是使用神经网络模型进行结构化预测的自然候选者，因为它们使建模者可以专注于特征提取和网络结构，而无需将神经网络评分集成到解码器中。实际上，重排序方法通常用于实验神经网络模型，这些模型不能直接集成到解码器中，例如卷积网络，循环网络和递归网络，这将在后面的部分中进行讨论。

### 8.2.3 MEMM 和混合方法

当然也可以采用其他公式。例如，通过用MLP替换逻辑回归（“最大熵”）组件，可以使MEMM轻松适应神经网络世界。

人们还探索了神经网络和线性模型之间的混合方法。  报告了两阶段模型中基于过渡的依赖项解析的强劲结果。在第一阶段，对静态前馈神经网络（MLP2）进行训练，使其能够独立处理结构化问题的各个决策。在第二阶段，将神经网络模型保持固定，然后将每个输入的不同层（输出以及隐藏层矢量）连接起来，并用作线性结构感知器模型的输入特征。经过训练可以执行波束搜索以获得最佳结果。虽然尚不清楚这种训练方式是否比训练单个结构化预测神经网络更有效，但是使用两个更简单的隔离模型可以使研究人员执行更广泛的超参数搜索（例如，调整层大小，激活功能，学习率等），而不是使用更复杂的网络所能实现的。

# 9 卷积层

有时我们对基于项目的有序集合（例如，句子中单词的顺序，文档中句子的顺序等）进行预测感兴趣。考虑例如预测句子的情绪（正面，负面或中立）。某些句子中的单词对情绪很有帮助，而其他单词则没有那么多信息，并且近似地，无论其在句子中的位置如何，提示性线索都是有益的。我们希望将所有的句子单词都提供给学习者，并让训练过程找出重要的线索。一种可能的解决方案是将CBOW表示馈送到完全连接的网络（例如MLP）中。但是，CBOW方法的一个缺点是它完全忽略了订购信息，将句子“it was not good, it was actually quite bad”和“it was not bad, it was actually quite good”分配了完全相同的表示形式。虽然指标“not good”和“not bad”的全局位置对于分类任务并不重要，但单词的局部排序（单词“  not”出现在单词“  bad”之前）非常重要。幼稚的方法建议嵌入单词对（二元语法）而不是单词，并在嵌入的二元组上构建CBOW。虽然这种架构可能有效，但会导致巨大的嵌入矩阵，无法扩展较长的 n-gram，并且由于数据稀疏性问题而无法在不同n-gram之间共享统计强度（ “quite good”和“very good”是彼此完全独立的，因此，如果学习者在训练中只看到其中一个，则将无法根据其组成词来推断另一个)。卷积和池化（也称为卷积神经网络或CNN）体系结构是解决此建模问题的一种优雅而强大的解决方案。卷积神经网络旨在识别大型结构中的指示性局部预测器，并将它们组合以生成结构的固定大小的矢量表示，捕获这些局部方面，这些方面对于手头的预测任务最为有用。

卷积和池化体系结构在神经网络视觉界发展起来，在那里它们作为对象检测器取得了巨大的成功：识别来自预定类别（“猫”，“自行车”）的对象，无论在其图片中位置如何。当应用于图像时，该体系结构使用二维（网格）卷积。当应用于文本时，我们主要关注一维（序列）卷积。卷积网络用于语义角色标记，情感和问题类型分类等。

## 9.1 基本卷积+池化

用于语言任务的卷积和池化体系结构背后的主要思想是在句子上的k字滑动窗口的每个实例上应用非线性（学习）函数。此函数（也称为“过滤器”）将k个单词的窗口转换为d维向量，该向量捕获窗口中单词的重要属性（文献中有时将每个维称为“通道”）。然后，通过获取在不同窗口上的每个d通道中观察到的最大值或平均值，使用“合并”操作将不同窗口产生的矢量组合为单个d维矢量。目的是将重点放在句子中最重要的“特征”上，而不管它们的位置如何。然后，将d维矢量进一步馈入用于预测的网络中。在训练过程中从网络损失传播回来的梯度用于调整过滤器功能的参数，以突出显示数据方面，这些方面对于训练网络的任务很重要。直观地，当滑动窗口在序列上运行时，过滤器函数将学习识别信息性k-gram。

更正式地说，考虑单词$x = x_1,...,x_n$的序列，每个都有其对应的$d_{emb}$维字嵌入$v（x_i）$。宽度为k的1d卷积层通过在句子上移动大小为k的滑动窗口，并对序列$[v(x_i);v(x_{i +  1});...;v(x_{i+k-1})]$的每个窗口应用相同的“过滤器”来工作。滤波函数通常是线性变换，其后是非线性激活函数。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205229100.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)


> 图4：在句子“the quick brown fox jumped over the lazy dog”的句子中的1d卷积+池化。这是窗口大小为3的窄卷积（不向句子中添加填充）。每个单词都转换为2维嵌入矢量（未显示）。然后将嵌入向量进行级联，以产生6维窗口表示。七个窗口中的每个窗口都通过$6\times 3$过滤器（线性变换，然后逐元素tanh）进行传输，从而得到七个三维过滤表示。然后，应用max-pooling操作，在每个维度上取最大值，从而生成最终的3维合并向量。

令第$i$个窗口的连接向量为$w_i  = [v(x_i);v(x_{i +  1});...;v(x_{i+k-1})],w_i\in R^{k·d_{emb}}$。根据我们是否在句子的两边填充$k −  1$个单词，我们可能得到$m = n − k + 1$（窄卷积）或$m = n + k + 1$窗口（宽卷积）。卷积层的结果是m个向量$p_1,...,p_m,p_i\in R^{d_{conv}}$：

![在这里插入图片描述](https://img-blog.csdnimg.cn/2021022020523990.png#pic_center)


g是一个非线性激活函数，可逐元素应用，$W\in R^{k·d_{emb}\times d_{conv}  },b\in R^{d_{conv}}$是网络的参数。每个$p_i$是一个$d_{conv}$维向量，编码$w_i$信息。理想情况下，每个维度都可以捕获不同种类的指示信息。然后使用最大池化层组合m个向量，得到单个$d_{conv}$向量c。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205247873.png#pic_center)



$p_i  [j]$表示$p_i$的第$j$个分量。  max-pooling操作的效果是在窗口位置上获得最显着的信息。理想情况下，每个维度都将“专门化”特定类型的预测变量，并且max操作将选择每种类型的最重要的预测变量。

图4提供了该过程的图示。

所得向量c是句子的表示，其中每个维度反映有关某个预测任务的最重要的信息。然后将c馈送到下游网络层，也许与其他向量并行，最后到达用于预测的输出层。网络的训练过程会计算与预测任务有关的损失，并且误差梯度会通过池化和卷积层以及嵌入层一路传播回去。

尽管最大池化是文本应用程序中最常见的池化操作，但其他池化操作也是可能的，第二最常见的操作是平均池化，采用每个索引的平均值而不是最大值。

## 9.2 动态、分层和k-max池化

与其在整个序列上执行单个合并操作，不如基于对当前预测问题的领域理解，我们可能希望保留一些位置信息。为此，我们可以将向量$p_i$划分为$l$“不同的组”，分别对每个组应用合并，然后将“所得$d_{conv}$维向量$c_1,...,c_l$串联起来。根据域知识将$p_i$分为几组。例如，我们可以推测句子中较早出现的单词比较晚出现的单词更具指示性。然后，我们可以将序列划分为大小相等的区域，并对每个区域应用单独的最大池。例如，在将文档分类为主题时，有20个平均池区域是有用的，可以将初始句子（通常是介绍主题的地方）与后面的句子区分开，而对于情感分类任务是在整个句子中进行单个最大合并操作是最佳的（建议一个或两个非常强烈的信号足以确定情绪，而与句子中的位置无关）。

类似地，在关系提取类型的任务中，我们可能会给两个单词并要求确定它们之间的关系。我们可以争辩说，第一个单词之前的单词，第二个单词之后的单词以及它们之间的单词提供了三种不同的信息。因此，我们可以相应地拆分$p_i$向量，分别合并每个组产生的窗口。

另一种变化是使用卷积层的层次结构，其中我们有一系列的卷积层和池化层，其中每个阶段将卷积应用于序列，池化每k个相邻向量，对所得池化序列执行卷积，再应用另一个卷积等等。这种架构允许对越来越大的结构敏感。

最后，人们引入了k-max合并操作，其中保留了每个维中的前k个值，而不是仅保留最佳维，同时保留了它们在文本中出现的顺序。例如，请考虑以下矩阵：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205301356.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)


k-max合并操作使得可以合并k个最活跃的指标，这些指标可能相距多个位置。它保留了特征的顺序，但对它们的特定位置不敏感。它还可以更精细地识别函数被高度激活的次数。

## 9.3 变体

除了单个卷积层，还可以并行应用几个卷积层。例如，我们可能有四个不同的卷积层，每个卷积层的窗口大小在2–5范围内，捕获不同长度的n-gram序列。然后将每个卷积层的结果合并起来，将所得向量进行级联并馈入进一步处理。

卷积架构不必局限于句子的线性顺序。卷积运算已在句法依赖树上工作。在那里，每个窗口都围绕语法树中的一个节点，并且在不同的节点上执行池化。同样，在从依赖树中提取的依赖路径之上应用了卷积架构。 建议对表示图表分析器中导致同一图表项的不同导数的向量进行最大池化。

# 10 循环神经网络-建模序列和堆栈

在处理语言数据时，使用单词（字母序列），句子（单词序列）和文档等序列非常普遍。我们看到了前馈网络如何通过使用向量级联和向量加法（CBOW）来容纳序列上的任意特征函数。特别地，CBOW表示允许将任意长度的序列编码为固定大小的向量。但是，CBOW表示形式非常有限，并且迫使人们忽略要素的顺序。卷积网络还允许将序列编码为固定大小的向量。尽管从卷积网络派生的表示形式比CBOW表示形式有所改进，因为它们提供了一些对单词顺序的敏感性，但它们的顺序敏感性仅限于局部模式，而忽略了顺序中相距甚远的模式的顺序。

循环神经网络（RNN）允许在固定大小的向量中表示任意大小的结构化输入，同时注意输入的结构化属性。

## 10.1 RNN概念

我们使用$x_{i:j}$表示向量$x_i,...,x_j$的序列。  RNN概念将输入向量$x_1,...,x_n$的有序列表与初始状态向量$s_0$一起作为输入，并返回状态向量的有序列$s_1,...,s_n$表以及输出的有序列表向量$y_1,...,y_n$。输出向量$y_i$是相应状态向量$s_i$的函数。输入向量$x_i$顺序呈现给RNN，状态向量$s_i$输出向量$y_i$表示观察输入$x_{1:i}$后RNN的状态。然后将输出向量$y_i$用于进一步的预测。例如，将给定序列$m_{1:i}$的事件e的条件概率预测模型定义为$p(e  = j | x_{1:i})= softmax(y_iW + b)[j]$，即输出中的第$j$个元素$softmax$操作产生的向量。  RNN模型提供了一个框架，用于对整个历史$x_1,..,x_i$进行条件处理。确实，与基于n-gram的模型相比，基于RNN的语言模型产生了很好的困惑度分数。

数学上，我们有一个递归定义的函数R，该函数将状态向量$s_i$和输入向量$x_{i + 1}$作为输入，并得出新的状态向量$s_{i +  1}$。附加函数$O$用于将状态向量$s_i$映射到输出向量$y_i$。在构造RNN时，就像在构造前馈网络时一样，必须指定输入$s_i$的尺寸以及输出$y_i$的尺寸。状态的尺寸取决于输出尺寸。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205317874.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)


在序列位置上，函数$R$和$O$相同，但是RNN通过在$R$调用之间保持并传递的状态向量来跟踪计算状态。

从图形上看，RNN传统上如图5所示。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205325271.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)



> 图5:RNN(递归)的图形表示

这种表示遵循递归定义，适用于任意长序列。然而，对于有限大小的输入序列(我们处理的所有输入序列都是有限的)，可以展开递归，得到图6中的结构。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205333194.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)


> 图6:RNN的图形表示(展开)。

虽然在可视化中通常不显示，但我们在这里包括参数$\theta$，以强调在所有时间步中共享相同的参数这一事实。$R$和$O$的不同实例化将导致不同的网络结构，并在运行时间和使用基于梯度的方法有效训练它们的能力方面表现出不同的特性。然而，它们都坚持使用相同的抽象接口。我们将在第11节中提供$R$和$O$的具体实例化细节——简单的RNN、LSTM和GRU。在此之前，让我们考虑使用RNN概念进行建模。

首先，我们注意到$s_i$的值基于整个输入$x_1,...,x_i$。例如，对$i  = 4$展开递归，我们得到:
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205342422.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)


因此，$s_n$(以及$y_n$)可以被认为是对整个输入序列的编码。编码有用吗?这取决于我们对有用性的定义。网络训练的任务是设置$R$和$O$的参数，使状态为我们要解决的任务传递有用的信息。

## 10.2 RNN训练

从图6中可以很容易地看出，展开的RNN只是一个非常深入的神经网络(或者更确切地说，是一个带有一些复杂节点的非常大的计算图)，其中相同的参数在计算的许多部分之间共享。为了训练一个RNN网络，我们所需要做的就是为给定的输入序列创建展开的计算图，在展开的图中添加一个损失节点，然后使用后退(backpropagation)算法计算与该损耗有关的梯度。在RNN文献中，这个过程被称为反向传播或BPTT。监测信号的应用有多种方式。

### 10.2.1 接受器

一种选择是只基于最终输出向量$y_n$的监督信号。从这个角度来看，RNN是一个受体。我们观察最终状态，然后决定结果。比如，考虑训练一个RNN读单词一个一个的字符,然后使用最终状态来预测这个词的词性；一个RNN读入一个句子，根据最终状态决定是否传达了积极或消极的情绪；或一个RNN读入一个序列的单词和决定是否它是一个有效的名词短语。这种情况下的损失是由$y_n=  O(s_n)$的函数定义的，误差梯度将反向传播到序列的其余部分(见图7)。损失可以采取任何熟悉的形式-交叉熵、hinge、裕度等。

### 10.2.2 编码器

类似于接受情况，编码器监督只使用最终输出向量$y_n$。然而，不同于接受者的预测仅仅基于最终向量，这里的最终向量被视为序列中信息的编码，并与其他信号一起用作附加信息。例如，一个抽取式文档摘要系统可能首先使用RNN对文档进行遍历，结果是在矢量中汇总整个文档。然后，$y_n$将和其他特征一起使用，以便选择要包含在摘要中的句子。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205354514.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)


> 图7:接受RNN训练图。

### 10.2.3 传感器

另一种选择是将RNN视为一个传感器，为它读取的每个输入产生一个输出。以这种方式建模，我们可以计算一个本地损失信号$L_{local}(y_i^*,y_i)$为每个输出$y_i^*$基于一个真实的标签$y_i$。展开序列将损失:$L(y_{i:n}^*,y_{i:n})=\sum_{i=1}^nL_{local}(y_i^*,y_i)$，或者使用其他组合，而不是诸如平均值或加权平均值之类的总和（请参见图8）。这种传感器的一个示例是序列标记器，其中我们将$x_{i:n}$用作句子中n个词的特征表示，并将$y_i$作为输入基于词$1:i$预测词$i$的标记分配。基于这种架构的CCG超级标语牌提供了最新的CCG超级标记结果。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205403215.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)


> 图8：传感器RNN训练图。

转导设置的一个非常自然的用例是用于语言建模，其中单词$x_{1：i}$的序列用于预测第$（i +  1）$个单词的分布。与传统的语言模型相比，基于RNN的语言模型提供了更好的困惑。

使用RNN作为传感器可以使我们放宽在语言模型和HMM标记器中传统采用的马尔可夫假设，并以整个预测历史为条件。在生成的字符级RNN模型中证明了对任意长历史条件进行调节的能力，在该模型中，逐个字符地生成文本，每个字符都以先前的为条件。生成的文本显示对n-gram语言模型未捕获的属性的敏感性，包括行长和嵌套括号平衡。

### 10.2.4 编码器-解码器

最后，编码器场景的一个重要特例是编码器-解码器框架。  RNN用于将序列编码为向量表示$y_n$，然后将该向量表示用作另一个RNN的辅助输入，该另一个RNN用作解码器。例如，在机器翻译设置中，第一个RNN将源句子编码为向量表示$y_n$，然后将此状态向量馈入一个单独的（解码器）RNN，该RNN经过训练可以预测（使用类似传感器的语言建模目标）基于先前预测的单词以及$y_n$的目标语言句子中的单词。监视仅针对解码器RNN进行，但梯度会一直传播回编码器RNN（请参见图9）。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205413402.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)


> 图9：编码器-解码器RNN训练图。

事实证明，这种方法对于使用LSTM  RNN的机器翻译非常有效。为了使这项技术起作用，发现反向输入源句子很有效，因此$x_n$对应于句子的第一个单词。这样，第二RNN更容易建立源句子的第一单词与目标句子的第一单词之间的关系。编码器-解码器框架的另一个用例是序列转换。这里，为了生成标签$t_1,...,t_n$，首先使用编码器RNN将句子$x_{1:n}$编码为固定大小的向量。然后将此向量作为另一个（传感器）RNN的初始状态向量进行馈送，该向量与$x_{1:n}$一起用于预测每个位置i的标记。  使用此方法来模拟通过删除进行句子压缩。

## 10.3 多层（堆叠）RNN

RNN可以分层堆积，形成网格。考虑k个RNN，$RNN_1,..., RNN_k$，其中第$j$个RNN的状态为$s^j_{1:n}$，输出$y^j_{1:n}$。第一个RNN的输入为$x_{1:n}$，而第$j$个RNN（$j>1$）的输入是其下的RNN的输出$y^{j-1}_{1:n}$。整个编队的输出是最后一个RNN的输出，$y^k_{1:n}$。这种分层架构通常称为深度RNN。图10给出了3层RNN的可视化表示。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205424146.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)


> 图10：3层（“深层”）RNN架构。

虽然理论上还不清楚更深层次的架构获得的额外能力是什么，但根据经验观察，在某些任务上，更深层次的RNN比更浅层次的RNN工作得更好。4层的深层架构对于在编码器-解码器框架中实现良好的机器翻译性能至关重要。许多其他作品使用分层RNN架构报告结果，但没有明确地与1层RNN进行比较。

## 10.4 双向RNN（biRNN）

RNN的一个有用的阐述是双向RNN（biRNN）。考虑对句子$x_1,...,x_n$进行序列标记的任务。一个RNN允许我们基于过去单词$x_{1:i}$计算第$i$个单词的函数$x_i$。但是，后面单词$x_{i:n}$也可能对预测有用，这可以从常见的滑动窗口方法中看出，在该方法中，根据围绕它的k个单词的窗口对焦点单词进行分类。就像RNN放宽了Markov假设并允许任意回顾过去一样，biRNN放宽了固定窗口大小的假设，允许任意回顾过去和未来。

考虑输入序列$x_{1:n}$。  biRNN通过维持两个独立的状态$s^f_i$和$s^b_i$或每个输入位置$i$来工作。基于$x_1,x_2,...,x_i$的前向状态$s^f_i$，而后向状态$s^b_i$基于$x_n,x_{n-1},...,x_i$。前向和后向状态由两个不同的RNN生成。第一个RNN$（R^f，O^f）$照原样输入输入序列$x_{1:n}$，而第二个RNN$（R^b，O^b）$则反向输入输入序列。然后，状态表示由正向和反向状态组成。

位置$i$的输出基于两个输出向量$y_i  = [y^f_i;y^b_i] = [O^f（s^f_i）; O^b（s^b_i）]$的串联，同时考虑了过去和未来。然后，向量$y_i$可直接用于预测，或作为输入的一部分馈入更复杂的网络。当两个RNN彼此独立运行时，位置$i$处的误差梯度将通过两个RNN前后流动。图11给出了biRNN架构的可视化表示。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205436680.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)

> 图11：biRNN在句子“the brown fox jumped .”上。

## 10.5 代表堆栈的RNN

语言处理中的某些算法，包括用于基于过渡的解析的算法，要求在堆栈上执行特征提取。  RNN框架不仅可以查看堆栈的k个最上面的元素，还可以用于提供整个堆栈的固定大小的矢量编码。

主要直觉是堆栈本质上是一个序列，因此可以通过获取堆栈元素并将它们按顺序输入RNN来表示堆栈状态，从而对整个堆栈进行最终编码。为了高效地执行此计算（每次堆栈更改时都无需执行$O（n）$堆栈编码操作），RNN状态与堆栈状态一起被维护。如果堆栈仅是推入式的，这将是微不足道的：每当将新元素$x$推入堆栈时，相应的向量$x$就会与RNN状态$s_i$一起使用，以获得新的状态$s_{i  + 1}$。处理pop操作更具挑战性，但可以通过使用持久性堆栈数据结构来解决。持久的或不变的数据结构在修改后会使它们的旧版本保持完整。持久堆栈构造将堆栈表示为指向链表头的指针。空堆栈是空列表。  push操作将元素添加到列表，返回新的头。然后，pop操作返回头的父级，但保持原始列表不变。从持有指向前一个指针的人的角度来看，堆栈没有更改。后续的推入操作会将新的子代添加到同一节点。在堆栈的整个生命周期中应用此过程会生成一棵树，其中根是一个空堆栈，从节点到根的每个路径都表示一个中间堆栈状态。图12提供了这种树的示例。可以在计算图构造中应用相同的过程，以树形结构而不是链形结构创建RNN。从给定节点向后传播错误将依次影响节点创建时参与堆栈的所有元素。图13显示了对应于图12中最后一个状态的stack-RNN的计算图。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205445124.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)


> 图12：操作序列的不可变堆栈构造

![\[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-CJTz23OZ-1613824700362)(D:\picture\image-20210220194525308.png)\]](https://img-blog.csdnimg.cn/202102202054525.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)


> 图13：对应于图12中最终状态的stack-RNN

## 10.6 阅读文献的注释

不幸的是，通常情况下，通过在研究论文中阅读其描述来推断确切的模型形式可能会非常具有挑战性。模型的许多方面尚未标准化，并且不同的研究人员使用相同的术语指代稍有不同的事物。列举一些例子，RNN的输入可以是one-hot向量（在这种情况下，嵌入矩阵在RNN内部）或嵌入式表示；输入序列可以用序列开始或序列结束符号填充，也可以不填充。虽然通常将RNN的输出假定为向量，并期望将其馈送到其他层，然后再馈入$softmax$进行预测（如本教程中的演示所示），但一些论文假定$softmax$属于以下内容：  RNN本身；在多层RNN中，“状态向量”可以是最顶层的输出，也可以是所有层的输出的并置。当使用编码器-解码器框架时，可以以各种不同的方式来解释编码器的输出条件。等等。最重要的是，下一节中描述的LSTM体系结构具有许多小的变体，所有这些变体都以通用名称LSTM进行引用。这些选择中的某些选择在文件中明确提出，其他选择需要仔细阅读，而其他选择甚至都没有提及，或者隐藏在模棱两可的数字或措辞后面。

作为读者，在阅读和解释模型描述时请注意这些问题。作为作者，也要注意以下问题：以数学符号完全指定您的模型，或者如果可用，请参考在其中完全指定了模型的其他资源。如果在不了解详细信息的情况下使用软件包的默认实现，请明确说明该事实并指定要使用的软件包。在任何情况下，描述模型时都不要仅仅依靠数字或自然语言文字，因为它们通常是模棱两可的。

# 11 具体的RNN架构

现在，我们将介绍上一节中讨论的抽象RN N体系结构的三种不同实例，提供函数$R$和$O$的具体定义。它们是简单RNN（SRNN），长短期内存（LSTM）和门控循环单元（GRU）。

## 11.1 SRNN

最简单的RNN公式称为Elman网络或Simple-RNN（S-RNN）。  S-RNN采用以下形式：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205507351.png#pic_center)


也就是说，位置$i$处的状态是位置$i$处的输入和通过非线性激活（通常为$tanh$或$ReLU$）传递的先前状态的线性组合。$ i$位置的输出与该位置的隐藏状态相同。

尽管简单，但简单RNN在序列标记以及语言建模方面提供了出色的结果。

## 11.2 LSTM

由于梯度消失的问题，S-RNN难以有效训练。序列中后续步骤中的误差信号（梯度）在反向传播过程中会迅速减小，并且不会到达较早的输入信号，这使得S-RNN难以捕获远程依赖关系。长短期记忆（LSTM）体系结构旨在解决梯度消失的问题。  LSTM背后的主要思想是，作为状态表示的一部分，引入可以保留跨时间梯度的“内存单元”（向量）。对存储单元的访问由门控组件控制：平滑的数学函数可模拟逻辑门。在每个输入状态下，门用于决定应将多少新输入写入存储单元，以及应忘记多少当前存储单元内容。具体而言，门$g\in [0,1]^n$]将一个向量范围为$[0,1]$的向量与另一个向量$v\in R^n$逐分量相乘，然后将结果添加到另一个向量。  g的值被设计为接近0或1，即通过使用sigmoid函数。 v中与g中接近1的值对应的索引被允许通过，而与n中接近0的值对应的索引被阻止。

在数学上，LSTM体系结构定义为：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205517798.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)


符号$\Theta$用于表示按组件分类的产品。时间$j$的状态由两个向量$c_j$和$  h_j$组成，其中$c_j$是内存成分，$h_j$是隐藏状态成分。  $i$，$f$和$o$这三个门控制着输入，忘记和输出。基于通过 sigmoid 激活函数传递的先前状态$h_{j-1}$的当前输入的线性组合来计算门值。将更新候选$g$计算为通过tanh激活函数的$x_j$和  $h_{j-1}$的线性组合。然后更新内存$c_j$：忘记门控制要保留的前一内存有多少（$c_{j-1}\Theta f$），输入门控制要保留的更新有多少（$g\Theta i$）。最后，基于存储器$c_j$的内容确定$h_j$（也是输出$y_j$）的值，该值经过tanh非线性并由输出门控制。门控机制允许与存储部分$c_j$相关的梯度在很长的时间范围内保持较高的水平。

LSTM是目前最成功的RNN体系结构类型，它们负责许多最新的序列建模结果。 LSTM-RNN的主要竞争对手是GRU，下面将进行讨论。

### 11.2.1 实际考虑

当训练LSTM网络时，强烈建议始终将遗忘门的偏差项初始化为接近1。 在使用LSTM将压降应用于RNN时发现，仅在非经常性连接上应用丢包是至关重要的，即仅在层之间而不是在序列位置之间应用丢包。

## 11.3 GRU

LSTM体系结构非常有效，但是也很复杂。系统的复杂性使其难以分析，并且使用起来在计算上也很昂贵。 最近引入了门控循环单元（GRU）作为LSTM的替代。随后，证明了在几个（非文本）数据集上的性能与LSTM相当。

像LSTM一样，GRU也基于门控机制，但门极少，并且没有单独的存储组件。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205529561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)


一个门$（r）$用于控制对先前状态$s_{j-1}$的访问，并计算建议的更新$s_j$。然后根据先前状态$s_j-1$和建议$s_j$的插值确定更新后的状态$s_j$（也用作输出$y_j$），其中插值的比例由门$z$控制。

事实证明，GRU在语言建模和机器翻译方面是有效的。但是，在GRU，LSTM和可能的替代RNN体系结构之间仍存在争议，因此对该主题进行了积极研究。

## 11.4 其他变体

LSTM和GRU的门控体系结构有助于缓解简单 RNN的消失梯度问题，并允许这些RNN捕获跨越较长时间范围的依存关系。为了获得类似的好处，一些研究人员探索了比LSTM和GRU更简单的体系结构。

矩阵乘法$s_{i-1}W^s$与简单RNN的更新规则$R$中的非线性$g$耦合，导致状态向量$s_i$在每个时间步长发生较大变化，从而阻止了它长时间记忆信息。他们建议将状态向量$s_i$分为慢变化分量$c_i$（“上下文单位”）和快变化分量$h_i$。慢变化分量$c_i$根据输入和先前分量的线性插值进行更新：$c_i =（1- \alpha）x_iW^{x_1}  +\alpha c_{i-1}，其中\alpha \in（0,1）$。此更新允许$c_i$累积以前的输入。快速变化的成分$h_i$的更新类似于简单RNN更新规则，但也进行了更改以考虑$c_i$：$h_i  =\sigma（x_iW^{x_2} + h-{i-1}W^h + c_iW^c）$。最后，输出$y_i$是状态的缓慢变化部分和快速变化部分的串联：$y_i = [c_i; h_i]$。  证明了该体系结构为语言建模任务上复杂得多的LSTM提供了竞争性的困惑。

该方法可以解释为将对应于$c_i$的S-RNN中的矩阵$W^s$的块约束为单位矩阵的乘积。  提出了一种更简单的方法：将S-RNN的激活函数设置为ReLU，并将偏差b初始化为零，并将矩阵$W^s$初始化为标识矩阵。这将导致未经训练的RNN将先前的状态复制到当前状态，加上当前输入$x_i$的影响并将负值设置为零。在将初始偏差设置为状态复制后，训练过程允许$W^s$自由更改。证明了这种简单的修改使S-RNN可以与LSTM相比，在多个任务（包括语言建模）上具有相同数量的参数。

# 12 建模树-递归神经网络

RNN对于序列建模非常有用。在语言处理中，使用树结构通常是很自然且合乎需要的。这些树可以是句法树，话语树，甚至是代表句子各个部分表达的情感的树。我们可能希望基于特定的树节点预测值，基于根节点预测值，或为完整的树或树的一部分分配质量得分。在其他情况下，我们可能不会直接在乎树的结构，而是在句子中跨度。在这种情况下，树仅用作主干结构，有助于将序列的编码过程引导到固定大小的向量中。

递归神经网络（RecNN）抽象是RNN从序列到（二叉）树的概括。

就像RNN将每个句子前缀编码为状态向量一样，RecNN将每个树节点编码为$R^d$中的状态向量。然后，我们可以使用这些状态向量来预测相应节点的值，为每个节点分配质量值，或者作为根植于节点的跨度的语义表示。

递归神经网络背后的主要直觉是，每个子树都表示为一个 d 维向量，带有子$c_1$和$c_2$的节点$p$的表示是节点表示的函数：$vec（p）=  f（vec（c_1  ），vec（c_2））$，其中$f$是一个合成函数，它接受两个d维向量并返回一个d维向量。与用于编码整个序列$x_{1:i}$的RNN状态$s_i$非常相似，与树节点p相关联的RecNN状态对以p为根的整个子树进行编码。有关说明，请参见图14。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210220205545101.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg1NzY4OA==,size_16,color_FFFFFF,t_70#pic_center)


> 图14:递归神经网络的图解。$V$和$NP_1$的表示法结合起来形成$VP$的表示法。然后把$VP$和$NP_2$的表示组合起来，形成$S$的表示。

## 12.1 正式定义

略

## 12.2 扩展和变体

由于上述R的所有定义都受到简单RNN的渐变消失问题的困扰，一些作者试图用受长期短期记忆(LSTM)门控架构启发的函数来替代它，从而产生了树形的LSTMs  。最优树表示问题仍然是一个非常开放的研究问题，可能的组合函数R的广阔空间还有待探索。树形结构的RNN的其他提出的变体包括递归矩阵向量模型和递归神经张量网络。在第一个变种，每个单词都被表示为一个向量和一个矩阵,向量定义了词的静态语义内容，而矩阵作为学习“操作符”这个词，让更多微妙的语义成分比隐含的加法和加权平均连接其次是线性变换函数。在第二种变体中，单词通常与向量联系在一起，但复合函数通过基于张量而不是矩阵运算变得更有表现力。

## 12.3 训练递归神经网络

递归神经网络的训练过程遵循与训练其他形式的网络相同的方法:定义一个损失，拼出计算图，使用反向传播计算梯度，并使用SGD训练参数。

至于损失函数，与序列RNN类似，我们可以将一个损失与树的根、任何给定节点或一组节点联系起来，在这种情况下，单个节点的损失是组合起来的，通常是求和的。损失函数基于标记的训练数据，该训练数据将一个标记或其他数量与不同的树节点相关联。

此外，可以将RecNN视为一个编码器，而与节点关联的内部向量则被视为以该节点为根的树的编码。编码可能会对结构的任意属性敏感。然后矢量被作为输入传递到另一个网络。

# 13 结论

神经网络是强大的学习者，提供了从非线性分类到序列和树的非马尔可夫建模的机会。我们希望这次的介绍能帮助NLP研究者在他们的工作中融入神经网络模型，并利用他们的力量。



回一个d维向量。与用于编码整个序列$x_{1:i}$的RNN状态$s_i$非常相似，与树节点p相关联的RecNN状态对以p为根的整个子树进行编码。有关说明，请参见图14。

[外链图片转存中...(img-UFprxNax-1613824700369)]

> 图14:递归神经网络的图解。$V$和$NP_1$的表示法结合起来形成$VP$的表示法。然后把$VP$和$NP_2$的表示组合起来，形成$S$的表示。

## 12.1 正式定义

略

## 12.2 扩展和变体

由于上述R的所有定义都受到简单RNN的渐变消失问题的困扰，一些作者试图用受长期短期记忆(LSTM)门控架构启发的函数来替代它，从而产生了树形的LSTMs  。最优树表示问题仍然是一个非常开放的研究问题，可能的组合函数R的广阔空间还有待探索。树形结构的RNN的其他提出的变体包括递归矩阵向量模型和递归神经张量网络。在第一个变种，每个单词都被表示为一个向量和一个矩阵,向量定义了词的静态语义内容，而矩阵作为学习“操作符”这个词，让更多微妙的语义成分比隐含的加法和加权平均连接其次是线性变换函数。在第二种变体中，单词通常与向量联系在一起，但复合函数通过基于张量而不是矩阵运算变得更有表现力。

## 12.3 训练递归神经网络

递归神经网络的训练过程遵循与训练其他形式的网络相同的方法:定义一个损失，拼出计算图，使用反向传播计算梯度，并使用SGD训练参数。

至于损失函数，与序列RNN类似，我们可以将一个损失与树的根、任何给定节点或一组节点联系起来，在这种情况下，单个节点的损失是组合起来的，通常是求和的。损失函数基于标记的训练数据，该训练数据将一个标记或其他数量与不同的树节点相关联。

此外，可以将RecNN视为一个编码器，而与节点关联的内部向量则被视为以该节点为根的树的编码。编码可能会对结构的任意属性敏感。然后矢量被作为输入传递到另一个网络。

# 13 结论

神经网络是强大的学习者，提供了从非线性分类到序列和树的非马尔可夫建模的机会。我们希望这次的介绍能帮助NLP研究者在他们的工作中融入神经网络模型，并利用他们的力量。